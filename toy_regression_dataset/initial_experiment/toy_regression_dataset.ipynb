{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "I'll create a toy dataset for regression. Then I'll test two different neural network settings. 1) I'll train an ensemble of neural networks without any additional regularization etc. 2) I'll train an ensemble where each ensemble member is trained using the diversity objective forcing it's output to be ``bad'' for inputs that are away from the training inputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x1373202b0>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "std = 0.5\n",
    "start = 0\n",
    "end = 10\n",
    "sampling = 100\n",
    "\n",
    "\n",
    "x = jnp.linspace(start,end,sampling)\n",
    "y = jnp.sin(x)+std*random.normal(key=key,shape=x.shape)\n",
    "\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a training, testing and unlabeled dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "x_training = x[jnp.where(jnp.logical_or(x>6,x<2))]\n",
    "y_training = y[jnp.where(jnp.logical_or(x>6,x<2))]\n",
    "\n",
    "x_test = x[jnp.where(jnp.logical_and(x<6,x>2))]\n",
    "y_test = y[jnp.where(jnp.logical_and(x<6,x>2))]\n",
    "\n",
    "\n",
    "std = 0.5\n",
    "start = 0\n",
    "end = 10\n",
    "sampling = 100\n",
    "\n",
    "key, new_key = random.split(key)\n",
    "x_unlabeled = jnp.linspace(start,end,sampling)\n",
    "y_unlabeled = jnp.sin(x)+std*random.normal(key=new_key,shape=x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x29d9aa440>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_training,y_training,c='red')\n",
    "plt.scatter(x_test,y_test,c='blue')\n",
    "plt.scatter(x_unlabeled,y_unlabeled,c='yellow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll expand the array dimensions so that they match what's expected at the neural network input and output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "x_training = jnp.expand_dims(x_training,axis=1)\n",
    "y_training = jnp.expand_dims(y_training,axis=1)\n",
    "\n",
    "\n",
    "y_test = jnp.expand_dims(y_test,axis=1)\n",
    "x_test = jnp.expand_dims(x_test,axis=1)\n",
    "\n",
    "y_unlabeled = jnp.expand_dims(y_unlabeled,axis=1)\n",
    "x_unlabeled = jnp.expand_dims(x_unlabeled,axis=1)\n",
    "\n",
    "print(x_training.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll define now the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP model.\n",
    "    \"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train):\n",
    "        out = nn.Dense(features=100)(x)\n",
    "        out = nn.relu(out)\n",
    "        out = nn.Dense(features=100)(out)\n",
    "        out = nn.relu(out)\n",
    "        out = nn.Dense(features=100)(out)\n",
    "        out = nn.relu(out)\n",
    "        out = nn.Dense(features=1)(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define now some routines which are useful for optimizing the neural network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.training import train_state\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from typing import Any\n",
    "import hydra\n",
    "\n",
    "ModuleDef = Any\n",
    "KeyArray = random.KeyArray\n",
    "Array = Any\n",
    "DTypeLikeFloat = Any\n",
    "DTypeLikeComplex = Any\n",
    "DTypeLikeInexact = Any\n",
    "RealNumeric = Any\n",
    "\n",
    "\n",
    "def squared_error(*, logits, targets):\n",
    "    \"\"\"\n",
    "    The squared error. Can also be seen as NLL with a Gaussian likelihood.\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits: float\n",
    "        The prediction preactivations of the neural network.\n",
    "    targets:\n",
    "        The regression targets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        : optax.l2_loss\n",
    "        The l2 loss.\n",
    "\n",
    "    \"\"\"\n",
    "    return optax.l2_loss(predictions=logits, targets=targets).mean()\n",
    "\n",
    "\n",
    "def compute_metrics(*, logits, targets):\n",
    "    \"\"\"\n",
    "    Computes the crossentropy loss and the accuracy for a given set of predictions and groundtruth labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits: float\n",
    "        The prediction preactivations of the neural network.\n",
    "    labels:\n",
    "        The groundtruth labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics: dict\n",
    "        A python dictionary with keys \"loss\" and \"accuracy\" corresponding to the cross-entropy loss and the accuracy\n",
    "        for some logits and labels.\n",
    "\n",
    "    \"\"\"\n",
    "    loss = squared_error(logits=logits, targets=targets)\n",
    "    metrics = {\n",
    "        'loss': loss\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def create_train_state(rng):\n",
    "    \"\"\"\n",
    "    Creates initial `TrainState`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rng : jax.random.PRNGKey\n",
    "        Pseudo-random number generator (PRNG) key for the random initialization of the neural network.\n",
    "    cfg : DictConfig\n",
    "        The configuration file for the experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        :train_state.TrainState\n",
    "        The initial training state of the experiment.\n",
    "    \"\"\"\n",
    "    network = MLP()\n",
    "    params = network.init(rng, jnp.ones([1, 1]), train=False)\n",
    "    tx = optax.adamw(0.001)\n",
    "    return train_state.TrainState.create(apply_fn=network.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, dropout_rng):\n",
    "    \"\"\"\n",
    "    Trains the neural network for a single step.\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The initial training state of the experiment.\n",
    "    batch : dict\n",
    "        Dictionary with keys 'image' and 'label' corresponding to a batch of the training set.\n",
    "    dropout_rng : jax.random.PRNGKey\n",
    "        Pseudo-random number generator (PRNG) key for the randomness of the dropout layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "    metrics : dict\n",
    "        A python dictionary with keys \"loss\" and \"accuracy\" corresponding to the cross-entropy loss and the accuracy\n",
    "        for some logits and labels.\n",
    "    new_dropout_rng : jax.random.PRNGKey\n",
    "        New pseudo-random number generator (PRNG) key for the randomness of the dropout layers.\n",
    "\n",
    "    \"\"\"\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, x_training, train=True, rngs={'dropout': dropout_rng})\n",
    "        loss = squared_error(logits=logits, targets=y_training)\n",
    "        return loss, logits\n",
    "    grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "    grads, logits = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, targets=y_training)\n",
    "    return state, metrics, new_dropout_rng\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state):\n",
    "    \"\"\"\n",
    "    A single evaluation step of the output logits of the neural network for a batch of inputs, as well as the\n",
    "    cross-entropy loss and the classification accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "    batch : dict\n",
    "        Dictionary with keys 'image' and 'label' corresponding to a batch of the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        A python dictionary with keys \"loss\" and \"accuracy\" corresponding to the cross-entropy loss and the accuracy\n",
    "        for some logits and labels.\n",
    "    \"\"\"\n",
    "    logits = state.apply_fn(state.params, x_test, train=False)\n",
    "    return compute_metrics(logits=logits, targets=y_test)\n",
    "\n",
    "\n",
    "def train_epoch(state, epoch, rng):\n",
    "    \"\"\"\n",
    "    Train for a single epoch.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "    train_ds: dict\n",
    "        Dictionary with keys 'image' and 'label' corresponding to the training set.\n",
    "    batch_size : int\n",
    "        The size of the batch.\n",
    "    epoch : int\n",
    "        The number of the current epoch.\n",
    "    rng : jax.random.PRNGKey\n",
    "        Pseudo-random number generator (PRNG) key for the random initialization of the neural network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    state : train_state.TrainState\n",
    "        The new training state of the experiment.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    batch_metrics = []\n",
    "    dropout_rng = jax.random.split(rng, jax.local_device_count())[0]\n",
    "\n",
    "    state, metrics, dropout_rng = train_step(state, dropout_rng)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "    #compute mean of metrics across each batch in epoch.train_state\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0] # jnp.mean does not work on lists\n",
    "    }\n",
    "\n",
    "    print('train epoch: %d, loss %.4f' % (epoch, epoch_metrics_np['loss']))\n",
    "\n",
    "    train_log_dir = 'logs/standard_training/train'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('l2_error', epoch_metrics_np['loss'], step=epoch)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def eval_model(state):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "    test_ds: dict\n",
    "        Dictionary with keys 'image' and 'label' corresponding to the test set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = eval_step(state)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_util.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss']\n",
    "\n",
    "\n",
    "def train_network():\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : DictConfig\n",
    "        The configuration file for the experiment.\n",
    "    Returns\n",
    "    -------\n",
    "    test_accuracy : float\n",
    "        The final test accuracy of the trained model. This is useful when doing hyperparameter search with optuna.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rng = jax.random.PRNGKey(40)#0\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "    state = create_train_state(init_rng)\n",
    "    del init_rng #Must not be used anymore\n",
    "\n",
    "    num_epochs = 5000\n",
    "\n",
    "    test_log_dir = 'logs/standard_training/test'\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        # Use a separate PRNG key to permute image data during shuffling\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        # Run an optimization step over a training batch\n",
    "        state = train_epoch(state, epoch, input_rng)\n",
    "        # Evaluate on the test set after each training epoch\n",
    "        test_loss = eval_model(state)\n",
    "        print('test epoch: %d, loss: %.2f' % (epoch, test_loss))\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss, step=epoch)\n",
    "\n",
    "    return test_loss, state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try to train the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss 1.3165\n",
      "test epoch: 1, loss: 0.73\n",
      "train epoch: 2, loss 0.3693\n",
      "test epoch: 2, loss: 1.24\n",
      "train epoch: 3, loss 0.9509\n",
      "test epoch: 3, loss: 0.98\n",
      "train epoch: 4, loss 0.6193\n",
      "test epoch: 4, loss: 0.56\n",
      "train epoch: 5, loss 0.2551\n",
      "test epoch: 5, loss: 0.31\n",
      "train epoch: 6, loss 0.3321\n",
      "test epoch: 6, loss: 0.25\n",
      "train epoch: 7, loss 0.5381\n",
      "test epoch: 7, loss: 0.25\n",
      "train epoch: 8, loss 0.5256\n",
      "test epoch: 8, loss: 0.30\n",
      "train epoch: 9, loss 0.3469\n",
      "test epoch: 9, loss: 0.43\n",
      "train epoch: 10, loss 0.2325\n",
      "test epoch: 10, loss: 0.61\n",
      "train epoch: 11, loss 0.2797\n",
      "test epoch: 11, loss: 0.76\n",
      "train epoch: 12, loss 0.3835\n",
      "test epoch: 12, loss: 0.78\n",
      "train epoch: 13, loss 0.4001\n",
      "test epoch: 13, loss: 0.68\n",
      "train epoch: 14, loss 0.3200\n",
      "test epoch: 14, loss: 0.54\n",
      "train epoch: 15, loss 0.2400\n",
      "test epoch: 15, loss: 0.41\n",
      "train epoch: 16, loss 0.2357\n",
      "test epoch: 16, loss: 0.34\n",
      "train epoch: 17, loss 0.2892\n",
      "test epoch: 17, loss: 0.31\n",
      "train epoch: 18, loss 0.3248\n",
      "test epoch: 18, loss: 0.33\n",
      "train epoch: 19, loss 0.3018\n",
      "test epoch: 19, loss: 0.38\n",
      "train epoch: 20, loss 0.2501\n",
      "test epoch: 20, loss: 0.46\n",
      "train epoch: 21, loss 0.2241\n",
      "test epoch: 21, loss: 0.56\n",
      "train epoch: 22, loss 0.2419\n",
      "test epoch: 22, loss: 0.62\n",
      "train epoch: 23, loss 0.2722\n",
      "test epoch: 23, loss: 0.63\n",
      "train epoch: 24, loss 0.2759\n",
      "test epoch: 24, loss: 0.58\n",
      "train epoch: 25, loss 0.2507\n",
      "test epoch: 25, loss: 0.51\n",
      "train epoch: 26, loss 0.2253\n",
      "test epoch: 26, loss: 0.44\n",
      "train epoch: 27, loss 0.2236\n",
      "test epoch: 27, loss: 0.39\n",
      "train epoch: 28, loss 0.2404\n",
      "test epoch: 28, loss: 0.37\n",
      "train epoch: 29, loss 0.2515\n",
      "test epoch: 29, loss: 0.38\n",
      "train epoch: 30, loss 0.2432\n",
      "test epoch: 30, loss: 0.42\n",
      "train epoch: 31, loss 0.2257\n",
      "test epoch: 31, loss: 0.48\n",
      "train epoch: 32, loss 0.2175\n",
      "test epoch: 32, loss: 0.53\n",
      "train epoch: 33, loss 0.2241\n",
      "test epoch: 33, loss: 0.56\n",
      "train epoch: 34, loss 0.2335\n",
      "test epoch: 34, loss: 0.56\n",
      "train epoch: 35, loss 0.2326\n",
      "test epoch: 35, loss: 0.53\n",
      "train epoch: 36, loss 0.2224\n",
      "test epoch: 36, loss: 0.49\n",
      "train epoch: 37, loss 0.2146\n",
      "test epoch: 37, loss: 0.45\n",
      "train epoch: 38, loss 0.2160\n",
      "test epoch: 38, loss: 0.42\n",
      "train epoch: 39, loss 0.2219\n",
      "test epoch: 39, loss: 0.42\n",
      "train epoch: 40, loss 0.2230\n",
      "test epoch: 40, loss: 0.43\n",
      "train epoch: 41, loss 0.2173\n",
      "test epoch: 41, loss: 0.47\n",
      "train epoch: 42, loss 0.2114\n",
      "test epoch: 42, loss: 0.50\n",
      "train epoch: 43, loss 0.2111\n",
      "test epoch: 43, loss: 0.53\n",
      "train epoch: 44, loss 0.2144\n",
      "test epoch: 44, loss: 0.54\n",
      "train epoch: 45, loss 0.2153\n",
      "test epoch: 45, loss: 0.52\n",
      "train epoch: 46, loss 0.2118\n",
      "test epoch: 46, loss: 0.50\n",
      "train epoch: 47, loss 0.2078\n",
      "test epoch: 47, loss: 0.47\n",
      "train epoch: 48, loss 0.2072\n",
      "test epoch: 48, loss: 0.45\n",
      "train epoch: 49, loss 0.2090\n",
      "test epoch: 49, loss: 0.45\n",
      "train epoch: 50, loss 0.2092\n",
      "test epoch: 50, loss: 0.46\n",
      "train epoch: 51, loss 0.2067\n",
      "test epoch: 51, loss: 0.48\n",
      "train epoch: 52, loss 0.2041\n",
      "test epoch: 52, loss: 0.50\n",
      "train epoch: 53, loss 0.2036\n",
      "test epoch: 53, loss: 0.52\n",
      "train epoch: 54, loss 0.2045\n",
      "test epoch: 54, loss: 0.52\n",
      "train epoch: 55, loss 0.2041\n",
      "test epoch: 55, loss: 0.51\n",
      "train epoch: 56, loss 0.2022\n",
      "test epoch: 56, loss: 0.50\n",
      "train epoch: 57, loss 0.2004\n",
      "test epoch: 57, loss: 0.48\n",
      "train epoch: 58, loss 0.2001\n",
      "test epoch: 58, loss: 0.47\n",
      "train epoch: 59, loss 0.2004\n",
      "test epoch: 59, loss: 0.47\n",
      "train epoch: 60, loss 0.1996\n",
      "test epoch: 60, loss: 0.48\n",
      "train epoch: 61, loss 0.1980\n",
      "test epoch: 61, loss: 0.50\n",
      "train epoch: 62, loss 0.1968\n",
      "test epoch: 62, loss: 0.51\n",
      "train epoch: 63, loss 0.1966\n",
      "test epoch: 63, loss: 0.52\n",
      "train epoch: 64, loss 0.1964\n",
      "test epoch: 64, loss: 0.52\n",
      "train epoch: 65, loss 0.1954\n",
      "test epoch: 65, loss: 0.51\n",
      "train epoch: 66, loss 0.1941\n",
      "test epoch: 66, loss: 0.50\n",
      "train epoch: 67, loss 0.1934\n",
      "test epoch: 67, loss: 0.49\n",
      "train epoch: 68, loss 0.1931\n",
      "test epoch: 68, loss: 0.49\n",
      "train epoch: 69, loss 0.1925\n",
      "test epoch: 69, loss: 0.50\n",
      "train epoch: 70, loss 0.1915\n",
      "test epoch: 70, loss: 0.51\n",
      "train epoch: 71, loss 0.1906\n",
      "test epoch: 71, loss: 0.52\n",
      "train epoch: 72, loss 0.1901\n",
      "test epoch: 72, loss: 0.52\n",
      "train epoch: 73, loss 0.1896\n",
      "test epoch: 73, loss: 0.52\n",
      "train epoch: 74, loss 0.1889\n",
      "test epoch: 74, loss: 0.51\n",
      "train epoch: 75, loss 0.1880\n",
      "test epoch: 75, loss: 0.51\n",
      "train epoch: 76, loss 0.1873\n",
      "test epoch: 76, loss: 0.50\n",
      "train epoch: 77, loss 0.1868\n",
      "test epoch: 77, loss: 0.50\n",
      "train epoch: 78, loss 0.1862\n",
      "test epoch: 78, loss: 0.51\n",
      "train epoch: 79, loss 0.1854\n",
      "test epoch: 79, loss: 0.52\n",
      "train epoch: 80, loss 0.1847\n",
      "test epoch: 80, loss: 0.52\n",
      "train epoch: 81, loss 0.1842\n",
      "test epoch: 81, loss: 0.53\n",
      "train epoch: 82, loss 0.1837\n",
      "test epoch: 82, loss: 0.52\n",
      "train epoch: 83, loss 0.1830\n",
      "test epoch: 83, loss: 0.52\n",
      "train epoch: 84, loss 0.1823\n",
      "test epoch: 84, loss: 0.52\n",
      "train epoch: 85, loss 0.1818\n",
      "test epoch: 85, loss: 0.51\n",
      "train epoch: 86, loss 0.1813\n",
      "test epoch: 86, loss: 0.52\n",
      "train epoch: 87, loss 0.1807\n",
      "test epoch: 87, loss: 0.52\n",
      "train epoch: 88, loss 0.1801\n",
      "test epoch: 88, loss: 0.53\n",
      "train epoch: 89, loss 0.1795\n",
      "test epoch: 89, loss: 0.53\n",
      "train epoch: 90, loss 0.1791\n",
      "test epoch: 90, loss: 0.53\n",
      "train epoch: 91, loss 0.1785\n",
      "test epoch: 91, loss: 0.53\n",
      "train epoch: 92, loss 0.1780\n",
      "test epoch: 92, loss: 0.53\n",
      "train epoch: 93, loss 0.1775\n",
      "test epoch: 93, loss: 0.53\n",
      "train epoch: 94, loss 0.1770\n",
      "test epoch: 94, loss: 0.53\n",
      "train epoch: 95, loss 0.1766\n",
      "test epoch: 95, loss: 0.53\n",
      "train epoch: 96, loss 0.1761\n",
      "test epoch: 96, loss: 0.53\n",
      "train epoch: 97, loss 0.1756\n",
      "test epoch: 97, loss: 0.54\n",
      "train epoch: 98, loss 0.1752\n",
      "test epoch: 98, loss: 0.54\n",
      "train epoch: 99, loss 0.1748\n",
      "test epoch: 99, loss: 0.54\n",
      "train epoch: 100, loss 0.1743\n",
      "test epoch: 100, loss: 0.54\n",
      "train epoch: 101, loss 0.1739\n",
      "test epoch: 101, loss: 0.54\n",
      "train epoch: 102, loss 0.1735\n",
      "test epoch: 102, loss: 0.54\n",
      "train epoch: 103, loss 0.1731\n",
      "test epoch: 103, loss: 0.54\n",
      "train epoch: 104, loss 0.1726\n",
      "test epoch: 104, loss: 0.54\n",
      "train epoch: 105, loss 0.1722\n",
      "test epoch: 105, loss: 0.54\n",
      "train epoch: 106, loss 0.1718\n",
      "test epoch: 106, loss: 0.55\n",
      "train epoch: 107, loss 0.1714\n",
      "test epoch: 107, loss: 0.55\n",
      "train epoch: 108, loss 0.1710\n",
      "test epoch: 108, loss: 0.55\n",
      "train epoch: 109, loss 0.1706\n",
      "test epoch: 109, loss: 0.54\n",
      "train epoch: 110, loss 0.1701\n",
      "test epoch: 110, loss: 0.55\n",
      "train epoch: 111, loss 0.1693\n",
      "test epoch: 111, loss: 0.55\n",
      "train epoch: 112, loss 0.1680\n",
      "test epoch: 112, loss: 0.59\n",
      "train epoch: 113, loss 0.1676\n",
      "test epoch: 113, loss: 0.56\n",
      "train epoch: 114, loss 0.1658\n",
      "test epoch: 114, loss: 0.54\n",
      "train epoch: 115, loss 0.1669\n",
      "test epoch: 115, loss: 0.57\n",
      "train epoch: 116, loss 0.1649\n",
      "test epoch: 116, loss: 0.60\n",
      "train epoch: 117, loss 0.1662\n",
      "test epoch: 117, loss: 0.59\n",
      "train epoch: 118, loss 0.1649\n",
      "test epoch: 118, loss: 0.56\n",
      "train epoch: 119, loss 0.1636\n",
      "test epoch: 119, loss: 0.54\n",
      "train epoch: 120, loss 0.1645\n",
      "test epoch: 120, loss: 0.55\n",
      "train epoch: 121, loss 0.1632\n",
      "test epoch: 121, loss: 0.58\n",
      "train epoch: 122, loss 0.1624\n",
      "test epoch: 122, loss: 0.60\n",
      "train epoch: 123, loss 0.1630\n",
      "test epoch: 123, loss: 0.59\n",
      "train epoch: 124, loss 0.1617\n",
      "test epoch: 124, loss: 0.57\n",
      "train epoch: 125, loss 0.1614\n",
      "test epoch: 125, loss: 0.56\n",
      "train epoch: 126, loss 0.1614\n",
      "test epoch: 126, loss: 0.58\n",
      "train epoch: 127, loss 0.1603\n",
      "test epoch: 127, loss: 0.60\n",
      "train epoch: 128, loss 0.1603\n",
      "test epoch: 128, loss: 0.60\n",
      "train epoch: 129, loss 0.1600\n",
      "test epoch: 129, loss: 0.59\n",
      "train epoch: 130, loss 0.1591\n",
      "test epoch: 130, loss: 0.57\n",
      "train epoch: 131, loss 0.1592\n",
      "test epoch: 131, loss: 0.58\n",
      "train epoch: 132, loss 0.1585\n",
      "test epoch: 132, loss: 0.60\n",
      "train epoch: 133, loss 0.1581\n",
      "test epoch: 133, loss: 0.61\n",
      "train epoch: 134, loss 0.1580\n",
      "test epoch: 134, loss: 0.60\n",
      "train epoch: 135, loss 0.1572\n",
      "test epoch: 135, loss: 0.59\n",
      "train epoch: 136, loss 0.1572\n",
      "test epoch: 136, loss: 0.59\n",
      "train epoch: 137, loss 0.1567\n",
      "test epoch: 137, loss: 0.61\n",
      "train epoch: 138, loss 0.1563\n",
      "test epoch: 138, loss: 0.62\n",
      "train epoch: 139, loss 0.1561\n",
      "test epoch: 139, loss: 0.61\n",
      "train epoch: 140, loss 0.1556\n",
      "test epoch: 140, loss: 0.60\n",
      "train epoch: 141, loss 0.1554\n",
      "test epoch: 141, loss: 0.60\n",
      "train epoch: 142, loss 0.1550\n",
      "test epoch: 142, loss: 0.62\n",
      "train epoch: 143, loss 0.1547\n",
      "test epoch: 143, loss: 0.62\n",
      "train epoch: 144, loss 0.1544\n",
      "test epoch: 144, loss: 0.61\n",
      "train epoch: 145, loss 0.1540\n",
      "test epoch: 145, loss: 0.61\n",
      "train epoch: 146, loss 0.1539\n",
      "test epoch: 146, loss: 0.62\n",
      "train epoch: 147, loss 0.1535\n",
      "test epoch: 147, loss: 0.63\n",
      "train epoch: 148, loss 0.1533\n",
      "test epoch: 148, loss: 0.63\n",
      "train epoch: 149, loss 0.1529\n",
      "test epoch: 149, loss: 0.62\n",
      "train epoch: 150, loss 0.1527\n",
      "test epoch: 150, loss: 0.62\n",
      "train epoch: 151, loss 0.1524\n",
      "test epoch: 151, loss: 0.63\n",
      "train epoch: 152, loss 0.1522\n",
      "test epoch: 152, loss: 0.64\n",
      "train epoch: 153, loss 0.1520\n",
      "test epoch: 153, loss: 0.63\n",
      "train epoch: 154, loss 0.1517\n",
      "test epoch: 154, loss: 0.63\n",
      "train epoch: 155, loss 0.1515\n",
      "test epoch: 155, loss: 0.64\n",
      "train epoch: 156, loss 0.1512\n",
      "test epoch: 156, loss: 0.65\n",
      "train epoch: 157, loss 0.1511\n",
      "test epoch: 157, loss: 0.64\n",
      "train epoch: 158, loss 0.1508\n",
      "test epoch: 158, loss: 0.64\n",
      "train epoch: 159, loss 0.1507\n",
      "test epoch: 159, loss: 0.65\n",
      "train epoch: 160, loss 0.1505\n",
      "test epoch: 160, loss: 0.65\n",
      "train epoch: 161, loss 0.1503\n",
      "test epoch: 161, loss: 0.65\n",
      "train epoch: 162, loss 0.1501\n",
      "test epoch: 162, loss: 0.65\n",
      "train epoch: 163, loss 0.1500\n",
      "test epoch: 163, loss: 0.65\n",
      "train epoch: 164, loss 0.1498\n",
      "test epoch: 164, loss: 0.66\n",
      "train epoch: 165, loss 0.1496\n",
      "test epoch: 165, loss: 0.66\n",
      "train epoch: 166, loss 0.1495\n",
      "test epoch: 166, loss: 0.66\n",
      "train epoch: 167, loss 0.1493\n",
      "test epoch: 167, loss: 0.66\n",
      "train epoch: 168, loss 0.1492\n",
      "test epoch: 168, loss: 0.67\n",
      "train epoch: 169, loss 0.1490\n",
      "test epoch: 169, loss: 0.66\n",
      "train epoch: 170, loss 0.1489\n",
      "test epoch: 170, loss: 0.66\n",
      "train epoch: 171, loss 0.1488\n",
      "test epoch: 171, loss: 0.67\n",
      "train epoch: 172, loss 0.1486\n",
      "test epoch: 172, loss: 0.67\n",
      "train epoch: 173, loss 0.1485\n",
      "test epoch: 173, loss: 0.67\n",
      "train epoch: 174, loss 0.1484\n",
      "test epoch: 174, loss: 0.67\n",
      "train epoch: 175, loss 0.1483\n",
      "test epoch: 175, loss: 0.68\n",
      "train epoch: 176, loss 0.1482\n",
      "test epoch: 176, loss: 0.68\n",
      "train epoch: 177, loss 0.1480\n",
      "test epoch: 177, loss: 0.68\n",
      "train epoch: 178, loss 0.1479\n",
      "test epoch: 178, loss: 0.68\n",
      "train epoch: 179, loss 0.1478\n",
      "test epoch: 179, loss: 0.68\n",
      "train epoch: 180, loss 0.1477\n",
      "test epoch: 180, loss: 0.68\n",
      "train epoch: 181, loss 0.1476\n",
      "test epoch: 181, loss: 0.69\n",
      "train epoch: 182, loss 0.1476\n",
      "test epoch: 182, loss: 0.69\n",
      "train epoch: 183, loss 0.1475\n",
      "test epoch: 183, loss: 0.69\n",
      "train epoch: 184, loss 0.1474\n",
      "test epoch: 184, loss: 0.69\n",
      "train epoch: 185, loss 0.1473\n",
      "test epoch: 185, loss: 0.69\n",
      "train epoch: 186, loss 0.1472\n",
      "test epoch: 186, loss: 0.69\n",
      "train epoch: 187, loss 0.1471\n",
      "test epoch: 187, loss: 0.69\n",
      "train epoch: 188, loss 0.1471\n",
      "test epoch: 188, loss: 0.70\n",
      "train epoch: 189, loss 0.1470\n",
      "test epoch: 189, loss: 0.70\n",
      "train epoch: 190, loss 0.1469\n",
      "test epoch: 190, loss: 0.70\n",
      "train epoch: 191, loss 0.1468\n",
      "test epoch: 191, loss: 0.70\n",
      "train epoch: 192, loss 0.1468\n",
      "test epoch: 192, loss: 0.70\n",
      "train epoch: 193, loss 0.1467\n",
      "test epoch: 193, loss: 0.70\n",
      "train epoch: 194, loss 0.1467\n",
      "test epoch: 194, loss: 0.70\n",
      "train epoch: 195, loss 0.1466\n",
      "test epoch: 195, loss: 0.71\n",
      "train epoch: 196, loss 0.1466\n",
      "test epoch: 196, loss: 0.70\n",
      "train epoch: 197, loss 0.1466\n",
      "test epoch: 197, loss: 0.71\n",
      "train epoch: 198, loss 0.1465\n",
      "test epoch: 198, loss: 0.71\n",
      "train epoch: 199, loss 0.1465\n",
      "test epoch: 199, loss: 0.71\n",
      "train epoch: 200, loss 0.1465\n",
      "test epoch: 200, loss: 0.71\n",
      "train epoch: 201, loss 0.1464\n",
      "test epoch: 201, loss: 0.71\n",
      "train epoch: 202, loss 0.1464\n",
      "test epoch: 202, loss: 0.71\n",
      "train epoch: 203, loss 0.1464\n",
      "test epoch: 203, loss: 0.71\n",
      "train epoch: 204, loss 0.1463\n",
      "test epoch: 204, loss: 0.71\n",
      "train epoch: 205, loss 0.1463\n",
      "test epoch: 205, loss: 0.71\n",
      "train epoch: 206, loss 0.1463\n",
      "test epoch: 206, loss: 0.72\n",
      "train epoch: 207, loss 0.1462\n",
      "test epoch: 207, loss: 0.72\n",
      "train epoch: 208, loss 0.1462\n",
      "test epoch: 208, loss: 0.72\n",
      "train epoch: 209, loss 0.1462\n",
      "test epoch: 209, loss: 0.72\n",
      "train epoch: 210, loss 0.1461\n",
      "test epoch: 210, loss: 0.72\n",
      "train epoch: 211, loss 0.1461\n",
      "test epoch: 211, loss: 0.72\n",
      "train epoch: 212, loss 0.1461\n",
      "test epoch: 212, loss: 0.72\n",
      "train epoch: 213, loss 0.1460\n",
      "test epoch: 213, loss: 0.72\n",
      "train epoch: 214, loss 0.1460\n",
      "test epoch: 214, loss: 0.72\n",
      "train epoch: 215, loss 0.1460\n",
      "test epoch: 215, loss: 0.72\n",
      "train epoch: 216, loss 0.1459\n",
      "test epoch: 216, loss: 0.72\n",
      "train epoch: 217, loss 0.1459\n",
      "test epoch: 217, loss: 0.72\n",
      "train epoch: 218, loss 0.1459\n",
      "test epoch: 218, loss: 0.72\n",
      "train epoch: 219, loss 0.1459\n",
      "test epoch: 219, loss: 0.72\n",
      "train epoch: 220, loss 0.1458\n",
      "test epoch: 220, loss: 0.72\n",
      "train epoch: 221, loss 0.1458\n",
      "test epoch: 221, loss: 0.73\n",
      "train epoch: 222, loss 0.1458\n",
      "test epoch: 222, loss: 0.73\n",
      "train epoch: 223, loss 0.1457\n",
      "test epoch: 223, loss: 0.73\n",
      "train epoch: 224, loss 0.1457\n",
      "test epoch: 224, loss: 0.73\n",
      "train epoch: 225, loss 0.1457\n",
      "test epoch: 225, loss: 0.73\n",
      "train epoch: 226, loss 0.1457\n",
      "test epoch: 226, loss: 0.73\n",
      "train epoch: 227, loss 0.1456\n",
      "test epoch: 227, loss: 0.73\n",
      "train epoch: 228, loss 0.1456\n",
      "test epoch: 228, loss: 0.73\n",
      "train epoch: 229, loss 0.1456\n",
      "test epoch: 229, loss: 0.73\n",
      "train epoch: 230, loss 0.1455\n",
      "test epoch: 230, loss: 0.73\n",
      "train epoch: 231, loss 0.1455\n",
      "test epoch: 231, loss: 0.73\n",
      "train epoch: 232, loss 0.1455\n",
      "test epoch: 232, loss: 0.73\n",
      "train epoch: 233, loss 0.1454\n",
      "test epoch: 233, loss: 0.73\n",
      "train epoch: 234, loss 0.1454\n",
      "test epoch: 234, loss: 0.73\n",
      "train epoch: 235, loss 0.1454\n",
      "test epoch: 235, loss: 0.73\n",
      "train epoch: 236, loss 0.1453\n",
      "test epoch: 236, loss: 0.73\n",
      "train epoch: 237, loss 0.1453\n",
      "test epoch: 237, loss: 0.73\n",
      "train epoch: 238, loss 0.1453\n",
      "test epoch: 238, loss: 0.73\n",
      "train epoch: 239, loss 0.1452\n",
      "test epoch: 239, loss: 0.73\n",
      "train epoch: 240, loss 0.1452\n",
      "test epoch: 240, loss: 0.73\n",
      "train epoch: 241, loss 0.1452\n",
      "test epoch: 241, loss: 0.74\n",
      "train epoch: 242, loss 0.1451\n",
      "test epoch: 242, loss: 0.74\n",
      "train epoch: 243, loss 0.1451\n",
      "test epoch: 243, loss: 0.74\n",
      "train epoch: 244, loss 0.1450\n",
      "test epoch: 244, loss: 0.74\n",
      "train epoch: 245, loss 0.1450\n",
      "test epoch: 245, loss: 0.74\n",
      "train epoch: 246, loss 0.1449\n",
      "test epoch: 246, loss: 0.74\n",
      "train epoch: 247, loss 0.1449\n",
      "test epoch: 247, loss: 0.74\n",
      "train epoch: 248, loss 0.1448\n",
      "test epoch: 248, loss: 0.74\n",
      "train epoch: 249, loss 0.1448\n",
      "test epoch: 249, loss: 0.74\n",
      "train epoch: 250, loss 0.1447\n",
      "test epoch: 250, loss: 0.74\n",
      "train epoch: 251, loss 0.1447\n",
      "test epoch: 251, loss: 0.74\n",
      "train epoch: 252, loss 0.1446\n",
      "test epoch: 252, loss: 0.74\n",
      "train epoch: 253, loss 0.1445\n",
      "test epoch: 253, loss: 0.74\n",
      "train epoch: 254, loss 0.1445\n",
      "test epoch: 254, loss: 0.74\n",
      "train epoch: 255, loss 0.1444\n",
      "test epoch: 255, loss: 0.74\n",
      "train epoch: 256, loss 0.1444\n",
      "test epoch: 256, loss: 0.74\n",
      "train epoch: 257, loss 0.1443\n",
      "test epoch: 257, loss: 0.74\n",
      "train epoch: 258, loss 0.1442\n",
      "test epoch: 258, loss: 0.74\n",
      "train epoch: 259, loss 0.1442\n",
      "test epoch: 259, loss: 0.74\n",
      "train epoch: 260, loss 0.1441\n",
      "test epoch: 260, loss: 0.74\n",
      "train epoch: 261, loss 0.1441\n",
      "test epoch: 261, loss: 0.74\n",
      "train epoch: 262, loss 0.1441\n",
      "test epoch: 262, loss: 0.74\n",
      "train epoch: 263, loss 0.1440\n",
      "test epoch: 263, loss: 0.74\n",
      "train epoch: 264, loss 0.1440\n",
      "test epoch: 264, loss: 0.74\n",
      "train epoch: 265, loss 0.1439\n",
      "test epoch: 265, loss: 0.74\n",
      "train epoch: 266, loss 0.1439\n",
      "test epoch: 266, loss: 0.74\n",
      "train epoch: 267, loss 0.1438\n",
      "test epoch: 267, loss: 0.74\n",
      "train epoch: 268, loss 0.1438\n",
      "test epoch: 268, loss: 0.74\n",
      "train epoch: 269, loss 0.1437\n",
      "test epoch: 269, loss: 0.74\n",
      "train epoch: 270, loss 0.1437\n",
      "test epoch: 270, loss: 0.74\n",
      "train epoch: 271, loss 0.1436\n",
      "test epoch: 271, loss: 0.75\n",
      "train epoch: 272, loss 0.1436\n",
      "test epoch: 272, loss: 0.74\n",
      "train epoch: 273, loss 0.1436\n",
      "test epoch: 273, loss: 0.75\n",
      "train epoch: 274, loss 0.1435\n",
      "test epoch: 274, loss: 0.74\n",
      "train epoch: 275, loss 0.1435\n",
      "test epoch: 275, loss: 0.75\n",
      "train epoch: 276, loss 0.1434\n",
      "test epoch: 276, loss: 0.74\n",
      "train epoch: 277, loss 0.1434\n",
      "test epoch: 277, loss: 0.75\n",
      "train epoch: 278, loss 0.1433\n",
      "test epoch: 278, loss: 0.74\n",
      "train epoch: 279, loss 0.1433\n",
      "test epoch: 279, loss: 0.75\n",
      "train epoch: 280, loss 0.1432\n",
      "test epoch: 280, loss: 0.74\n",
      "train epoch: 281, loss 0.1432\n",
      "test epoch: 281, loss: 0.75\n",
      "train epoch: 282, loss 0.1431\n",
      "test epoch: 282, loss: 0.74\n",
      "train epoch: 283, loss 0.1430\n",
      "test epoch: 283, loss: 0.75\n",
      "train epoch: 284, loss 0.1430\n",
      "test epoch: 284, loss: 0.73\n",
      "train epoch: 285, loss 0.1429\n",
      "test epoch: 285, loss: 0.75\n",
      "train epoch: 286, loss 0.1429\n",
      "test epoch: 286, loss: 0.74\n",
      "train epoch: 287, loss 0.1428\n",
      "test epoch: 287, loss: 0.75\n",
      "train epoch: 288, loss 0.1427\n",
      "test epoch: 288, loss: 0.74\n",
      "train epoch: 289, loss 0.1426\n",
      "test epoch: 289, loss: 0.75\n",
      "train epoch: 290, loss 0.1425\n",
      "test epoch: 290, loss: 0.75\n",
      "train epoch: 291, loss 0.1424\n",
      "test epoch: 291, loss: 0.75\n",
      "train epoch: 292, loss 0.1423\n",
      "test epoch: 292, loss: 0.75\n",
      "train epoch: 293, loss 0.1422\n",
      "test epoch: 293, loss: 0.75\n",
      "train epoch: 294, loss 0.1422\n",
      "test epoch: 294, loss: 0.75\n",
      "train epoch: 295, loss 0.1421\n",
      "test epoch: 295, loss: 0.74\n",
      "train epoch: 296, loss 0.1420\n",
      "test epoch: 296, loss: 0.76\n",
      "train epoch: 297, loss 0.1419\n",
      "test epoch: 297, loss: 0.74\n",
      "train epoch: 298, loss 0.1419\n",
      "test epoch: 298, loss: 0.76\n",
      "train epoch: 299, loss 0.1418\n",
      "test epoch: 299, loss: 0.74\n",
      "train epoch: 300, loss 0.1418\n",
      "test epoch: 300, loss: 0.76\n",
      "train epoch: 301, loss 0.1418\n",
      "test epoch: 301, loss: 0.73\n",
      "train epoch: 302, loss 0.1418\n",
      "test epoch: 302, loss: 0.77\n",
      "train epoch: 303, loss 0.1418\n",
      "test epoch: 303, loss: 0.73\n",
      "train epoch: 304, loss 0.1417\n",
      "test epoch: 304, loss: 0.77\n",
      "train epoch: 305, loss 0.1416\n",
      "test epoch: 305, loss: 0.74\n",
      "train epoch: 306, loss 0.1414\n",
      "test epoch: 306, loss: 0.76\n",
      "train epoch: 307, loss 0.1412\n",
      "test epoch: 307, loss: 0.76\n",
      "train epoch: 308, loss 0.1412\n",
      "test epoch: 308, loss: 0.74\n",
      "train epoch: 309, loss 0.1412\n",
      "test epoch: 309, loss: 0.77\n",
      "train epoch: 310, loss 0.1412\n",
      "test epoch: 310, loss: 0.74\n",
      "train epoch: 311, loss 0.1411\n",
      "test epoch: 311, loss: 0.76\n",
      "train epoch: 312, loss 0.1410\n",
      "test epoch: 312, loss: 0.75\n",
      "train epoch: 313, loss 0.1409\n",
      "test epoch: 313, loss: 0.76\n",
      "train epoch: 314, loss 0.1408\n",
      "test epoch: 314, loss: 0.77\n",
      "train epoch: 315, loss 0.1408\n",
      "test epoch: 315, loss: 0.75\n",
      "train epoch: 316, loss 0.1408\n",
      "test epoch: 316, loss: 0.77\n",
      "train epoch: 317, loss 0.1408\n",
      "test epoch: 317, loss: 0.75\n",
      "train epoch: 318, loss 0.1407\n",
      "test epoch: 318, loss: 0.77\n",
      "train epoch: 319, loss 0.1406\n",
      "test epoch: 319, loss: 0.76\n",
      "train epoch: 320, loss 0.1405\n",
      "test epoch: 320, loss: 0.76\n",
      "train epoch: 321, loss 0.1404\n",
      "test epoch: 321, loss: 0.77\n",
      "train epoch: 322, loss 0.1404\n",
      "test epoch: 322, loss: 0.76\n",
      "train epoch: 323, loss 0.1403\n",
      "test epoch: 323, loss: 0.77\n",
      "train epoch: 324, loss 0.1403\n",
      "test epoch: 324, loss: 0.76\n",
      "train epoch: 325, loss 0.1402\n",
      "test epoch: 325, loss: 0.77\n",
      "train epoch: 326, loss 0.1401\n",
      "test epoch: 326, loss: 0.76\n",
      "train epoch: 327, loss 0.1401\n",
      "test epoch: 327, loss: 0.77\n",
      "train epoch: 328, loss 0.1400\n",
      "test epoch: 328, loss: 0.77\n",
      "train epoch: 329, loss 0.1399\n",
      "test epoch: 329, loss: 0.77\n",
      "train epoch: 330, loss 0.1398\n",
      "test epoch: 330, loss: 0.77\n",
      "train epoch: 331, loss 0.1398\n",
      "test epoch: 331, loss: 0.76\n",
      "train epoch: 332, loss 0.1397\n",
      "test epoch: 332, loss: 0.77\n",
      "train epoch: 333, loss 0.1397\n",
      "test epoch: 333, loss: 0.75\n",
      "train epoch: 334, loss 0.1397\n",
      "test epoch: 334, loss: 0.78\n",
      "train epoch: 335, loss 0.1396\n",
      "test epoch: 335, loss: 0.76\n",
      "train epoch: 336, loss 0.1397\n",
      "test epoch: 336, loss: 0.79\n",
      "train epoch: 337, loss 0.1397\n",
      "test epoch: 337, loss: 0.76\n",
      "train epoch: 338, loss 0.1397\n",
      "test epoch: 338, loss: 0.79\n",
      "train epoch: 339, loss 0.1396\n",
      "test epoch: 339, loss: 0.77\n",
      "train epoch: 340, loss 0.1394\n",
      "test epoch: 340, loss: 0.78\n",
      "train epoch: 341, loss 0.1393\n",
      "test epoch: 341, loss: 0.78\n",
      "train epoch: 342, loss 0.1393\n",
      "test epoch: 342, loss: 0.77\n",
      "train epoch: 343, loss 0.1392\n",
      "test epoch: 343, loss: 0.79\n",
      "train epoch: 344, loss 0.1392\n",
      "test epoch: 344, loss: 0.77\n",
      "train epoch: 345, loss 0.1391\n",
      "test epoch: 345, loss: 0.79\n",
      "train epoch: 346, loss 0.1391\n",
      "test epoch: 346, loss: 0.77\n",
      "train epoch: 347, loss 0.1390\n",
      "test epoch: 347, loss: 0.79\n",
      "train epoch: 348, loss 0.1388\n",
      "test epoch: 348, loss: 0.78\n",
      "train epoch: 349, loss 0.1387\n",
      "test epoch: 349, loss: 0.79\n",
      "train epoch: 350, loss 0.1385\n",
      "test epoch: 350, loss: 0.78\n",
      "train epoch: 351, loss 0.1384\n",
      "test epoch: 351, loss: 0.78\n",
      "train epoch: 352, loss 0.1383\n",
      "test epoch: 352, loss: 0.79\n",
      "train epoch: 353, loss 0.1382\n",
      "test epoch: 353, loss: 0.77\n",
      "train epoch: 354, loss 0.1382\n",
      "test epoch: 354, loss: 0.79\n",
      "train epoch: 355, loss 0.1381\n",
      "test epoch: 355, loss: 0.76\n",
      "train epoch: 356, loss 0.1380\n",
      "test epoch: 356, loss: 0.77\n",
      "train epoch: 357, loss 0.1387\n",
      "test epoch: 357, loss: 0.76\n",
      "train epoch: 358, loss 0.1387\n",
      "test epoch: 358, loss: 0.76\n",
      "train epoch: 359, loss 0.1386\n",
      "test epoch: 359, loss: 0.78\n",
      "train epoch: 360, loss 0.1386\n",
      "test epoch: 360, loss: 0.75\n",
      "train epoch: 361, loss 0.1387\n",
      "test epoch: 361, loss: 0.80\n",
      "train epoch: 362, loss 0.1394\n",
      "test epoch: 362, loss: 0.72\n",
      "train epoch: 363, loss 0.1409\n",
      "test epoch: 363, loss: 0.84\n",
      "train epoch: 364, loss 0.1410\n",
      "test epoch: 364, loss: 0.75\n",
      "train epoch: 365, loss 0.1392\n",
      "test epoch: 365, loss: 0.79\n",
      "train epoch: 366, loss 0.1374\n",
      "test epoch: 366, loss: 0.84\n",
      "train epoch: 367, loss 0.1391\n",
      "test epoch: 367, loss: 0.75\n",
      "train epoch: 368, loss 0.1394\n",
      "test epoch: 368, loss: 0.81\n",
      "train epoch: 369, loss 0.1375\n",
      "test epoch: 369, loss: 0.83\n",
      "train epoch: 370, loss 0.1381\n",
      "test epoch: 370, loss: 0.76\n",
      "train epoch: 371, loss 0.1388\n",
      "test epoch: 371, loss: 0.81\n",
      "train epoch: 372, loss 0.1374\n",
      "test epoch: 372, loss: 0.82\n",
      "train epoch: 373, loss 0.1376\n",
      "test epoch: 373, loss: 0.77\n",
      "train epoch: 374, loss 0.1382\n",
      "test epoch: 374, loss: 0.81\n",
      "train epoch: 375, loss 0.1371\n",
      "test epoch: 375, loss: 0.82\n",
      "train epoch: 376, loss 0.1371\n",
      "test epoch: 376, loss: 0.77\n",
      "train epoch: 377, loss 0.1375\n",
      "test epoch: 377, loss: 0.81\n",
      "train epoch: 378, loss 0.1367\n",
      "test epoch: 378, loss: 0.82\n",
      "train epoch: 379, loss 0.1366\n",
      "test epoch: 379, loss: 0.78\n",
      "train epoch: 380, loss 0.1369\n",
      "test epoch: 380, loss: 0.81\n",
      "train epoch: 381, loss 0.1362\n",
      "test epoch: 381, loss: 0.81\n",
      "train epoch: 382, loss 0.1362\n",
      "test epoch: 382, loss: 0.78\n",
      "train epoch: 383, loss 0.1363\n",
      "test epoch: 383, loss: 0.80\n",
      "train epoch: 384, loss 0.1359\n",
      "test epoch: 384, loss: 0.80\n",
      "train epoch: 385, loss 0.1358\n",
      "test epoch: 385, loss: 0.76\n",
      "train epoch: 386, loss 0.1358\n",
      "test epoch: 386, loss: 0.77\n",
      "train epoch: 387, loss 0.1363\n",
      "test epoch: 387, loss: 0.81\n",
      "train epoch: 388, loss 0.1359\n",
      "test epoch: 388, loss: 0.77\n",
      "train epoch: 389, loss 0.1361\n",
      "test epoch: 389, loss: 0.80\n",
      "train epoch: 390, loss 0.1355\n",
      "test epoch: 390, loss: 0.82\n",
      "train epoch: 391, loss 0.1359\n",
      "test epoch: 391, loss: 0.79\n",
      "train epoch: 392, loss 0.1361\n",
      "test epoch: 392, loss: 0.81\n",
      "train epoch: 393, loss 0.1357\n",
      "test epoch: 393, loss: 0.82\n",
      "train epoch: 394, loss 0.1358\n",
      "test epoch: 394, loss: 0.79\n",
      "train epoch: 395, loss 0.1359\n",
      "test epoch: 395, loss: 0.82\n",
      "train epoch: 396, loss 0.1355\n",
      "test epoch: 396, loss: 0.83\n",
      "train epoch: 397, loss 0.1355\n",
      "test epoch: 397, loss: 0.80\n",
      "train epoch: 398, loss 0.1356\n",
      "test epoch: 398, loss: 0.82\n",
      "train epoch: 399, loss 0.1353\n",
      "test epoch: 399, loss: 0.82\n",
      "train epoch: 400, loss 0.1352\n",
      "test epoch: 400, loss: 0.80\n",
      "train epoch: 401, loss 0.1353\n",
      "test epoch: 401, loss: 0.82\n",
      "train epoch: 402, loss 0.1350\n",
      "test epoch: 402, loss: 0.82\n",
      "train epoch: 403, loss 0.1349\n",
      "test epoch: 403, loss: 0.80\n",
      "train epoch: 404, loss 0.1349\n",
      "test epoch: 404, loss: 0.81\n",
      "train epoch: 405, loss 0.1347\n",
      "test epoch: 405, loss: 0.81\n",
      "train epoch: 406, loss 0.1345\n",
      "test epoch: 406, loss: 0.80\n",
      "train epoch: 407, loss 0.1345\n",
      "test epoch: 407, loss: 0.82\n",
      "train epoch: 408, loss 0.1344\n",
      "test epoch: 408, loss: 0.80\n",
      "train epoch: 409, loss 0.1342\n",
      "test epoch: 409, loss: 0.80\n",
      "train epoch: 410, loss 0.1341\n",
      "test epoch: 410, loss: 0.81\n",
      "train epoch: 411, loss 0.1340\n",
      "test epoch: 411, loss: 0.80\n",
      "train epoch: 412, loss 0.1339\n",
      "test epoch: 412, loss: 0.80\n",
      "train epoch: 413, loss 0.1338\n",
      "test epoch: 413, loss: 0.81\n",
      "train epoch: 414, loss 0.1337\n",
      "test epoch: 414, loss: 0.81\n",
      "train epoch: 415, loss 0.1336\n",
      "test epoch: 415, loss: 0.81\n",
      "train epoch: 416, loss 0.1335\n",
      "test epoch: 416, loss: 0.81\n",
      "train epoch: 417, loss 0.1335\n",
      "test epoch: 417, loss: 0.80\n",
      "train epoch: 418, loss 0.1334\n",
      "test epoch: 418, loss: 0.81\n",
      "train epoch: 419, loss 0.1332\n",
      "test epoch: 419, loss: 0.81\n",
      "train epoch: 420, loss 0.1332\n",
      "test epoch: 420, loss: 0.80\n",
      "train epoch: 421, loss 0.1331\n",
      "test epoch: 421, loss: 0.82\n",
      "train epoch: 422, loss 0.1330\n",
      "test epoch: 422, loss: 0.81\n",
      "train epoch: 423, loss 0.1329\n",
      "test epoch: 423, loss: 0.81\n",
      "train epoch: 424, loss 0.1328\n",
      "test epoch: 424, loss: 0.82\n",
      "train epoch: 425, loss 0.1328\n",
      "test epoch: 425, loss: 0.81\n",
      "train epoch: 426, loss 0.1326\n",
      "test epoch: 426, loss: 0.81\n",
      "train epoch: 427, loss 0.1326\n",
      "test epoch: 427, loss: 0.81\n",
      "train epoch: 428, loss 0.1325\n",
      "test epoch: 428, loss: 0.81\n",
      "train epoch: 429, loss 0.1324\n",
      "test epoch: 429, loss: 0.82\n",
      "train epoch: 430, loss 0.1324\n",
      "test epoch: 430, loss: 0.82\n",
      "train epoch: 431, loss 0.1322\n",
      "test epoch: 431, loss: 0.81\n",
      "train epoch: 432, loss 0.1322\n",
      "test epoch: 432, loss: 0.82\n",
      "train epoch: 433, loss 0.1320\n",
      "test epoch: 433, loss: 0.82\n",
      "train epoch: 434, loss 0.1320\n",
      "test epoch: 434, loss: 0.82\n",
      "train epoch: 435, loss 0.1319\n",
      "test epoch: 435, loss: 0.82\n",
      "train epoch: 436, loss 0.1317\n",
      "test epoch: 436, loss: 0.82\n",
      "train epoch: 437, loss 0.1317\n",
      "test epoch: 437, loss: 0.82\n",
      "train epoch: 438, loss 0.1315\n",
      "test epoch: 438, loss: 0.82\n",
      "train epoch: 439, loss 0.1315\n",
      "test epoch: 439, loss: 0.82\n",
      "train epoch: 440, loss 0.1314\n",
      "test epoch: 440, loss: 0.82\n",
      "train epoch: 441, loss 0.1313\n",
      "test epoch: 441, loss: 0.82\n",
      "train epoch: 442, loss 0.1312\n",
      "test epoch: 442, loss: 0.82\n",
      "train epoch: 443, loss 0.1311\n",
      "test epoch: 443, loss: 0.83\n",
      "train epoch: 444, loss 0.1310\n",
      "test epoch: 444, loss: 0.82\n",
      "train epoch: 445, loss 0.1309\n",
      "test epoch: 445, loss: 0.82\n",
      "train epoch: 446, loss 0.1308\n",
      "test epoch: 446, loss: 0.83\n",
      "train epoch: 447, loss 0.1307\n",
      "test epoch: 447, loss: 0.82\n",
      "train epoch: 448, loss 0.1307\n",
      "test epoch: 448, loss: 0.83\n",
      "train epoch: 449, loss 0.1306\n",
      "test epoch: 449, loss: 0.82\n",
      "train epoch: 450, loss 0.1305\n",
      "test epoch: 450, loss: 0.83\n",
      "train epoch: 451, loss 0.1303\n",
      "test epoch: 451, loss: 0.83\n",
      "train epoch: 452, loss 0.1302\n",
      "test epoch: 452, loss: 0.83\n",
      "train epoch: 453, loss 0.1301\n",
      "test epoch: 453, loss: 0.83\n",
      "train epoch: 454, loss 0.1300\n",
      "test epoch: 454, loss: 0.83\n",
      "train epoch: 455, loss 0.1299\n",
      "test epoch: 455, loss: 0.84\n",
      "train epoch: 456, loss 0.1298\n",
      "test epoch: 456, loss: 0.83\n",
      "train epoch: 457, loss 0.1297\n",
      "test epoch: 457, loss: 0.84\n",
      "train epoch: 458, loss 0.1296\n",
      "test epoch: 458, loss: 0.83\n",
      "train epoch: 459, loss 0.1295\n",
      "test epoch: 459, loss: 0.84\n",
      "train epoch: 460, loss 0.1294\n",
      "test epoch: 460, loss: 0.84\n",
      "train epoch: 461, loss 0.1293\n",
      "test epoch: 461, loss: 0.83\n",
      "train epoch: 462, loss 0.1292\n",
      "test epoch: 462, loss: 0.84\n",
      "train epoch: 463, loss 0.1291\n",
      "test epoch: 463, loss: 0.84\n",
      "train epoch: 464, loss 0.1290\n",
      "test epoch: 464, loss: 0.84\n",
      "train epoch: 465, loss 0.1289\n",
      "test epoch: 465, loss: 0.83\n",
      "train epoch: 466, loss 0.1288\n",
      "test epoch: 466, loss: 0.84\n",
      "train epoch: 467, loss 0.1288\n",
      "test epoch: 467, loss: 0.83\n",
      "train epoch: 468, loss 0.1287\n",
      "test epoch: 468, loss: 0.85\n",
      "train epoch: 469, loss 0.1286\n",
      "test epoch: 469, loss: 0.83\n",
      "train epoch: 470, loss 0.1285\n",
      "test epoch: 470, loss: 0.85\n",
      "train epoch: 471, loss 0.1285\n",
      "test epoch: 471, loss: 0.82\n",
      "train epoch: 472, loss 0.1287\n",
      "test epoch: 472, loss: 0.87\n",
      "train epoch: 473, loss 0.1287\n",
      "test epoch: 473, loss: 0.82\n",
      "train epoch: 474, loss 0.1289\n",
      "test epoch: 474, loss: 0.88\n",
      "train epoch: 475, loss 0.1290\n",
      "test epoch: 475, loss: 0.81\n",
      "train epoch: 476, loss 0.1287\n",
      "test epoch: 476, loss: 0.87\n",
      "train epoch: 477, loss 0.1289\n",
      "test epoch: 477, loss: 0.82\n",
      "train epoch: 478, loss 0.1283\n",
      "test epoch: 478, loss: 0.87\n",
      "train epoch: 479, loss 0.1280\n",
      "test epoch: 479, loss: 0.84\n",
      "train epoch: 480, loss 0.1276\n",
      "test epoch: 480, loss: 0.85\n",
      "train epoch: 481, loss 0.1272\n",
      "test epoch: 481, loss: 0.85\n",
      "train epoch: 482, loss 0.1271\n",
      "test epoch: 482, loss: 0.84\n",
      "train epoch: 483, loss 0.1270\n",
      "test epoch: 483, loss: 0.86\n",
      "train epoch: 484, loss 0.1270\n",
      "test epoch: 484, loss: 0.84\n",
      "train epoch: 485, loss 0.1269\n",
      "test epoch: 485, loss: 0.86\n",
      "train epoch: 486, loss 0.1269\n",
      "test epoch: 486, loss: 0.83\n",
      "train epoch: 487, loss 0.1267\n",
      "test epoch: 487, loss: 0.86\n",
      "train epoch: 488, loss 0.1266\n",
      "test epoch: 488, loss: 0.84\n",
      "train epoch: 489, loss 0.1265\n",
      "test epoch: 489, loss: 0.86\n",
      "train epoch: 490, loss 0.1263\n",
      "test epoch: 490, loss: 0.84\n",
      "train epoch: 491, loss 0.1262\n",
      "test epoch: 491, loss: 0.86\n",
      "train epoch: 492, loss 0.1261\n",
      "test epoch: 492, loss: 0.84\n",
      "train epoch: 493, loss 0.1260\n",
      "test epoch: 493, loss: 0.86\n",
      "train epoch: 494, loss 0.1259\n",
      "test epoch: 494, loss: 0.85\n",
      "train epoch: 495, loss 0.1257\n",
      "test epoch: 495, loss: 0.86\n",
      "train epoch: 496, loss 0.1256\n",
      "test epoch: 496, loss: 0.85\n",
      "train epoch: 497, loss 0.1255\n",
      "test epoch: 497, loss: 0.86\n",
      "train epoch: 498, loss 0.1254\n",
      "test epoch: 498, loss: 0.85\n",
      "train epoch: 499, loss 0.1253\n",
      "test epoch: 499, loss: 0.87\n",
      "train epoch: 500, loss 0.1252\n",
      "test epoch: 500, loss: 0.84\n",
      "train epoch: 501, loss 0.1252\n",
      "test epoch: 501, loss: 0.88\n",
      "train epoch: 502, loss 0.1253\n",
      "test epoch: 502, loss: 0.83\n",
      "train epoch: 503, loss 0.1256\n",
      "test epoch: 503, loss: 0.90\n",
      "train epoch: 504, loss 0.1261\n",
      "test epoch: 504, loss: 0.81\n",
      "train epoch: 505, loss 0.1272\n",
      "test epoch: 505, loss: 0.92\n",
      "train epoch: 506, loss 0.1285\n",
      "test epoch: 506, loss: 0.79\n",
      "train epoch: 507, loss 0.1294\n",
      "test epoch: 507, loss: 0.92\n",
      "train epoch: 508, loss 0.1283\n",
      "test epoch: 508, loss: 0.82\n",
      "train epoch: 509, loss 0.1255\n",
      "test epoch: 509, loss: 0.86\n",
      "train epoch: 510, loss 0.1240\n",
      "test epoch: 510, loss: 0.89\n",
      "train epoch: 511, loss 0.1251\n",
      "test epoch: 511, loss: 0.81\n",
      "train epoch: 512, loss 0.1266\n",
      "test epoch: 512, loss: 0.91\n",
      "train epoch: 513, loss 0.1261\n",
      "test epoch: 513, loss: 0.83\n",
      "train epoch: 514, loss 0.1242\n",
      "test epoch: 514, loss: 0.85\n",
      "train epoch: 515, loss 0.1236\n",
      "test epoch: 515, loss: 0.89\n",
      "train epoch: 516, loss 0.1246\n",
      "test epoch: 516, loss: 0.82\n",
      "train epoch: 517, loss 0.1251\n",
      "test epoch: 517, loss: 0.89\n",
      "train epoch: 518, loss 0.1242\n",
      "test epoch: 518, loss: 0.85\n",
      "train epoch: 519, loss 0.1231\n",
      "test epoch: 519, loss: 0.84\n",
      "train epoch: 520, loss 0.1234\n",
      "test epoch: 520, loss: 0.90\n",
      "train epoch: 521, loss 0.1240\n",
      "test epoch: 521, loss: 0.83\n",
      "train epoch: 522, loss 0.1237\n",
      "test epoch: 522, loss: 0.88\n",
      "train epoch: 523, loss 0.1229\n",
      "test epoch: 523, loss: 0.86\n",
      "train epoch: 524, loss 0.1226\n",
      "test epoch: 524, loss: 0.84\n",
      "train epoch: 525, loss 0.1230\n",
      "test epoch: 525, loss: 0.90\n",
      "train epoch: 526, loss 0.1234\n",
      "test epoch: 526, loss: 0.84\n",
      "train epoch: 527, loss 0.1229\n",
      "test epoch: 527, loss: 0.87\n",
      "train epoch: 528, loss 0.1222\n",
      "test epoch: 528, loss: 0.87\n",
      "train epoch: 529, loss 0.1222\n",
      "test epoch: 529, loss: 0.84\n",
      "train epoch: 530, loss 0.1225\n",
      "test epoch: 530, loss: 0.89\n",
      "train epoch: 531, loss 0.1226\n",
      "test epoch: 531, loss: 0.85\n",
      "train epoch: 532, loss 0.1220\n",
      "test epoch: 532, loss: 0.87\n",
      "train epoch: 533, loss 0.1216\n",
      "test epoch: 533, loss: 0.88\n",
      "train epoch: 534, loss 0.1217\n",
      "test epoch: 534, loss: 0.85\n",
      "train epoch: 535, loss 0.1218\n",
      "test epoch: 535, loss: 0.89\n",
      "train epoch: 536, loss 0.1218\n",
      "test epoch: 536, loss: 0.85\n",
      "train epoch: 537, loss 0.1214\n",
      "test epoch: 537, loss: 0.87\n",
      "train epoch: 538, loss 0.1211\n",
      "test epoch: 538, loss: 0.87\n",
      "train epoch: 539, loss 0.1211\n",
      "test epoch: 539, loss: 0.85\n",
      "train epoch: 540, loss 0.1210\n",
      "test epoch: 540, loss: 0.89\n",
      "train epoch: 541, loss 0.1210\n",
      "test epoch: 541, loss: 0.86\n",
      "train epoch: 542, loss 0.1208\n",
      "test epoch: 542, loss: 0.88\n",
      "train epoch: 543, loss 0.1205\n",
      "test epoch: 543, loss: 0.87\n",
      "train epoch: 544, loss 0.1204\n",
      "test epoch: 544, loss: 0.86\n",
      "train epoch: 545, loss 0.1203\n",
      "test epoch: 545, loss: 0.88\n",
      "train epoch: 546, loss 0.1202\n",
      "test epoch: 546, loss: 0.86\n",
      "train epoch: 547, loss 0.1201\n",
      "test epoch: 547, loss: 0.88\n",
      "train epoch: 548, loss 0.1200\n",
      "test epoch: 548, loss: 0.86\n",
      "train epoch: 549, loss 0.1198\n",
      "test epoch: 549, loss: 0.87\n",
      "train epoch: 550, loss 0.1197\n",
      "test epoch: 550, loss: 0.87\n",
      "train epoch: 551, loss 0.1196\n",
      "test epoch: 551, loss: 0.86\n",
      "train epoch: 552, loss 0.1195\n",
      "test epoch: 552, loss: 0.88\n",
      "train epoch: 553, loss 0.1194\n",
      "test epoch: 553, loss: 0.86\n",
      "train epoch: 554, loss 0.1193\n",
      "test epoch: 554, loss: 0.88\n",
      "train epoch: 555, loss 0.1192\n",
      "test epoch: 555, loss: 0.87\n",
      "train epoch: 556, loss 0.1190\n",
      "test epoch: 556, loss: 0.88\n",
      "train epoch: 557, loss 0.1189\n",
      "test epoch: 557, loss: 0.87\n",
      "train epoch: 558, loss 0.1188\n",
      "test epoch: 558, loss: 0.87\n",
      "train epoch: 559, loss 0.1186\n",
      "test epoch: 559, loss: 0.87\n",
      "train epoch: 560, loss 0.1185\n",
      "test epoch: 560, loss: 0.87\n",
      "train epoch: 561, loss 0.1184\n",
      "test epoch: 561, loss: 0.88\n",
      "train epoch: 562, loss 0.1183\n",
      "test epoch: 562, loss: 0.87\n",
      "train epoch: 563, loss 0.1182\n",
      "test epoch: 563, loss: 0.88\n",
      "train epoch: 564, loss 0.1181\n",
      "test epoch: 564, loss: 0.87\n",
      "train epoch: 565, loss 0.1180\n",
      "test epoch: 565, loss: 0.88\n",
      "train epoch: 566, loss 0.1179\n",
      "test epoch: 566, loss: 0.87\n",
      "train epoch: 567, loss 0.1178\n",
      "test epoch: 567, loss: 0.88\n",
      "train epoch: 568, loss 0.1177\n",
      "test epoch: 568, loss: 0.86\n",
      "train epoch: 569, loss 0.1176\n",
      "test epoch: 569, loss: 0.89\n",
      "train epoch: 570, loss 0.1176\n",
      "test epoch: 570, loss: 0.86\n",
      "train epoch: 571, loss 0.1176\n",
      "test epoch: 571, loss: 0.89\n",
      "train epoch: 572, loss 0.1176\n",
      "test epoch: 572, loss: 0.85\n",
      "train epoch: 573, loss 0.1178\n",
      "test epoch: 573, loss: 0.91\n",
      "train epoch: 574, loss 0.1182\n",
      "test epoch: 574, loss: 0.83\n",
      "train epoch: 575, loss 0.1191\n",
      "test epoch: 575, loss: 0.93\n",
      "train epoch: 576, loss 0.1208\n",
      "test epoch: 576, loss: 0.80\n",
      "train epoch: 577, loss 0.1239\n",
      "test epoch: 577, loss: 0.97\n",
      "train epoch: 578, loss 0.1273\n",
      "test epoch: 578, loss: 0.77\n",
      "train epoch: 579, loss 0.1274\n",
      "test epoch: 579, loss: 0.94\n",
      "train epoch: 580, loss 0.1229\n",
      "test epoch: 580, loss: 0.84\n",
      "train epoch: 581, loss 0.1169\n",
      "test epoch: 581, loss: 0.84\n",
      "train epoch: 582, loss 0.1173\n",
      "test epoch: 582, loss: 0.95\n",
      "train epoch: 583, loss 0.1220\n",
      "test epoch: 583, loss: 0.80\n",
      "train epoch: 584, loss 0.1227\n",
      "test epoch: 584, loss: 0.92\n",
      "train epoch: 585, loss 0.1184\n",
      "test epoch: 585, loss: 0.87\n",
      "train epoch: 586, loss 0.1158\n",
      "test epoch: 586, loss: 0.82\n",
      "train epoch: 587, loss 0.1183\n",
      "test epoch: 587, loss: 0.94\n",
      "train epoch: 588, loss 0.1207\n",
      "test epoch: 588, loss: 0.82\n",
      "train epoch: 589, loss 0.1181\n",
      "test epoch: 589, loss: 0.87\n",
      "train epoch: 590, loss 0.1154\n",
      "test epoch: 590, loss: 0.91\n",
      "train epoch: 591, loss 0.1169\n",
      "test epoch: 591, loss: 0.82\n",
      "train epoch: 592, loss 0.1188\n",
      "test epoch: 592, loss: 0.92\n",
      "train epoch: 593, loss 0.1173\n",
      "test epoch: 593, loss: 0.86\n",
      "train epoch: 594, loss 0.1150\n",
      "test epoch: 594, loss: 0.84\n",
      "train epoch: 595, loss 0.1157\n",
      "test epoch: 595, loss: 0.92\n",
      "train epoch: 596, loss 0.1174\n",
      "test epoch: 596, loss: 0.83\n",
      "train epoch: 597, loss 0.1163\n",
      "test epoch: 597, loss: 0.87\n",
      "train epoch: 598, loss 0.1147\n",
      "test epoch: 598, loss: 0.89\n",
      "train epoch: 599, loss 0.1150\n",
      "test epoch: 599, loss: 0.83\n",
      "train epoch: 600, loss 0.1162\n",
      "test epoch: 600, loss: 0.91\n",
      "train epoch: 601, loss 0.1158\n",
      "test epoch: 601, loss: 0.86\n",
      "train epoch: 602, loss 0.1144\n",
      "test epoch: 602, loss: 0.86\n",
      "train epoch: 603, loss 0.1141\n",
      "test epoch: 603, loss: 0.90\n",
      "train epoch: 604, loss 0.1150\n",
      "test epoch: 604, loss: 0.84\n",
      "train epoch: 605, loss 0.1150\n",
      "test epoch: 605, loss: 0.89\n",
      "train epoch: 606, loss 0.1140\n",
      "test epoch: 606, loss: 0.88\n",
      "train epoch: 607, loss 0.1136\n",
      "test epoch: 607, loss: 0.85\n",
      "train epoch: 608, loss 0.1140\n",
      "test epoch: 608, loss: 0.90\n",
      "train epoch: 609, loss 0.1142\n",
      "test epoch: 609, loss: 0.85\n",
      "train epoch: 610, loss 0.1137\n",
      "test epoch: 610, loss: 0.88\n",
      "train epoch: 611, loss 0.1131\n",
      "test epoch: 611, loss: 0.88\n",
      "train epoch: 612, loss 0.1131\n",
      "test epoch: 612, loss: 0.85\n",
      "train epoch: 613, loss 0.1134\n",
      "test epoch: 613, loss: 0.89\n",
      "train epoch: 614, loss 0.1132\n",
      "test epoch: 614, loss: 0.86\n",
      "train epoch: 615, loss 0.1129\n",
      "test epoch: 615, loss: 0.87\n",
      "train epoch: 616, loss 0.1126\n",
      "test epoch: 616, loss: 0.88\n",
      "train epoch: 617, loss 0.1126\n",
      "test epoch: 617, loss: 0.85\n",
      "train epoch: 618, loss 0.1127\n",
      "test epoch: 618, loss: 0.88\n",
      "train epoch: 619, loss 0.1125\n",
      "test epoch: 619, loss: 0.87\n",
      "train epoch: 620, loss 0.1122\n",
      "test epoch: 620, loss: 0.87\n",
      "train epoch: 621, loss 0.1121\n",
      "test epoch: 621, loss: 0.88\n",
      "train epoch: 622, loss 0.1121\n",
      "test epoch: 622, loss: 0.85\n",
      "train epoch: 623, loss 0.1122\n",
      "test epoch: 623, loss: 0.89\n",
      "train epoch: 624, loss 0.1121\n",
      "test epoch: 624, loss: 0.86\n",
      "train epoch: 625, loss 0.1118\n",
      "test epoch: 625, loss: 0.88\n",
      "train epoch: 626, loss 0.1116\n",
      "test epoch: 626, loss: 0.89\n",
      "train epoch: 627, loss 0.1116\n",
      "test epoch: 627, loss: 0.86\n",
      "train epoch: 628, loss 0.1117\n",
      "test epoch: 628, loss: 0.89\n",
      "train epoch: 629, loss 0.1116\n",
      "test epoch: 629, loss: 0.86\n",
      "train epoch: 630, loss 0.1114\n",
      "test epoch: 630, loss: 0.88\n",
      "train epoch: 631, loss 0.1111\n",
      "test epoch: 631, loss: 0.88\n",
      "train epoch: 632, loss 0.1109\n",
      "test epoch: 632, loss: 0.88\n",
      "train epoch: 633, loss 0.1108\n",
      "test epoch: 633, loss: 0.89\n",
      "train epoch: 634, loss 0.1108\n",
      "test epoch: 634, loss: 0.86\n",
      "train epoch: 635, loss 0.1108\n",
      "test epoch: 635, loss: 0.89\n",
      "train epoch: 636, loss 0.1108\n",
      "test epoch: 636, loss: 0.85\n",
      "train epoch: 637, loss 0.1108\n",
      "test epoch: 637, loss: 0.89\n",
      "train epoch: 638, loss 0.1108\n",
      "test epoch: 638, loss: 0.86\n",
      "train epoch: 639, loss 0.1106\n",
      "test epoch: 639, loss: 0.90\n",
      "train epoch: 640, loss 0.1105\n",
      "test epoch: 640, loss: 0.86\n",
      "train epoch: 641, loss 0.1104\n",
      "test epoch: 641, loss: 0.89\n",
      "train epoch: 642, loss 0.1102\n",
      "test epoch: 642, loss: 0.87\n",
      "train epoch: 643, loss 0.1100\n",
      "test epoch: 643, loss: 0.89\n",
      "train epoch: 644, loss 0.1099\n",
      "test epoch: 644, loss: 0.87\n",
      "train epoch: 645, loss 0.1097\n",
      "test epoch: 645, loss: 0.89\n",
      "train epoch: 646, loss 0.1096\n",
      "test epoch: 646, loss: 0.87\n",
      "train epoch: 647, loss 0.1095\n",
      "test epoch: 647, loss: 0.89\n",
      "train epoch: 648, loss 0.1095\n",
      "test epoch: 648, loss: 0.86\n",
      "train epoch: 649, loss 0.1095\n",
      "test epoch: 649, loss: 0.90\n",
      "train epoch: 650, loss 0.1096\n",
      "test epoch: 650, loss: 0.85\n",
      "train epoch: 651, loss 0.1097\n",
      "test epoch: 651, loss: 0.91\n",
      "train epoch: 652, loss 0.1098\n",
      "test epoch: 652, loss: 0.85\n",
      "train epoch: 653, loss 0.1099\n",
      "test epoch: 653, loss: 0.92\n",
      "train epoch: 654, loss 0.1103\n",
      "test epoch: 654, loss: 0.84\n",
      "train epoch: 655, loss 0.1111\n",
      "test epoch: 655, loss: 0.94\n",
      "train epoch: 656, loss 0.1122\n",
      "test epoch: 656, loss: 0.81\n",
      "train epoch: 657, loss 0.1135\n",
      "test epoch: 657, loss: 0.96\n",
      "train epoch: 658, loss 0.1153\n",
      "test epoch: 658, loss: 0.80\n",
      "train epoch: 659, loss 0.1164\n",
      "test epoch: 659, loss: 0.97\n",
      "train epoch: 660, loss 0.1171\n",
      "test epoch: 660, loss: 0.80\n",
      "train epoch: 661, loss 0.1149\n",
      "test epoch: 661, loss: 0.93\n",
      "train epoch: 662, loss 0.1117\n",
      "test epoch: 662, loss: 0.85\n",
      "train epoch: 663, loss 0.1087\n",
      "test epoch: 663, loss: 0.88\n",
      "train epoch: 664, loss 0.1079\n",
      "test epoch: 664, loss: 0.92\n",
      "train epoch: 665, loss 0.1094\n",
      "test epoch: 665, loss: 0.83\n",
      "train epoch: 666, loss 0.1117\n",
      "test epoch: 666, loss: 0.95\n",
      "train epoch: 667, loss 0.1129\n",
      "test epoch: 667, loss: 0.82\n",
      "train epoch: 668, loss 0.1118\n",
      "test epoch: 668, loss: 0.92\n",
      "train epoch: 669, loss 0.1095\n",
      "test epoch: 669, loss: 0.86\n",
      "train epoch: 670, loss 0.1077\n",
      "test epoch: 670, loss: 0.86\n",
      "train epoch: 671, loss 0.1076\n",
      "test epoch: 671, loss: 0.92\n",
      "train epoch: 672, loss 0.1092\n",
      "test epoch: 672, loss: 0.83\n",
      "train epoch: 673, loss 0.1107\n",
      "test epoch: 673, loss: 0.94\n",
      "train epoch: 674, loss 0.1110\n",
      "test epoch: 674, loss: 0.83\n",
      "train epoch: 675, loss 0.1096\n",
      "test epoch: 675, loss: 0.91\n",
      "train epoch: 676, loss 0.1077\n",
      "test epoch: 676, loss: 0.88\n",
      "train epoch: 677, loss 0.1068\n",
      "test epoch: 677, loss: 0.86\n",
      "train epoch: 678, loss 0.1075\n",
      "test epoch: 678, loss: 0.92\n",
      "train epoch: 679, loss 0.1085\n",
      "test epoch: 679, loss: 0.84\n",
      "train epoch: 680, loss 0.1088\n",
      "test epoch: 680, loss: 0.92\n",
      "train epoch: 681, loss 0.1084\n",
      "test epoch: 681, loss: 0.85\n",
      "train epoch: 682, loss 0.1072\n",
      "test epoch: 682, loss: 0.89\n",
      "train epoch: 683, loss 0.1064\n",
      "test epoch: 683, loss: 0.90\n",
      "train epoch: 684, loss 0.1065\n",
      "test epoch: 684, loss: 0.86\n",
      "train epoch: 685, loss 0.1070\n",
      "test epoch: 685, loss: 0.92\n",
      "train epoch: 686, loss 0.1074\n",
      "test epoch: 686, loss: 0.86\n",
      "train epoch: 687, loss 0.1073\n",
      "test epoch: 687, loss: 0.91\n",
      "train epoch: 688, loss 0.1069\n",
      "test epoch: 688, loss: 0.86\n",
      "train epoch: 689, loss 0.1063\n",
      "test epoch: 689, loss: 0.88\n",
      "train epoch: 690, loss 0.1059\n",
      "test epoch: 690, loss: 0.89\n",
      "train epoch: 691, loss 0.1059\n",
      "test epoch: 691, loss: 0.87\n",
      "train epoch: 692, loss 0.1063\n",
      "test epoch: 692, loss: 0.92\n",
      "train epoch: 693, loss 0.1068\n",
      "test epoch: 693, loss: 0.85\n",
      "train epoch: 694, loss 0.1073\n",
      "test epoch: 694, loss: 0.93\n",
      "train epoch: 695, loss 0.1074\n",
      "test epoch: 695, loss: 0.85\n",
      "train epoch: 696, loss 0.1069\n",
      "test epoch: 696, loss: 0.91\n",
      "train epoch: 697, loss 0.1061\n",
      "test epoch: 697, loss: 0.88\n",
      "train epoch: 698, loss 0.1056\n",
      "test epoch: 698, loss: 0.89\n",
      "train epoch: 699, loss 0.1053\n",
      "test epoch: 699, loss: 0.89\n",
      "train epoch: 700, loss 0.1052\n",
      "test epoch: 700, loss: 0.87\n",
      "train epoch: 701, loss 0.1054\n",
      "test epoch: 701, loss: 0.91\n",
      "train epoch: 702, loss 0.1057\n",
      "test epoch: 702, loss: 0.86\n",
      "train epoch: 703, loss 0.1058\n",
      "test epoch: 703, loss: 0.92\n",
      "train epoch: 704, loss 0.1060\n",
      "test epoch: 704, loss: 0.87\n",
      "train epoch: 705, loss 0.1061\n",
      "test epoch: 705, loss: 0.91\n",
      "train epoch: 706, loss 0.1059\n",
      "test epoch: 706, loss: 0.84\n",
      "train epoch: 707, loss 0.1063\n",
      "test epoch: 707, loss: 0.91\n",
      "train epoch: 708, loss 0.1058\n",
      "test epoch: 708, loss: 0.87\n",
      "train epoch: 709, loss 0.1057\n",
      "test epoch: 709, loss: 0.93\n",
      "train epoch: 710, loss 0.1059\n",
      "test epoch: 710, loss: 0.86\n",
      "train epoch: 711, loss 0.1059\n",
      "test epoch: 711, loss: 0.93\n",
      "train epoch: 712, loss 0.1062\n",
      "test epoch: 712, loss: 0.84\n",
      "train epoch: 713, loss 0.1068\n",
      "test epoch: 713, loss: 0.93\n",
      "train epoch: 714, loss 0.1068\n",
      "test epoch: 714, loss: 0.84\n",
      "train epoch: 715, loss 0.1068\n",
      "test epoch: 715, loss: 0.94\n",
      "train epoch: 716, loss 0.1068\n",
      "test epoch: 716, loss: 0.84\n",
      "train epoch: 717, loss 0.1069\n",
      "test epoch: 717, loss: 0.94\n",
      "train epoch: 718, loss 0.1067\n",
      "test epoch: 718, loss: 0.84\n",
      "train epoch: 719, loss 0.1063\n",
      "test epoch: 719, loss: 0.93\n",
      "train epoch: 720, loss 0.1061\n",
      "test epoch: 720, loss: 0.85\n",
      "train epoch: 721, loss 0.1055\n",
      "test epoch: 721, loss: 0.92\n",
      "train epoch: 722, loss 0.1049\n",
      "test epoch: 722, loss: 0.88\n",
      "train epoch: 723, loss 0.1044\n",
      "test epoch: 723, loss: 0.90\n",
      "train epoch: 724, loss 0.1039\n",
      "test epoch: 724, loss: 0.87\n",
      "train epoch: 725, loss 0.1041\n",
      "test epoch: 725, loss: 0.89\n",
      "train epoch: 726, loss 0.1037\n",
      "test epoch: 726, loss: 0.90\n",
      "train epoch: 727, loss 0.1036\n",
      "test epoch: 727, loss: 0.90\n",
      "train epoch: 728, loss 0.1038\n",
      "test epoch: 728, loss: 0.91\n",
      "train epoch: 729, loss 0.1036\n",
      "test epoch: 729, loss: 0.87\n",
      "train epoch: 730, loss 0.1038\n",
      "test epoch: 730, loss: 0.90\n",
      "train epoch: 731, loss 0.1040\n",
      "test epoch: 731, loss: 0.87\n",
      "train epoch: 732, loss 0.1038\n",
      "test epoch: 732, loss: 0.92\n",
      "train epoch: 733, loss 0.1040\n",
      "test epoch: 733, loss: 0.87\n",
      "train epoch: 734, loss 0.1049\n",
      "test epoch: 734, loss: 0.95\n",
      "train epoch: 735, loss 0.1060\n",
      "test epoch: 735, loss: 0.82\n",
      "train epoch: 736, loss 0.1085\n",
      "test epoch: 736, loss: 0.97\n",
      "train epoch: 737, loss 0.1126\n",
      "test epoch: 737, loss: 0.78\n",
      "train epoch: 738, loss 0.1155\n",
      "test epoch: 738, loss: 1.01\n",
      "train epoch: 739, loss 0.1176\n",
      "test epoch: 739, loss: 0.79\n",
      "train epoch: 740, loss 0.1184\n",
      "test epoch: 740, loss: 1.01\n",
      "train epoch: 741, loss 0.1163\n",
      "test epoch: 741, loss: 0.80\n",
      "train epoch: 742, loss 0.1099\n",
      "test epoch: 742, loss: 0.91\n",
      "train epoch: 743, loss 0.1046\n",
      "test epoch: 743, loss: 0.90\n",
      "train epoch: 744, loss 0.1035\n",
      "test epoch: 744, loss: 0.83\n",
      "train epoch: 745, loss 0.1071\n",
      "test epoch: 745, loss: 1.00\n",
      "train epoch: 746, loss 0.1121\n",
      "test epoch: 746, loss: 0.82\n",
      "train epoch: 747, loss 0.1137\n",
      "test epoch: 747, loss: 0.99\n",
      "train epoch: 748, loss 0.1115\n",
      "test epoch: 748, loss: 0.82\n",
      "train epoch: 749, loss 0.1067\n",
      "test epoch: 749, loss: 0.88\n",
      "train epoch: 750, loss 0.1033\n",
      "test epoch: 750, loss: 0.92\n",
      "train epoch: 751, loss 0.1044\n",
      "test epoch: 751, loss: 0.82\n",
      "train epoch: 752, loss 0.1081\n",
      "test epoch: 752, loss: 0.97\n",
      "train epoch: 753, loss 0.1094\n",
      "test epoch: 753, loss: 0.83\n",
      "train epoch: 754, loss 0.1066\n",
      "test epoch: 754, loss: 0.91\n",
      "train epoch: 755, loss 0.1034\n",
      "test epoch: 755, loss: 0.90\n",
      "train epoch: 756, loss 0.1025\n",
      "test epoch: 756, loss: 0.85\n",
      "train epoch: 757, loss 0.1046\n",
      "test epoch: 757, loss: 0.96\n",
      "train epoch: 758, loss 0.1068\n",
      "test epoch: 758, loss: 0.84\n",
      "train epoch: 759, loss 0.1064\n",
      "test epoch: 759, loss: 0.93\n",
      "train epoch: 760, loss 0.1038\n",
      "test epoch: 760, loss: 0.87\n",
      "train epoch: 761, loss 0.1023\n",
      "test epoch: 761, loss: 0.86\n",
      "train epoch: 762, loss 0.1026\n",
      "test epoch: 762, loss: 0.93\n",
      "train epoch: 763, loss 0.1041\n",
      "test epoch: 763, loss: 0.85\n",
      "train epoch: 764, loss 0.1043\n",
      "test epoch: 764, loss: 0.92\n",
      "train epoch: 765, loss 0.1032\n",
      "test epoch: 765, loss: 0.88\n",
      "train epoch: 766, loss 0.1021\n",
      "test epoch: 766, loss: 0.87\n",
      "train epoch: 767, loss 0.1023\n",
      "test epoch: 767, loss: 0.93\n",
      "train epoch: 768, loss 0.1035\n",
      "test epoch: 768, loss: 0.85\n",
      "train epoch: 769, loss 0.1040\n",
      "test epoch: 769, loss: 0.93\n",
      "train epoch: 770, loss 0.1030\n",
      "test epoch: 770, loss: 0.88\n",
      "train epoch: 771, loss 0.1019\n",
      "test epoch: 771, loss: 0.88\n",
      "train epoch: 772, loss 0.1018\n",
      "test epoch: 772, loss: 0.91\n",
      "train epoch: 773, loss 0.1027\n",
      "test epoch: 773, loss: 0.85\n",
      "train epoch: 774, loss 0.1030\n",
      "test epoch: 774, loss: 0.92\n",
      "train epoch: 775, loss 0.1025\n",
      "test epoch: 775, loss: 0.89\n",
      "train epoch: 776, loss 0.1021\n",
      "test epoch: 776, loss: 0.90\n",
      "train epoch: 777, loss 0.1016\n",
      "test epoch: 777, loss: 0.90\n",
      "train epoch: 778, loss 0.1016\n",
      "test epoch: 778, loss: 0.86\n",
      "train epoch: 779, loss 0.1020\n",
      "test epoch: 779, loss: 0.91\n",
      "train epoch: 780, loss 0.1020\n",
      "test epoch: 780, loss: 0.87\n",
      "train epoch: 781, loss 0.1019\n",
      "test epoch: 781, loss: 0.91\n",
      "train epoch: 782, loss 0.1018\n",
      "test epoch: 782, loss: 0.89\n",
      "train epoch: 783, loss 0.1015\n",
      "test epoch: 783, loss: 0.88\n",
      "train epoch: 784, loss 0.1013\n",
      "test epoch: 784, loss: 0.90\n",
      "train epoch: 785, loss 0.1017\n",
      "test epoch: 785, loss: 0.86\n",
      "train epoch: 786, loss 0.1019\n",
      "test epoch: 786, loss: 0.91\n",
      "train epoch: 787, loss 0.1014\n",
      "test epoch: 787, loss: 0.89\n",
      "train epoch: 788, loss 0.1013\n",
      "test epoch: 788, loss: 0.90\n",
      "train epoch: 789, loss 0.1012\n",
      "test epoch: 789, loss: 0.90\n",
      "train epoch: 790, loss 0.1012\n",
      "test epoch: 790, loss: 0.88\n",
      "train epoch: 791, loss 0.1012\n",
      "test epoch: 791, loss: 0.90\n",
      "train epoch: 792, loss 0.1011\n",
      "test epoch: 792, loss: 0.87\n",
      "train epoch: 793, loss 0.1011\n",
      "test epoch: 793, loss: 0.90\n",
      "train epoch: 794, loss 0.1011\n",
      "test epoch: 794, loss: 0.88\n",
      "train epoch: 795, loss 0.1010\n",
      "test epoch: 795, loss: 0.89\n",
      "train epoch: 796, loss 0.1009\n",
      "test epoch: 796, loss: 0.89\n",
      "train epoch: 797, loss 0.1008\n",
      "test epoch: 797, loss: 0.88\n",
      "train epoch: 798, loss 0.1008\n",
      "test epoch: 798, loss: 0.89\n",
      "train epoch: 799, loss 0.1007\n",
      "test epoch: 799, loss: 0.88\n",
      "train epoch: 800, loss 0.1006\n",
      "test epoch: 800, loss: 0.88\n",
      "train epoch: 801, loss 0.1006\n",
      "test epoch: 801, loss: 0.87\n",
      "train epoch: 802, loss 0.1005\n",
      "test epoch: 802, loss: 0.87\n",
      "train epoch: 803, loss 0.1005\n",
      "test epoch: 803, loss: 0.87\n",
      "train epoch: 804, loss 0.1004\n",
      "test epoch: 804, loss: 0.88\n",
      "train epoch: 805, loss 0.1004\n",
      "test epoch: 805, loss: 0.87\n",
      "train epoch: 806, loss 0.1006\n",
      "test epoch: 806, loss: 0.89\n",
      "train epoch: 807, loss 0.1006\n",
      "test epoch: 807, loss: 0.86\n",
      "train epoch: 808, loss 0.1008\n",
      "test epoch: 808, loss: 0.90\n",
      "train epoch: 809, loss 0.1012\n",
      "test epoch: 809, loss: 0.84\n",
      "train epoch: 810, loss 0.1016\n",
      "test epoch: 810, loss: 0.92\n",
      "train epoch: 811, loss 0.1019\n",
      "test epoch: 811, loss: 0.84\n",
      "train epoch: 812, loss 0.1024\n",
      "test epoch: 812, loss: 0.93\n",
      "train epoch: 813, loss 0.1032\n",
      "test epoch: 813, loss: 0.82\n",
      "train epoch: 814, loss 0.1041\n",
      "test epoch: 814, loss: 0.94\n",
      "train epoch: 815, loss 0.1049\n",
      "test epoch: 815, loss: 0.81\n",
      "train epoch: 816, loss 0.1052\n",
      "test epoch: 816, loss: 0.96\n",
      "train epoch: 817, loss 0.1057\n",
      "test epoch: 817, loss: 0.82\n",
      "train epoch: 818, loss 0.1059\n",
      "test epoch: 818, loss: 0.96\n",
      "train epoch: 819, loss 0.1065\n",
      "test epoch: 819, loss: 0.81\n",
      "train epoch: 820, loss 0.1061\n",
      "test epoch: 820, loss: 0.95\n",
      "train epoch: 821, loss 0.1050\n",
      "test epoch: 821, loss: 0.83\n",
      "train epoch: 822, loss 0.1030\n",
      "test epoch: 822, loss: 0.93\n",
      "train epoch: 823, loss 0.1013\n",
      "test epoch: 823, loss: 0.87\n",
      "train epoch: 824, loss 0.1001\n",
      "test epoch: 824, loss: 0.88\n",
      "train epoch: 825, loss 0.0997\n",
      "test epoch: 825, loss: 0.90\n",
      "train epoch: 826, loss 0.1000\n",
      "test epoch: 826, loss: 0.86\n",
      "train epoch: 827, loss 0.1008\n",
      "test epoch: 827, loss: 0.93\n",
      "train epoch: 828, loss 0.1021\n",
      "test epoch: 828, loss: 0.82\n",
      "train epoch: 829, loss 0.1034\n",
      "test epoch: 829, loss: 0.94\n",
      "train epoch: 830, loss 0.1044\n",
      "test epoch: 830, loss: 0.82\n",
      "train epoch: 831, loss 0.1047\n",
      "test epoch: 831, loss: 0.94\n",
      "train epoch: 832, loss 0.1043\n",
      "test epoch: 832, loss: 0.82\n",
      "train epoch: 833, loss 0.1029\n",
      "test epoch: 833, loss: 0.92\n",
      "train epoch: 834, loss 0.1016\n",
      "test epoch: 834, loss: 0.86\n",
      "train epoch: 835, loss 0.1003\n",
      "test epoch: 835, loss: 0.90\n",
      "train epoch: 836, loss 0.0997\n",
      "test epoch: 836, loss: 0.89\n",
      "train epoch: 837, loss 0.0994\n",
      "test epoch: 837, loss: 0.87\n",
      "train epoch: 838, loss 0.0995\n",
      "test epoch: 838, loss: 0.91\n",
      "train epoch: 839, loss 0.1002\n",
      "test epoch: 839, loss: 0.84\n",
      "train epoch: 840, loss 0.1010\n",
      "test epoch: 840, loss: 0.93\n",
      "train epoch: 841, loss 0.1020\n",
      "test epoch: 841, loss: 0.83\n",
      "train epoch: 842, loss 0.1027\n",
      "test epoch: 842, loss: 0.94\n",
      "train epoch: 843, loss 0.1031\n",
      "test epoch: 843, loss: 0.83\n",
      "train epoch: 844, loss 0.1026\n",
      "test epoch: 844, loss: 0.93\n",
      "train epoch: 845, loss 0.1023\n",
      "test epoch: 845, loss: 0.83\n",
      "train epoch: 846, loss 0.1015\n",
      "test epoch: 846, loss: 0.92\n",
      "train epoch: 847, loss 0.1004\n",
      "test epoch: 847, loss: 0.87\n",
      "train epoch: 848, loss 0.0998\n",
      "test epoch: 848, loss: 0.90\n",
      "train epoch: 849, loss 0.0994\n",
      "test epoch: 849, loss: 0.87\n",
      "train epoch: 850, loss 0.0991\n",
      "test epoch: 850, loss: 0.88\n",
      "train epoch: 851, loss 0.0991\n",
      "test epoch: 851, loss: 0.89\n",
      "train epoch: 852, loss 0.0990\n",
      "test epoch: 852, loss: 0.87\n",
      "train epoch: 853, loss 0.0992\n",
      "test epoch: 853, loss: 0.90\n",
      "train epoch: 854, loss 0.0992\n",
      "test epoch: 854, loss: 0.87\n",
      "train epoch: 855, loss 0.0993\n",
      "test epoch: 855, loss: 0.90\n",
      "train epoch: 856, loss 0.0995\n",
      "test epoch: 856, loss: 0.86\n",
      "train epoch: 857, loss 0.0995\n",
      "test epoch: 857, loss: 0.91\n",
      "train epoch: 858, loss 0.0996\n",
      "test epoch: 858, loss: 0.86\n",
      "train epoch: 859, loss 0.0998\n",
      "test epoch: 859, loss: 0.92\n",
      "train epoch: 860, loss 0.1001\n",
      "test epoch: 860, loss: 0.84\n",
      "train epoch: 861, loss 0.1004\n",
      "test epoch: 861, loss: 0.92\n",
      "train epoch: 862, loss 0.1006\n",
      "test epoch: 862, loss: 0.84\n",
      "train epoch: 863, loss 0.1005\n",
      "test epoch: 863, loss: 0.93\n",
      "train epoch: 864, loss 0.1008\n",
      "test epoch: 864, loss: 0.84\n",
      "train epoch: 865, loss 0.1015\n",
      "test epoch: 865, loss: 0.94\n",
      "train epoch: 866, loss 0.1030\n",
      "test epoch: 866, loss: 0.81\n",
      "train epoch: 867, loss 0.1050\n",
      "test epoch: 867, loss: 0.97\n",
      "train epoch: 868, loss 0.1087\n",
      "test epoch: 868, loss: 0.77\n",
      "train epoch: 869, loss 0.1109\n",
      "test epoch: 869, loss: 0.99\n",
      "train epoch: 870, loss 0.1132\n",
      "test epoch: 870, loss: 0.77\n",
      "train epoch: 871, loss 0.1104\n",
      "test epoch: 871, loss: 0.96\n",
      "train epoch: 872, loss 0.1062\n",
      "test epoch: 872, loss: 0.83\n",
      "train epoch: 873, loss 0.1012\n",
      "test epoch: 873, loss: 0.90\n",
      "train epoch: 874, loss 0.0986\n",
      "test epoch: 874, loss: 0.92\n",
      "train epoch: 875, loss 0.0995\n",
      "test epoch: 875, loss: 0.84\n",
      "train epoch: 876, loss 0.1020\n",
      "test epoch: 876, loss: 0.96\n",
      "train epoch: 877, loss 0.1053\n",
      "test epoch: 877, loss: 0.79\n",
      "train epoch: 878, loss 0.1070\n",
      "test epoch: 878, loss: 0.96\n",
      "train epoch: 879, loss 0.1064\n",
      "test epoch: 879, loss: 0.82\n",
      "train epoch: 880, loss 0.1029\n",
      "test epoch: 880, loss: 0.91\n",
      "train epoch: 881, loss 0.0998\n",
      "test epoch: 881, loss: 0.88\n",
      "train epoch: 882, loss 0.0985\n",
      "test epoch: 882, loss: 0.84\n",
      "train epoch: 883, loss 0.0995\n",
      "test epoch: 883, loss: 0.93\n",
      "train epoch: 884, loss 0.1012\n",
      "test epoch: 884, loss: 0.83\n",
      "train epoch: 885, loss 0.1028\n",
      "test epoch: 885, loss: 0.96\n",
      "train epoch: 886, loss 0.1032\n",
      "test epoch: 886, loss: 0.83\n",
      "train epoch: 887, loss 0.1020\n",
      "test epoch: 887, loss: 0.92\n",
      "train epoch: 888, loss 0.1003\n",
      "test epoch: 888, loss: 0.85\n",
      "train epoch: 889, loss 0.0988\n",
      "test epoch: 889, loss: 0.87\n",
      "train epoch: 890, loss 0.0984\n",
      "test epoch: 890, loss: 0.91\n",
      "train epoch: 891, loss 0.0991\n",
      "test epoch: 891, loss: 0.84\n",
      "train epoch: 892, loss 0.1001\n",
      "test epoch: 892, loss: 0.92\n",
      "train epoch: 893, loss 0.1007\n",
      "test epoch: 893, loss: 0.84\n",
      "train epoch: 894, loss 0.1004\n",
      "test epoch: 894, loss: 0.92\n",
      "train epoch: 895, loss 0.0996\n",
      "test epoch: 895, loss: 0.86\n",
      "train epoch: 896, loss 0.0988\n",
      "test epoch: 896, loss: 0.90\n",
      "train epoch: 897, loss 0.0983\n",
      "test epoch: 897, loss: 0.88\n",
      "train epoch: 898, loss 0.0982\n",
      "test epoch: 898, loss: 0.85\n",
      "train epoch: 899, loss 0.0987\n",
      "test epoch: 899, loss: 0.90\n",
      "train epoch: 900, loss 0.0989\n",
      "test epoch: 900, loss: 0.85\n",
      "train epoch: 901, loss 0.0990\n",
      "test epoch: 901, loss: 0.91\n",
      "train epoch: 902, loss 0.0990\n",
      "test epoch: 902, loss: 0.86\n",
      "train epoch: 903, loss 0.0988\n",
      "test epoch: 903, loss: 0.90\n",
      "train epoch: 904, loss 0.0986\n",
      "test epoch: 904, loss: 0.86\n",
      "train epoch: 905, loss 0.0983\n",
      "test epoch: 905, loss: 0.88\n",
      "train epoch: 906, loss 0.0979\n",
      "test epoch: 906, loss: 0.88\n",
      "train epoch: 907, loss 0.0980\n",
      "test epoch: 907, loss: 0.87\n",
      "train epoch: 908, loss 0.0978\n",
      "test epoch: 908, loss: 0.88\n",
      "train epoch: 909, loss 0.0979\n",
      "test epoch: 909, loss: 0.85\n",
      "train epoch: 910, loss 0.0981\n",
      "test epoch: 910, loss: 0.89\n",
      "train epoch: 911, loss 0.0980\n",
      "test epoch: 911, loss: 0.86\n",
      "train epoch: 912, loss 0.0983\n",
      "test epoch: 912, loss: 0.90\n",
      "train epoch: 913, loss 0.0983\n",
      "test epoch: 913, loss: 0.84\n",
      "train epoch: 914, loss 0.0987\n",
      "test epoch: 914, loss: 0.89\n",
      "train epoch: 915, loss 0.0993\n",
      "test epoch: 915, loss: 0.82\n",
      "train epoch: 916, loss 0.0995\n",
      "test epoch: 916, loss: 0.91\n",
      "train epoch: 917, loss 0.0994\n",
      "test epoch: 917, loss: 0.84\n",
      "train epoch: 918, loss 0.0995\n",
      "test epoch: 918, loss: 0.92\n",
      "train epoch: 919, loss 0.0995\n",
      "test epoch: 919, loss: 0.83\n",
      "train epoch: 920, loss 0.0996\n",
      "test epoch: 920, loss: 0.91\n",
      "train epoch: 921, loss 0.0991\n",
      "test epoch: 921, loss: 0.85\n",
      "train epoch: 922, loss 0.0986\n",
      "test epoch: 922, loss: 0.91\n",
      "train epoch: 923, loss 0.0984\n",
      "test epoch: 923, loss: 0.85\n",
      "train epoch: 924, loss 0.0981\n",
      "test epoch: 924, loss: 0.89\n",
      "train epoch: 925, loss 0.0980\n",
      "test epoch: 925, loss: 0.85\n",
      "train epoch: 926, loss 0.0978\n",
      "test epoch: 926, loss: 0.89\n",
      "train epoch: 927, loss 0.0976\n",
      "test epoch: 927, loss: 0.87\n",
      "train epoch: 928, loss 0.0976\n",
      "test epoch: 928, loss: 0.88\n",
      "train epoch: 929, loss 0.0974\n",
      "test epoch: 929, loss: 0.86\n",
      "train epoch: 930, loss 0.0975\n",
      "test epoch: 930, loss: 0.89\n",
      "train epoch: 931, loss 0.0975\n",
      "test epoch: 931, loss: 0.86\n",
      "train epoch: 932, loss 0.0977\n",
      "test epoch: 932, loss: 0.90\n",
      "train epoch: 933, loss 0.0979\n",
      "test epoch: 933, loss: 0.85\n",
      "train epoch: 934, loss 0.0980\n",
      "test epoch: 934, loss: 0.90\n",
      "train epoch: 935, loss 0.0983\n",
      "test epoch: 935, loss: 0.83\n",
      "train epoch: 936, loss 0.0988\n",
      "test epoch: 936, loss: 0.92\n",
      "train epoch: 937, loss 0.0998\n",
      "test epoch: 937, loss: 0.82\n",
      "train epoch: 938, loss 0.1009\n",
      "test epoch: 938, loss: 0.94\n",
      "train epoch: 939, loss 0.1030\n",
      "test epoch: 939, loss: 0.79\n",
      "train epoch: 940, loss 0.1050\n",
      "test epoch: 940, loss: 0.97\n",
      "train epoch: 941, loss 0.1086\n",
      "test epoch: 941, loss: 0.77\n",
      "train epoch: 942, loss 0.1104\n",
      "test epoch: 942, loss: 0.99\n",
      "train epoch: 943, loss 0.1118\n",
      "test epoch: 943, loss: 0.77\n",
      "train epoch: 944, loss 0.1101\n",
      "test epoch: 944, loss: 0.96\n",
      "train epoch: 945, loss 0.1073\n",
      "test epoch: 945, loss: 0.80\n",
      "train epoch: 946, loss 0.1023\n",
      "test epoch: 946, loss: 0.90\n",
      "train epoch: 947, loss 0.0981\n",
      "test epoch: 947, loss: 0.89\n",
      "train epoch: 948, loss 0.0973\n",
      "test epoch: 948, loss: 0.85\n",
      "train epoch: 949, loss 0.0993\n",
      "test epoch: 949, loss: 0.95\n",
      "train epoch: 950, loss 0.1030\n",
      "test epoch: 950, loss: 0.78\n",
      "train epoch: 951, loss 0.1070\n",
      "test epoch: 951, loss: 0.97\n",
      "train epoch: 952, loss 0.1091\n",
      "test epoch: 952, loss: 0.78\n",
      "train epoch: 953, loss 0.1063\n",
      "test epoch: 953, loss: 0.93\n",
      "train epoch: 954, loss 0.1016\n",
      "test epoch: 954, loss: 0.85\n",
      "train epoch: 955, loss 0.0980\n",
      "test epoch: 955, loss: 0.86\n",
      "train epoch: 956, loss 0.0974\n",
      "test epoch: 956, loss: 0.92\n",
      "train epoch: 957, loss 0.0992\n",
      "test epoch: 957, loss: 0.82\n",
      "train epoch: 958, loss 0.1013\n",
      "test epoch: 958, loss: 0.95\n",
      "train epoch: 959, loss 0.1020\n",
      "test epoch: 959, loss: 0.82\n",
      "train epoch: 960, loss 0.1010\n",
      "test epoch: 960, loss: 0.92\n",
      "train epoch: 961, loss 0.0997\n",
      "test epoch: 961, loss: 0.84\n",
      "train epoch: 962, loss 0.0981\n",
      "test epoch: 962, loss: 0.88\n",
      "train epoch: 963, loss 0.0970\n",
      "test epoch: 963, loss: 0.89\n",
      "train epoch: 964, loss 0.0973\n",
      "test epoch: 964, loss: 0.84\n",
      "train epoch: 965, loss 0.0986\n",
      "test epoch: 965, loss: 0.92\n",
      "train epoch: 966, loss 0.0995\n",
      "test epoch: 966, loss: 0.82\n",
      "train epoch: 967, loss 0.0998\n",
      "test epoch: 967, loss: 0.92\n",
      "train epoch: 968, loss 0.0993\n",
      "test epoch: 968, loss: 0.85\n",
      "train epoch: 969, loss 0.0981\n",
      "test epoch: 969, loss: 0.90\n",
      "train epoch: 970, loss 0.0971\n",
      "test epoch: 970, loss: 0.88\n",
      "train epoch: 971, loss 0.0969\n",
      "test epoch: 971, loss: 0.86\n",
      "train epoch: 972, loss 0.0971\n",
      "test epoch: 972, loss: 0.89\n",
      "train epoch: 973, loss 0.0976\n",
      "test epoch: 973, loss: 0.84\n",
      "train epoch: 974, loss 0.0977\n",
      "test epoch: 974, loss: 0.90\n",
      "train epoch: 975, loss 0.0977\n",
      "test epoch: 975, loss: 0.86\n",
      "train epoch: 976, loss 0.0976\n",
      "test epoch: 976, loss: 0.90\n",
      "train epoch: 977, loss 0.0972\n",
      "test epoch: 977, loss: 0.86\n",
      "train epoch: 978, loss 0.0970\n",
      "test epoch: 978, loss: 0.88\n",
      "train epoch: 979, loss 0.0969\n",
      "test epoch: 979, loss: 0.87\n",
      "train epoch: 980, loss 0.0967\n",
      "test epoch: 980, loss: 0.88\n",
      "train epoch: 981, loss 0.0968\n",
      "test epoch: 981, loss: 0.89\n",
      "train epoch: 982, loss 0.0969\n",
      "test epoch: 982, loss: 0.87\n",
      "train epoch: 983, loss 0.0968\n",
      "test epoch: 983, loss: 0.88\n",
      "train epoch: 984, loss 0.0970\n",
      "test epoch: 984, loss: 0.86\n",
      "train epoch: 985, loss 0.0970\n",
      "test epoch: 985, loss: 0.88\n",
      "train epoch: 986, loss 0.0967\n",
      "test epoch: 986, loss: 0.88\n",
      "train epoch: 987, loss 0.0969\n",
      "test epoch: 987, loss: 0.89\n",
      "train epoch: 988, loss 0.0969\n",
      "test epoch: 988, loss: 0.86\n",
      "train epoch: 989, loss 0.0969\n",
      "test epoch: 989, loss: 0.89\n",
      "train epoch: 990, loss 0.0972\n",
      "test epoch: 990, loss: 0.84\n",
      "train epoch: 991, loss 0.0974\n",
      "test epoch: 991, loss: 0.90\n",
      "train epoch: 992, loss 0.0972\n",
      "test epoch: 992, loss: 0.86\n",
      "train epoch: 993, loss 0.0972\n",
      "test epoch: 993, loss: 0.90\n",
      "train epoch: 994, loss 0.0970\n",
      "test epoch: 994, loss: 0.86\n",
      "train epoch: 995, loss 0.0970\n",
      "test epoch: 995, loss: 0.89\n",
      "train epoch: 996, loss 0.0969\n",
      "test epoch: 996, loss: 0.86\n",
      "train epoch: 997, loss 0.0968\n",
      "test epoch: 997, loss: 0.89\n",
      "train epoch: 998, loss 0.0968\n",
      "test epoch: 998, loss: 0.86\n",
      "train epoch: 999, loss 0.0968\n",
      "test epoch: 999, loss: 0.89\n",
      "train epoch: 1000, loss 0.0967\n",
      "test epoch: 1000, loss: 0.87\n",
      "train epoch: 1001, loss 0.0967\n",
      "test epoch: 1001, loss: 0.89\n",
      "train epoch: 1002, loss 0.0967\n",
      "test epoch: 1002, loss: 0.87\n",
      "train epoch: 1003, loss 0.0966\n",
      "test epoch: 1003, loss: 0.88\n",
      "train epoch: 1004, loss 0.0966\n",
      "test epoch: 1004, loss: 0.87\n",
      "train epoch: 1005, loss 0.0965\n",
      "test epoch: 1005, loss: 0.88\n",
      "train epoch: 1006, loss 0.0965\n",
      "test epoch: 1006, loss: 0.88\n",
      "train epoch: 1007, loss 0.0966\n",
      "test epoch: 1007, loss: 0.88\n",
      "train epoch: 1008, loss 0.0965\n",
      "test epoch: 1008, loss: 0.86\n",
      "train epoch: 1009, loss 0.0966\n",
      "test epoch: 1009, loss: 0.88\n",
      "train epoch: 1010, loss 0.0967\n",
      "test epoch: 1010, loss: 0.87\n",
      "train epoch: 1011, loss 0.0966\n",
      "test epoch: 1011, loss: 0.89\n",
      "train epoch: 1012, loss 0.0966\n",
      "test epoch: 1012, loss: 0.87\n",
      "train epoch: 1013, loss 0.0967\n",
      "test epoch: 1013, loss: 0.89\n",
      "train epoch: 1014, loss 0.0969\n",
      "test epoch: 1014, loss: 0.84\n",
      "train epoch: 1015, loss 0.0975\n",
      "test epoch: 1015, loss: 0.91\n",
      "train epoch: 1016, loss 0.0980\n",
      "test epoch: 1016, loss: 0.84\n",
      "train epoch: 1017, loss 0.0987\n",
      "test epoch: 1017, loss: 0.94\n",
      "train epoch: 1018, loss 0.0999\n",
      "test epoch: 1018, loss: 0.82\n",
      "train epoch: 1019, loss 0.1013\n",
      "test epoch: 1019, loss: 0.95\n",
      "train epoch: 1020, loss 0.1048\n",
      "test epoch: 1020, loss: 0.76\n",
      "train epoch: 1021, loss 0.1081\n",
      "test epoch: 1021, loss: 0.97\n",
      "train epoch: 1022, loss 0.1094\n",
      "test epoch: 1022, loss: 0.77\n",
      "train epoch: 1023, loss 0.1073\n",
      "test epoch: 1023, loss: 0.96\n",
      "train epoch: 1024, loss 0.1043\n",
      "test epoch: 1024, loss: 0.83\n",
      "train epoch: 1025, loss 0.1016\n",
      "test epoch: 1025, loss: 0.92\n",
      "train epoch: 1026, loss 0.0994\n",
      "test epoch: 1026, loss: 0.83\n",
      "train epoch: 1027, loss 0.0982\n",
      "test epoch: 1027, loss: 0.88\n",
      "train epoch: 1028, loss 0.0971\n",
      "test epoch: 1028, loss: 0.87\n",
      "train epoch: 1029, loss 0.0966\n",
      "test epoch: 1029, loss: 0.86\n",
      "train epoch: 1030, loss 0.0970\n",
      "test epoch: 1030, loss: 0.93\n",
      "train epoch: 1031, loss 0.0983\n",
      "test epoch: 1031, loss: 0.84\n",
      "train epoch: 1032, loss 0.1007\n",
      "test epoch: 1032, loss: 0.95\n",
      "train epoch: 1033, loss 0.1042\n",
      "test epoch: 1033, loss: 0.76\n",
      "train epoch: 1034, loss 0.1079\n",
      "test epoch: 1034, loss: 0.95\n",
      "train epoch: 1035, loss 0.1068\n",
      "test epoch: 1035, loss: 0.80\n",
      "train epoch: 1036, loss 0.1014\n",
      "test epoch: 1036, loss: 0.90\n",
      "train epoch: 1037, loss 0.0970\n",
      "test epoch: 1037, loss: 0.91\n",
      "train epoch: 1038, loss 0.0974\n",
      "test epoch: 1038, loss: 0.85\n",
      "train epoch: 1039, loss 0.0990\n",
      "test epoch: 1039, loss: 0.94\n",
      "train epoch: 1040, loss 0.1018\n",
      "test epoch: 1040, loss: 0.78\n",
      "train epoch: 1041, loss 0.1054\n",
      "test epoch: 1041, loss: 0.95\n",
      "train epoch: 1042, loss 0.1056\n",
      "test epoch: 1042, loss: 0.80\n",
      "train epoch: 1043, loss 0.1022\n",
      "test epoch: 1043, loss: 0.93\n",
      "train epoch: 1044, loss 0.0986\n",
      "test epoch: 1044, loss: 0.88\n",
      "train epoch: 1045, loss 0.0979\n",
      "test epoch: 1045, loss: 0.88\n",
      "train epoch: 1046, loss 0.0972\n",
      "test epoch: 1046, loss: 0.89\n",
      "train epoch: 1047, loss 0.0974\n",
      "test epoch: 1047, loss: 0.81\n",
      "train epoch: 1048, loss 0.0995\n",
      "test epoch: 1048, loss: 0.91\n",
      "train epoch: 1049, loss 0.0998\n",
      "test epoch: 1049, loss: 0.83\n",
      "train epoch: 1050, loss 0.0985\n",
      "test epoch: 1050, loss: 0.91\n",
      "train epoch: 1051, loss 0.0972\n",
      "test epoch: 1051, loss: 0.89\n",
      "train epoch: 1052, loss 0.0972\n",
      "test epoch: 1052, loss: 0.89\n",
      "train epoch: 1053, loss 0.0966\n",
      "test epoch: 1053, loss: 0.88\n",
      "train epoch: 1054, loss 0.0964\n",
      "test epoch: 1054, loss: 0.84\n",
      "train epoch: 1055, loss 0.0970\n",
      "test epoch: 1055, loss: 0.88\n",
      "train epoch: 1056, loss 0.0973\n",
      "test epoch: 1056, loss: 0.84\n",
      "train epoch: 1057, loss 0.0970\n",
      "test epoch: 1057, loss: 0.89\n",
      "train epoch: 1058, loss 0.0966\n",
      "test epoch: 1058, loss: 0.88\n",
      "train epoch: 1059, loss 0.0965\n",
      "test epoch: 1059, loss: 0.88\n",
      "train epoch: 1060, loss 0.0963\n",
      "test epoch: 1060, loss: 0.87\n",
      "train epoch: 1061, loss 0.0964\n",
      "test epoch: 1061, loss: 0.86\n",
      "train epoch: 1062, loss 0.0965\n",
      "test epoch: 1062, loss: 0.88\n",
      "train epoch: 1063, loss 0.0964\n",
      "test epoch: 1063, loss: 0.87\n",
      "train epoch: 1064, loss 0.0964\n",
      "test epoch: 1064, loss: 0.89\n",
      "train epoch: 1065, loss 0.0964\n",
      "test epoch: 1065, loss: 0.87\n",
      "train epoch: 1066, loss 0.0963\n",
      "test epoch: 1066, loss: 0.88\n",
      "train epoch: 1067, loss 0.0963\n",
      "test epoch: 1067, loss: 0.87\n",
      "train epoch: 1068, loss 0.0962\n",
      "test epoch: 1068, loss: 0.88\n",
      "train epoch: 1069, loss 0.0962\n",
      "test epoch: 1069, loss: 0.89\n",
      "train epoch: 1070, loss 0.0963\n",
      "test epoch: 1070, loss: 0.88\n",
      "train epoch: 1071, loss 0.0962\n",
      "test epoch: 1071, loss: 0.87\n",
      "train epoch: 1072, loss 0.0962\n",
      "test epoch: 1072, loss: 0.87\n",
      "train epoch: 1073, loss 0.0962\n",
      "test epoch: 1073, loss: 0.87\n",
      "train epoch: 1074, loss 0.0962\n",
      "test epoch: 1074, loss: 0.88\n",
      "train epoch: 1075, loss 0.0962\n",
      "test epoch: 1075, loss: 0.87\n",
      "train epoch: 1076, loss 0.0963\n",
      "test epoch: 1076, loss: 0.89\n",
      "train epoch: 1077, loss 0.0962\n",
      "test epoch: 1077, loss: 0.86\n",
      "train epoch: 1078, loss 0.0964\n",
      "test epoch: 1078, loss: 0.89\n",
      "train epoch: 1079, loss 0.0965\n",
      "test epoch: 1079, loss: 0.86\n",
      "train epoch: 1080, loss 0.0966\n",
      "test epoch: 1080, loss: 0.90\n",
      "train epoch: 1081, loss 0.0967\n",
      "test epoch: 1081, loss: 0.86\n",
      "train epoch: 1082, loss 0.0969\n",
      "test epoch: 1082, loss: 0.90\n",
      "train epoch: 1083, loss 0.0972\n",
      "test epoch: 1083, loss: 0.84\n",
      "train epoch: 1084, loss 0.0974\n",
      "test epoch: 1084, loss: 0.91\n",
      "train epoch: 1085, loss 0.0974\n",
      "test epoch: 1085, loss: 0.85\n",
      "train epoch: 1086, loss 0.0973\n",
      "test epoch: 1086, loss: 0.91\n",
      "train epoch: 1087, loss 0.0972\n",
      "test epoch: 1087, loss: 0.85\n",
      "train epoch: 1088, loss 0.0971\n",
      "test epoch: 1088, loss: 0.90\n",
      "train epoch: 1089, loss 0.0970\n",
      "test epoch: 1089, loss: 0.85\n",
      "train epoch: 1090, loss 0.0968\n",
      "test epoch: 1090, loss: 0.90\n",
      "train epoch: 1091, loss 0.0967\n",
      "test epoch: 1091, loss: 0.86\n",
      "train epoch: 1092, loss 0.0965\n",
      "test epoch: 1092, loss: 0.89\n",
      "train epoch: 1093, loss 0.0965\n",
      "test epoch: 1093, loss: 0.85\n",
      "train epoch: 1094, loss 0.0965\n",
      "test epoch: 1094, loss: 0.89\n",
      "train epoch: 1095, loss 0.0965\n",
      "test epoch: 1095, loss: 0.86\n",
      "train epoch: 1096, loss 0.0965\n",
      "test epoch: 1096, loss: 0.89\n",
      "train epoch: 1097, loss 0.0966\n",
      "test epoch: 1097, loss: 0.85\n",
      "train epoch: 1098, loss 0.0966\n",
      "test epoch: 1098, loss: 0.89\n",
      "train epoch: 1099, loss 0.0969\n",
      "test epoch: 1099, loss: 0.84\n",
      "train epoch: 1100, loss 0.0971\n",
      "test epoch: 1100, loss: 0.90\n",
      "train epoch: 1101, loss 0.0972\n",
      "test epoch: 1101, loss: 0.85\n",
      "train epoch: 1102, loss 0.0974\n",
      "test epoch: 1102, loss: 0.92\n",
      "train epoch: 1103, loss 0.0980\n",
      "test epoch: 1103, loss: 0.83\n",
      "train epoch: 1104, loss 0.0989\n",
      "test epoch: 1104, loss: 0.93\n",
      "train epoch: 1105, loss 0.0999\n",
      "test epoch: 1105, loss: 0.81\n",
      "train epoch: 1106, loss 0.1006\n",
      "test epoch: 1106, loss: 0.94\n",
      "train epoch: 1107, loss 0.1014\n",
      "test epoch: 1107, loss: 0.81\n",
      "train epoch: 1108, loss 0.1017\n",
      "test epoch: 1108, loss: 0.94\n",
      "train epoch: 1109, loss 0.1018\n",
      "test epoch: 1109, loss: 0.80\n",
      "train epoch: 1110, loss 0.1012\n",
      "test epoch: 1110, loss: 0.93\n",
      "train epoch: 1111, loss 0.1005\n",
      "test epoch: 1111, loss: 0.82\n",
      "train epoch: 1112, loss 0.0990\n",
      "test epoch: 1112, loss: 0.90\n",
      "train epoch: 1113, loss 0.0975\n",
      "test epoch: 1113, loss: 0.86\n",
      "train epoch: 1114, loss 0.0965\n",
      "test epoch: 1114, loss: 0.89\n",
      "train epoch: 1115, loss 0.0962\n",
      "test epoch: 1115, loss: 0.88\n",
      "train epoch: 1116, loss 0.0961\n",
      "test epoch: 1116, loss: 0.87\n",
      "train epoch: 1117, loss 0.0961\n",
      "test epoch: 1117, loss: 0.89\n",
      "train epoch: 1118, loss 0.0964\n",
      "test epoch: 1118, loss: 0.85\n",
      "train epoch: 1119, loss 0.0967\n",
      "test epoch: 1119, loss: 0.90\n",
      "train epoch: 1120, loss 0.0969\n",
      "test epoch: 1120, loss: 0.85\n",
      "train epoch: 1121, loss 0.0972\n",
      "test epoch: 1121, loss: 0.91\n",
      "train epoch: 1122, loss 0.0974\n",
      "test epoch: 1122, loss: 0.84\n",
      "train epoch: 1123, loss 0.0975\n",
      "test epoch: 1123, loss: 0.91\n",
      "train epoch: 1124, loss 0.0977\n",
      "test epoch: 1124, loss: 0.83\n",
      "train epoch: 1125, loss 0.0978\n",
      "test epoch: 1125, loss: 0.90\n",
      "train epoch: 1126, loss 0.0978\n",
      "test epoch: 1126, loss: 0.84\n",
      "train epoch: 1127, loss 0.0975\n",
      "test epoch: 1127, loss: 0.91\n",
      "train epoch: 1128, loss 0.0973\n",
      "test epoch: 1128, loss: 0.85\n",
      "train epoch: 1129, loss 0.0969\n",
      "test epoch: 1129, loss: 0.89\n",
      "train epoch: 1130, loss 0.0967\n",
      "test epoch: 1130, loss: 0.85\n",
      "train epoch: 1131, loss 0.0966\n",
      "test epoch: 1131, loss: 0.88\n",
      "train epoch: 1132, loss 0.0964\n",
      "test epoch: 1132, loss: 0.86\n",
      "train epoch: 1133, loss 0.0962\n",
      "test epoch: 1133, loss: 0.89\n",
      "train epoch: 1134, loss 0.0961\n",
      "test epoch: 1134, loss: 0.88\n",
      "train epoch: 1135, loss 0.0961\n",
      "test epoch: 1135, loss: 0.87\n",
      "train epoch: 1136, loss 0.0960\n",
      "test epoch: 1136, loss: 0.86\n",
      "train epoch: 1137, loss 0.0961\n",
      "test epoch: 1137, loss: 0.87\n",
      "train epoch: 1138, loss 0.0961\n",
      "test epoch: 1138, loss: 0.87\n",
      "train epoch: 1139, loss 0.0960\n",
      "test epoch: 1139, loss: 0.88\n",
      "train epoch: 1140, loss 0.0961\n",
      "test epoch: 1140, loss: 0.88\n",
      "train epoch: 1141, loss 0.0960\n",
      "test epoch: 1141, loss: 0.87\n",
      "train epoch: 1142, loss 0.0960\n",
      "test epoch: 1142, loss: 0.87\n",
      "train epoch: 1143, loss 0.0961\n",
      "test epoch: 1143, loss: 0.87\n",
      "train epoch: 1144, loss 0.0961\n",
      "test epoch: 1144, loss: 0.88\n",
      "train epoch: 1145, loss 0.0960\n",
      "test epoch: 1145, loss: 0.88\n",
      "train epoch: 1146, loss 0.0960\n",
      "test epoch: 1146, loss: 0.89\n",
      "train epoch: 1147, loss 0.0961\n",
      "test epoch: 1147, loss: 0.86\n",
      "train epoch: 1148, loss 0.0962\n",
      "test epoch: 1148, loss: 0.88\n",
      "train epoch: 1149, loss 0.0963\n",
      "test epoch: 1149, loss: 0.85\n",
      "train epoch: 1150, loss 0.0963\n",
      "test epoch: 1150, loss: 0.89\n",
      "train epoch: 1151, loss 0.0964\n",
      "test epoch: 1151, loss: 0.86\n",
      "train epoch: 1152, loss 0.0967\n",
      "test epoch: 1152, loss: 0.91\n",
      "train epoch: 1153, loss 0.0971\n",
      "test epoch: 1153, loss: 0.83\n",
      "train epoch: 1154, loss 0.0977\n",
      "test epoch: 1154, loss: 0.91\n",
      "train epoch: 1155, loss 0.0990\n",
      "test epoch: 1155, loss: 0.80\n",
      "train epoch: 1156, loss 0.1004\n",
      "test epoch: 1156, loss: 0.94\n",
      "train epoch: 1157, loss 0.1018\n",
      "test epoch: 1157, loss: 0.80\n",
      "train epoch: 1158, loss 0.1031\n",
      "test epoch: 1158, loss: 0.97\n",
      "train epoch: 1159, loss 0.1055\n",
      "test epoch: 1159, loss: 0.78\n",
      "train epoch: 1160, loss 0.1084\n",
      "test epoch: 1160, loss: 0.99\n",
      "train epoch: 1161, loss 0.1134\n",
      "test epoch: 1161, loss: 0.73\n",
      "train epoch: 1162, loss 0.1156\n",
      "test epoch: 1162, loss: 0.97\n",
      "train epoch: 1163, loss 0.1117\n",
      "test epoch: 1163, loss: 0.78\n",
      "train epoch: 1164, loss 0.1035\n",
      "test epoch: 1164, loss: 0.91\n",
      "train epoch: 1165, loss 0.0973\n",
      "test epoch: 1165, loss: 0.90\n",
      "train epoch: 1166, loss 0.0968\n",
      "test epoch: 1166, loss: 0.85\n",
      "train epoch: 1167, loss 0.0991\n",
      "test epoch: 1167, loss: 0.95\n",
      "train epoch: 1168, loss 0.1035\n",
      "test epoch: 1168, loss: 0.76\n",
      "train epoch: 1169, loss 0.1091\n",
      "test epoch: 1169, loss: 0.97\n",
      "train epoch: 1170, loss 0.1109\n",
      "test epoch: 1170, loss: 0.77\n",
      "train epoch: 1171, loss 0.1055\n",
      "test epoch: 1171, loss: 0.91\n",
      "train epoch: 1172, loss 0.0984\n",
      "test epoch: 1172, loss: 0.89\n",
      "train epoch: 1173, loss 0.0963\n",
      "test epoch: 1173, loss: 0.84\n",
      "train epoch: 1174, loss 0.0989\n",
      "test epoch: 1174, loss: 0.96\n",
      "train epoch: 1175, loss 0.1032\n",
      "test epoch: 1175, loss: 0.77\n",
      "train epoch: 1176, loss 0.1065\n",
      "test epoch: 1176, loss: 0.95\n",
      "train epoch: 1177, loss 0.1046\n",
      "test epoch: 1177, loss: 0.81\n",
      "train epoch: 1178, loss 0.0993\n",
      "test epoch: 1178, loss: 0.88\n",
      "train epoch: 1179, loss 0.0961\n",
      "test epoch: 1179, loss: 0.91\n",
      "train epoch: 1180, loss 0.0972\n",
      "test epoch: 1180, loss: 0.83\n",
      "train epoch: 1181, loss 0.1011\n",
      "test epoch: 1181, loss: 0.96\n",
      "train epoch: 1182, loss 0.1052\n",
      "test epoch: 1182, loss: 0.77\n",
      "train epoch: 1183, loss 0.1061\n",
      "test epoch: 1183, loss: 0.93\n",
      "train epoch: 1184, loss 0.1020\n",
      "test epoch: 1184, loss: 0.83\n",
      "train epoch: 1185, loss 0.0970\n",
      "test epoch: 1185, loss: 0.84\n",
      "train epoch: 1186, loss 0.0967\n",
      "test epoch: 1186, loss: 0.94\n",
      "train epoch: 1187, loss 0.1004\n",
      "test epoch: 1187, loss: 0.81\n",
      "train epoch: 1188, loss 0.1040\n",
      "test epoch: 1188, loss: 0.96\n",
      "train epoch: 1189, loss 0.1047\n",
      "test epoch: 1189, loss: 0.79\n",
      "train epoch: 1190, loss 0.1021\n",
      "test epoch: 1190, loss: 0.88\n",
      "train epoch: 1191, loss 0.0978\n",
      "test epoch: 1191, loss: 0.88\n",
      "train epoch: 1192, loss 0.0968\n",
      "test epoch: 1192, loss: 0.82\n",
      "train epoch: 1193, loss 0.1000\n",
      "test epoch: 1193, loss: 0.97\n",
      "train epoch: 1194, loss 0.1033\n",
      "test epoch: 1194, loss: 0.82\n",
      "train epoch: 1195, loss 0.1043\n",
      "test epoch: 1195, loss: 0.95\n",
      "train epoch: 1196, loss 0.1025\n",
      "test epoch: 1196, loss: 0.81\n",
      "train epoch: 1197, loss 0.0992\n",
      "test epoch: 1197, loss: 0.85\n",
      "train epoch: 1198, loss 0.0970\n",
      "test epoch: 1198, loss: 0.89\n",
      "train epoch: 1199, loss 0.0982\n",
      "test epoch: 1199, loss: 0.81\n",
      "train epoch: 1200, loss 0.1003\n",
      "test epoch: 1200, loss: 0.94\n",
      "train epoch: 1201, loss 0.1003\n",
      "test epoch: 1201, loss: 0.84\n",
      "train epoch: 1202, loss 0.0982\n",
      "test epoch: 1202, loss: 0.89\n",
      "train epoch: 1203, loss 0.0962\n",
      "test epoch: 1203, loss: 0.88\n",
      "train epoch: 1204, loss 0.0962\n",
      "test epoch: 1204, loss: 0.84\n",
      "train epoch: 1205, loss 0.0973\n",
      "test epoch: 1205, loss: 0.91\n",
      "train epoch: 1206, loss 0.0980\n",
      "test epoch: 1206, loss: 0.83\n",
      "train epoch: 1207, loss 0.0975\n",
      "test epoch: 1207, loss: 0.89\n",
      "train epoch: 1208, loss 0.0963\n",
      "test epoch: 1208, loss: 0.88\n",
      "train epoch: 1209, loss 0.0961\n",
      "test epoch: 1209, loss: 0.86\n",
      "train epoch: 1210, loss 0.0963\n",
      "test epoch: 1210, loss: 0.89\n",
      "train epoch: 1211, loss 0.0966\n",
      "test epoch: 1211, loss: 0.84\n",
      "train epoch: 1212, loss 0.0966\n",
      "test epoch: 1212, loss: 0.88\n",
      "train epoch: 1213, loss 0.0962\n",
      "test epoch: 1213, loss: 0.87\n",
      "train epoch: 1214, loss 0.0959\n",
      "test epoch: 1214, loss: 0.87\n",
      "train epoch: 1215, loss 0.0961\n",
      "test epoch: 1215, loss: 0.89\n",
      "train epoch: 1216, loss 0.0963\n",
      "test epoch: 1216, loss: 0.85\n",
      "train epoch: 1217, loss 0.0966\n",
      "test epoch: 1217, loss: 0.89\n",
      "train epoch: 1218, loss 0.0965\n",
      "test epoch: 1218, loss: 0.85\n",
      "train epoch: 1219, loss 0.0961\n",
      "test epoch: 1219, loss: 0.87\n",
      "train epoch: 1220, loss 0.0959\n",
      "test epoch: 1220, loss: 0.89\n",
      "train epoch: 1221, loss 0.0961\n",
      "test epoch: 1221, loss: 0.86\n",
      "train epoch: 1222, loss 0.0962\n",
      "test epoch: 1222, loss: 0.88\n",
      "train epoch: 1223, loss 0.0963\n",
      "test epoch: 1223, loss: 0.85\n",
      "train epoch: 1224, loss 0.0963\n",
      "test epoch: 1224, loss: 0.87\n",
      "train epoch: 1225, loss 0.0961\n",
      "test epoch: 1225, loss: 0.87\n",
      "train epoch: 1226, loss 0.0959\n",
      "test epoch: 1226, loss: 0.87\n",
      "train epoch: 1227, loss 0.0959\n",
      "test epoch: 1227, loss: 0.89\n",
      "train epoch: 1228, loss 0.0962\n",
      "test epoch: 1228, loss: 0.87\n",
      "train epoch: 1229, loss 0.0964\n",
      "test epoch: 1229, loss: 0.88\n",
      "train epoch: 1230, loss 0.0962\n",
      "test epoch: 1230, loss: 0.85\n",
      "train epoch: 1231, loss 0.0964\n",
      "test epoch: 1231, loss: 0.87\n",
      "train epoch: 1232, loss 0.0963\n",
      "test epoch: 1232, loss: 0.86\n",
      "train epoch: 1233, loss 0.0961\n",
      "test epoch: 1233, loss: 0.87\n",
      "train epoch: 1234, loss 0.0960\n",
      "test epoch: 1234, loss: 0.89\n",
      "train epoch: 1235, loss 0.0961\n",
      "test epoch: 1235, loss: 0.87\n",
      "train epoch: 1236, loss 0.0964\n",
      "test epoch: 1236, loss: 0.89\n",
      "train epoch: 1237, loss 0.0963\n",
      "test epoch: 1237, loss: 0.84\n",
      "train epoch: 1238, loss 0.0965\n",
      "test epoch: 1238, loss: 0.88\n",
      "train epoch: 1239, loss 0.0963\n",
      "test epoch: 1239, loss: 0.86\n",
      "train epoch: 1240, loss 0.0960\n",
      "test epoch: 1240, loss: 0.87\n",
      "train epoch: 1241, loss 0.0959\n",
      "test epoch: 1241, loss: 0.89\n",
      "train epoch: 1242, loss 0.0962\n",
      "test epoch: 1242, loss: 0.86\n",
      "train epoch: 1243, loss 0.0962\n",
      "test epoch: 1243, loss: 0.89\n",
      "train epoch: 1244, loss 0.0964\n",
      "test epoch: 1244, loss: 0.84\n",
      "train epoch: 1245, loss 0.0964\n",
      "test epoch: 1245, loss: 0.88\n",
      "train epoch: 1246, loss 0.0963\n",
      "test epoch: 1246, loss: 0.86\n",
      "train epoch: 1247, loss 0.0961\n",
      "test epoch: 1247, loss: 0.87\n",
      "train epoch: 1248, loss 0.0958\n",
      "test epoch: 1248, loss: 0.89\n",
      "train epoch: 1249, loss 0.0960\n",
      "test epoch: 1249, loss: 0.87\n",
      "train epoch: 1250, loss 0.0960\n",
      "test epoch: 1250, loss: 0.88\n",
      "train epoch: 1251, loss 0.0961\n",
      "test epoch: 1251, loss: 0.85\n",
      "train epoch: 1252, loss 0.0962\n",
      "test epoch: 1252, loss: 0.88\n",
      "train epoch: 1253, loss 0.0962\n",
      "test epoch: 1253, loss: 0.86\n",
      "train epoch: 1254, loss 0.0960\n",
      "test epoch: 1254, loss: 0.87\n",
      "train epoch: 1255, loss 0.0959\n",
      "test epoch: 1255, loss: 0.89\n",
      "train epoch: 1256, loss 0.0960\n",
      "test epoch: 1256, loss: 0.87\n",
      "train epoch: 1257, loss 0.0959\n",
      "test epoch: 1257, loss: 0.88\n",
      "train epoch: 1258, loss 0.0960\n",
      "test epoch: 1258, loss: 0.86\n",
      "train epoch: 1259, loss 0.0961\n",
      "test epoch: 1259, loss: 0.87\n",
      "train epoch: 1260, loss 0.0960\n",
      "test epoch: 1260, loss: 0.86\n",
      "train epoch: 1261, loss 0.0959\n",
      "test epoch: 1261, loss: 0.88\n",
      "train epoch: 1262, loss 0.0959\n",
      "test epoch: 1262, loss: 0.88\n",
      "train epoch: 1263, loss 0.0958\n",
      "test epoch: 1263, loss: 0.86\n",
      "train epoch: 1264, loss 0.0959\n",
      "test epoch: 1264, loss: 0.87\n",
      "train epoch: 1265, loss 0.0960\n",
      "test epoch: 1265, loss: 0.86\n",
      "train epoch: 1266, loss 0.0960\n",
      "test epoch: 1266, loss: 0.87\n",
      "train epoch: 1267, loss 0.0959\n",
      "test epoch: 1267, loss: 0.87\n",
      "train epoch: 1268, loss 0.0959\n",
      "test epoch: 1268, loss: 0.88\n",
      "train epoch: 1269, loss 0.0958\n",
      "test epoch: 1269, loss: 0.86\n",
      "train epoch: 1270, loss 0.0958\n",
      "test epoch: 1270, loss: 0.87\n",
      "train epoch: 1271, loss 0.0959\n",
      "test epoch: 1271, loss: 0.86\n",
      "train epoch: 1272, loss 0.0959\n",
      "test epoch: 1272, loss: 0.87\n",
      "train epoch: 1273, loss 0.0958\n",
      "test epoch: 1273, loss: 0.87\n",
      "train epoch: 1274, loss 0.0958\n",
      "test epoch: 1274, loss: 0.87\n",
      "train epoch: 1275, loss 0.0958\n",
      "test epoch: 1275, loss: 0.86\n",
      "train epoch: 1276, loss 0.0958\n",
      "test epoch: 1276, loss: 0.87\n",
      "train epoch: 1277, loss 0.0958\n",
      "test epoch: 1277, loss: 0.86\n",
      "train epoch: 1278, loss 0.0958\n",
      "test epoch: 1278, loss: 0.87\n",
      "train epoch: 1279, loss 0.0958\n",
      "test epoch: 1279, loss: 0.87\n",
      "train epoch: 1280, loss 0.0958\n",
      "test epoch: 1280, loss: 0.86\n",
      "train epoch: 1281, loss 0.0958\n",
      "test epoch: 1281, loss: 0.87\n",
      "train epoch: 1282, loss 0.0959\n",
      "test epoch: 1282, loss: 0.86\n",
      "train epoch: 1283, loss 0.0960\n",
      "test epoch: 1283, loss: 0.88\n",
      "train epoch: 1284, loss 0.0961\n",
      "test epoch: 1284, loss: 0.84\n",
      "train epoch: 1285, loss 0.0963\n",
      "test epoch: 1285, loss: 0.89\n",
      "train epoch: 1286, loss 0.0966\n",
      "test epoch: 1286, loss: 0.84\n",
      "train epoch: 1287, loss 0.0967\n",
      "test epoch: 1287, loss: 0.90\n",
      "train epoch: 1288, loss 0.0970\n",
      "test epoch: 1288, loss: 0.83\n",
      "train epoch: 1289, loss 0.0970\n",
      "test epoch: 1289, loss: 0.90\n",
      "train epoch: 1290, loss 0.0968\n",
      "test epoch: 1290, loss: 0.85\n",
      "train epoch: 1291, loss 0.0966\n",
      "test epoch: 1291, loss: 0.90\n",
      "train epoch: 1292, loss 0.0966\n",
      "test epoch: 1292, loss: 0.84\n",
      "train epoch: 1293, loss 0.0966\n",
      "test epoch: 1293, loss: 0.89\n",
      "train epoch: 1294, loss 0.0965\n",
      "test epoch: 1294, loss: 0.85\n",
      "train epoch: 1295, loss 0.0962\n",
      "test epoch: 1295, loss: 0.89\n",
      "train epoch: 1296, loss 0.0960\n",
      "test epoch: 1296, loss: 0.87\n",
      "train epoch: 1297, loss 0.0959\n",
      "test epoch: 1297, loss: 0.88\n",
      "train epoch: 1298, loss 0.0958\n",
      "test epoch: 1298, loss: 0.86\n",
      "train epoch: 1299, loss 0.0958\n",
      "test epoch: 1299, loss: 0.87\n",
      "train epoch: 1300, loss 0.0958\n",
      "test epoch: 1300, loss: 0.87\n",
      "train epoch: 1301, loss 0.0958\n",
      "test epoch: 1301, loss: 0.87\n",
      "train epoch: 1302, loss 0.0959\n",
      "test epoch: 1302, loss: 0.88\n",
      "train epoch: 1303, loss 0.0959\n",
      "test epoch: 1303, loss: 0.85\n",
      "train epoch: 1304, loss 0.0962\n",
      "test epoch: 1304, loss: 0.89\n",
      "train epoch: 1305, loss 0.0964\n",
      "test epoch: 1305, loss: 0.84\n",
      "train epoch: 1306, loss 0.0965\n",
      "test epoch: 1306, loss: 0.90\n",
      "train epoch: 1307, loss 0.0967\n",
      "test epoch: 1307, loss: 0.84\n",
      "train epoch: 1308, loss 0.0969\n",
      "test epoch: 1308, loss: 0.90\n",
      "train epoch: 1309, loss 0.0971\n",
      "test epoch: 1309, loss: 0.83\n",
      "train epoch: 1310, loss 0.0975\n",
      "test epoch: 1310, loss: 0.91\n",
      "train epoch: 1311, loss 0.0979\n",
      "test epoch: 1311, loss: 0.82\n",
      "train epoch: 1312, loss 0.0981\n",
      "test epoch: 1312, loss: 0.91\n",
      "train epoch: 1313, loss 0.0988\n",
      "test epoch: 1313, loss: 0.81\n",
      "train epoch: 1314, loss 0.0991\n",
      "test epoch: 1314, loss: 0.92\n",
      "train epoch: 1315, loss 0.0991\n",
      "test epoch: 1315, loss: 0.82\n",
      "train epoch: 1316, loss 0.0989\n",
      "test epoch: 1316, loss: 0.92\n",
      "train epoch: 1317, loss 0.0991\n",
      "test epoch: 1317, loss: 0.81\n",
      "train epoch: 1318, loss 0.0990\n",
      "test epoch: 1318, loss: 0.91\n",
      "train epoch: 1319, loss 0.0987\n",
      "test epoch: 1319, loss: 0.82\n",
      "train epoch: 1320, loss 0.0979\n",
      "test epoch: 1320, loss: 0.91\n",
      "train epoch: 1321, loss 0.0970\n",
      "test epoch: 1321, loss: 0.86\n",
      "train epoch: 1322, loss 0.0965\n",
      "test epoch: 1322, loss: 0.89\n",
      "train epoch: 1323, loss 0.0961\n",
      "test epoch: 1323, loss: 0.87\n",
      "train epoch: 1324, loss 0.0958\n",
      "test epoch: 1324, loss: 0.86\n",
      "train epoch: 1325, loss 0.0959\n",
      "test epoch: 1325, loss: 0.88\n",
      "train epoch: 1326, loss 0.0962\n",
      "test epoch: 1326, loss: 0.84\n",
      "train epoch: 1327, loss 0.0965\n",
      "test epoch: 1327, loss: 0.90\n",
      "train epoch: 1328, loss 0.0968\n",
      "test epoch: 1328, loss: 0.84\n",
      "train epoch: 1329, loss 0.0973\n",
      "test epoch: 1329, loss: 0.92\n",
      "train epoch: 1330, loss 0.0981\n",
      "test epoch: 1330, loss: 0.82\n",
      "train epoch: 1331, loss 0.0988\n",
      "test epoch: 1331, loss: 0.92\n",
      "train epoch: 1332, loss 0.0992\n",
      "test epoch: 1332, loss: 0.81\n",
      "train epoch: 1333, loss 0.0990\n",
      "test epoch: 1333, loss: 0.91\n",
      "train epoch: 1334, loss 0.0984\n",
      "test epoch: 1334, loss: 0.83\n",
      "train epoch: 1335, loss 0.0976\n",
      "test epoch: 1335, loss: 0.90\n",
      "train epoch: 1336, loss 0.0969\n",
      "test epoch: 1336, loss: 0.85\n",
      "train epoch: 1337, loss 0.0963\n",
      "test epoch: 1337, loss: 0.88\n",
      "train epoch: 1338, loss 0.0960\n",
      "test epoch: 1338, loss: 0.85\n",
      "train epoch: 1339, loss 0.0959\n",
      "test epoch: 1339, loss: 0.87\n",
      "train epoch: 1340, loss 0.0958\n",
      "test epoch: 1340, loss: 0.87\n",
      "train epoch: 1341, loss 0.0957\n",
      "test epoch: 1341, loss: 0.87\n",
      "train epoch: 1342, loss 0.0958\n",
      "test epoch: 1342, loss: 0.88\n",
      "train epoch: 1343, loss 0.0959\n",
      "test epoch: 1343, loss: 0.86\n",
      "train epoch: 1344, loss 0.0960\n",
      "test epoch: 1344, loss: 0.88\n",
      "train epoch: 1345, loss 0.0961\n",
      "test epoch: 1345, loss: 0.84\n",
      "train epoch: 1346, loss 0.0963\n",
      "test epoch: 1346, loss: 0.89\n",
      "train epoch: 1347, loss 0.0965\n",
      "test epoch: 1347, loss: 0.84\n",
      "train epoch: 1348, loss 0.0968\n",
      "test epoch: 1348, loss: 0.90\n",
      "train epoch: 1349, loss 0.0973\n",
      "test epoch: 1349, loss: 0.82\n",
      "train epoch: 1350, loss 0.0981\n",
      "test epoch: 1350, loss: 0.92\n",
      "train epoch: 1351, loss 0.0992\n",
      "test epoch: 1351, loss: 0.80\n",
      "train epoch: 1352, loss 0.1000\n",
      "test epoch: 1352, loss: 0.93\n",
      "train epoch: 1353, loss 0.1011\n",
      "test epoch: 1353, loss: 0.80\n",
      "train epoch: 1354, loss 0.1011\n",
      "test epoch: 1354, loss: 0.93\n",
      "train epoch: 1355, loss 0.1007\n",
      "test epoch: 1355, loss: 0.82\n",
      "train epoch: 1356, loss 0.0999\n",
      "test epoch: 1356, loss: 0.92\n",
      "train epoch: 1357, loss 0.0994\n",
      "test epoch: 1357, loss: 0.81\n",
      "train epoch: 1358, loss 0.0987\n",
      "test epoch: 1358, loss: 0.90\n",
      "train epoch: 1359, loss 0.0977\n",
      "test epoch: 1359, loss: 0.84\n",
      "train epoch: 1360, loss 0.0966\n",
      "test epoch: 1360, loss: 0.89\n",
      "train epoch: 1361, loss 0.0960\n",
      "test epoch: 1361, loss: 0.88\n",
      "train epoch: 1362, loss 0.0958\n",
      "test epoch: 1362, loss: 0.87\n",
      "train epoch: 1363, loss 0.0958\n",
      "test epoch: 1363, loss: 0.88\n",
      "train epoch: 1364, loss 0.0960\n",
      "test epoch: 1364, loss: 0.85\n",
      "train epoch: 1365, loss 0.0963\n",
      "test epoch: 1365, loss: 0.89\n",
      "train epoch: 1366, loss 0.0964\n",
      "test epoch: 1366, loss: 0.84\n",
      "train epoch: 1367, loss 0.0965\n",
      "test epoch: 1367, loss: 0.90\n",
      "train epoch: 1368, loss 0.0965\n",
      "test epoch: 1368, loss: 0.85\n",
      "train epoch: 1369, loss 0.0964\n",
      "test epoch: 1369, loss: 0.89\n",
      "train epoch: 1370, loss 0.0963\n",
      "test epoch: 1370, loss: 0.84\n",
      "train epoch: 1371, loss 0.0963\n",
      "test epoch: 1371, loss: 0.88\n",
      "train epoch: 1372, loss 0.0963\n",
      "test epoch: 1372, loss: 0.85\n",
      "train epoch: 1373, loss 0.0962\n",
      "test epoch: 1373, loss: 0.89\n",
      "train epoch: 1374, loss 0.0961\n",
      "test epoch: 1374, loss: 0.85\n",
      "train epoch: 1375, loss 0.0961\n",
      "test epoch: 1375, loss: 0.87\n",
      "train epoch: 1376, loss 0.0960\n",
      "test epoch: 1376, loss: 0.84\n",
      "train epoch: 1377, loss 0.0961\n",
      "test epoch: 1377, loss: 0.87\n",
      "train epoch: 1378, loss 0.0959\n",
      "test epoch: 1378, loss: 0.86\n",
      "train epoch: 1379, loss 0.0958\n",
      "test epoch: 1379, loss: 0.88\n",
      "train epoch: 1380, loss 0.0959\n",
      "test epoch: 1380, loss: 0.86\n",
      "train epoch: 1381, loss 0.0957\n",
      "test epoch: 1381, loss: 0.87\n",
      "train epoch: 1382, loss 0.0957\n",
      "test epoch: 1382, loss: 0.86\n",
      "train epoch: 1383, loss 0.0957\n",
      "test epoch: 1383, loss: 0.86\n",
      "train epoch: 1384, loss 0.0957\n",
      "test epoch: 1384, loss: 0.87\n",
      "train epoch: 1385, loss 0.0957\n",
      "test epoch: 1385, loss: 0.87\n",
      "train epoch: 1386, loss 0.0957\n",
      "test epoch: 1386, loss: 0.87\n",
      "train epoch: 1387, loss 0.0957\n",
      "test epoch: 1387, loss: 0.86\n",
      "train epoch: 1388, loss 0.0958\n",
      "test epoch: 1388, loss: 0.88\n",
      "train epoch: 1389, loss 0.0960\n",
      "test epoch: 1389, loss: 0.84\n",
      "train epoch: 1390, loss 0.0964\n",
      "test epoch: 1390, loss: 0.90\n",
      "train epoch: 1391, loss 0.0969\n",
      "test epoch: 1391, loss: 0.83\n",
      "train epoch: 1392, loss 0.0977\n",
      "test epoch: 1392, loss: 0.92\n",
      "train epoch: 1393, loss 0.0988\n",
      "test epoch: 1393, loss: 0.80\n",
      "train epoch: 1394, loss 0.1002\n",
      "test epoch: 1394, loss: 0.94\n",
      "train epoch: 1395, loss 0.1030\n",
      "test epoch: 1395, loss: 0.77\n",
      "train epoch: 1396, loss 0.1051\n",
      "test epoch: 1396, loss: 0.95\n",
      "train epoch: 1397, loss 0.1063\n",
      "test epoch: 1397, loss: 0.77\n",
      "train epoch: 1398, loss 0.1052\n",
      "test epoch: 1398, loss: 0.95\n",
      "train epoch: 1399, loss 0.1032\n",
      "test epoch: 1399, loss: 0.81\n",
      "train epoch: 1400, loss 0.1013\n",
      "test epoch: 1400, loss: 0.93\n",
      "train epoch: 1401, loss 0.0997\n",
      "test epoch: 1401, loss: 0.81\n",
      "train epoch: 1402, loss 0.0984\n",
      "test epoch: 1402, loss: 0.89\n",
      "train epoch: 1403, loss 0.0971\n",
      "test epoch: 1403, loss: 0.85\n",
      "train epoch: 1404, loss 0.0961\n",
      "test epoch: 1404, loss: 0.87\n",
      "train epoch: 1405, loss 0.0958\n",
      "test epoch: 1405, loss: 0.90\n",
      "train epoch: 1406, loss 0.0962\n",
      "test epoch: 1406, loss: 0.85\n",
      "train epoch: 1407, loss 0.0969\n",
      "test epoch: 1407, loss: 0.92\n",
      "train epoch: 1408, loss 0.0977\n",
      "test epoch: 1408, loss: 0.82\n",
      "train epoch: 1409, loss 0.0986\n",
      "test epoch: 1409, loss: 0.92\n",
      "train epoch: 1410, loss 0.0990\n",
      "test epoch: 1410, loss: 0.82\n",
      "train epoch: 1411, loss 0.0985\n",
      "test epoch: 1411, loss: 0.90\n",
      "train epoch: 1412, loss 0.0974\n",
      "test epoch: 1412, loss: 0.85\n",
      "train epoch: 1413, loss 0.0965\n",
      "test epoch: 1413, loss: 0.88\n",
      "train epoch: 1414, loss 0.0959\n",
      "test epoch: 1414, loss: 0.86\n",
      "train epoch: 1415, loss 0.0957\n",
      "test epoch: 1415, loss: 0.85\n",
      "train epoch: 1416, loss 0.0958\n",
      "test epoch: 1416, loss: 0.89\n",
      "train epoch: 1417, loss 0.0961\n",
      "test epoch: 1417, loss: 0.85\n",
      "train epoch: 1418, loss 0.0966\n",
      "test epoch: 1418, loss: 0.91\n",
      "train epoch: 1419, loss 0.0974\n",
      "test epoch: 1419, loss: 0.81\n",
      "train epoch: 1420, loss 0.0984\n",
      "test epoch: 1420, loss: 0.91\n",
      "train epoch: 1421, loss 0.0991\n",
      "test epoch: 1421, loss: 0.81\n",
      "train epoch: 1422, loss 0.0989\n",
      "test epoch: 1422, loss: 0.91\n",
      "train epoch: 1423, loss 0.0982\n",
      "test epoch: 1423, loss: 0.83\n",
      "train epoch: 1424, loss 0.0975\n",
      "test epoch: 1424, loss: 0.89\n",
      "train epoch: 1425, loss 0.0968\n",
      "test epoch: 1425, loss: 0.83\n",
      "train epoch: 1426, loss 0.0964\n",
      "test epoch: 1426, loss: 0.87\n",
      "train epoch: 1427, loss 0.0960\n",
      "test epoch: 1427, loss: 0.86\n",
      "train epoch: 1428, loss 0.0957\n",
      "test epoch: 1428, loss: 0.86\n",
      "train epoch: 1429, loss 0.0958\n",
      "test epoch: 1429, loss: 0.89\n",
      "train epoch: 1430, loss 0.0960\n",
      "test epoch: 1430, loss: 0.86\n",
      "train epoch: 1431, loss 0.0959\n",
      "test epoch: 1431, loss: 0.88\n",
      "train epoch: 1432, loss 0.0960\n",
      "test epoch: 1432, loss: 0.84\n",
      "train epoch: 1433, loss 0.0962\n",
      "test epoch: 1433, loss: 0.87\n",
      "train epoch: 1434, loss 0.0962\n",
      "test epoch: 1434, loss: 0.85\n",
      "train epoch: 1435, loss 0.0960\n",
      "test epoch: 1435, loss: 0.88\n",
      "train epoch: 1436, loss 0.0957\n",
      "test epoch: 1436, loss: 0.87\n",
      "train epoch: 1437, loss 0.0959\n",
      "test epoch: 1437, loss: 0.89\n",
      "train epoch: 1438, loss 0.0960\n",
      "test epoch: 1438, loss: 0.87\n",
      "train epoch: 1439, loss 0.0956\n",
      "test epoch: 1439, loss: 0.85\n",
      "train epoch: 1440, loss 0.0959\n",
      "test epoch: 1440, loss: 0.87\n",
      "train epoch: 1441, loss 0.0961\n",
      "test epoch: 1441, loss: 0.84\n",
      "train epoch: 1442, loss 0.0962\n",
      "test epoch: 1442, loss: 0.89\n",
      "train epoch: 1443, loss 0.0963\n",
      "test epoch: 1443, loss: 0.84\n",
      "train epoch: 1444, loss 0.0966\n",
      "test epoch: 1444, loss: 0.91\n",
      "train epoch: 1445, loss 0.0972\n",
      "test epoch: 1445, loss: 0.83\n",
      "train epoch: 1446, loss 0.0980\n",
      "test epoch: 1446, loss: 0.92\n",
      "train epoch: 1447, loss 0.0989\n",
      "test epoch: 1447, loss: 0.81\n",
      "train epoch: 1448, loss 0.0996\n",
      "test epoch: 1448, loss: 0.92\n",
      "train epoch: 1449, loss 0.0998\n",
      "test epoch: 1449, loss: 0.81\n",
      "train epoch: 1450, loss 0.0995\n",
      "test epoch: 1450, loss: 0.92\n",
      "train epoch: 1451, loss 0.0993\n",
      "test epoch: 1451, loss: 0.81\n",
      "train epoch: 1452, loss 0.0987\n",
      "test epoch: 1452, loss: 0.91\n",
      "train epoch: 1453, loss 0.0983\n",
      "test epoch: 1453, loss: 0.83\n",
      "train epoch: 1454, loss 0.0975\n",
      "test epoch: 1454, loss: 0.90\n",
      "train epoch: 1455, loss 0.0969\n",
      "test epoch: 1455, loss: 0.85\n",
      "train epoch: 1456, loss 0.0965\n",
      "test epoch: 1456, loss: 0.89\n",
      "train epoch: 1457, loss 0.0961\n",
      "test epoch: 1457, loss: 0.84\n",
      "train epoch: 1458, loss 0.0960\n",
      "test epoch: 1458, loss: 0.87\n",
      "train epoch: 1459, loss 0.0958\n",
      "test epoch: 1459, loss: 0.86\n",
      "train epoch: 1460, loss 0.0956\n",
      "test epoch: 1460, loss: 0.87\n",
      "train epoch: 1461, loss 0.0956\n",
      "test epoch: 1461, loss: 0.88\n",
      "train epoch: 1462, loss 0.0957\n",
      "test epoch: 1462, loss: 0.85\n",
      "train epoch: 1463, loss 0.0958\n",
      "test epoch: 1463, loss: 0.88\n",
      "train epoch: 1464, loss 0.0960\n",
      "test epoch: 1464, loss: 0.84\n",
      "train epoch: 1465, loss 0.0962\n",
      "test epoch: 1465, loss: 0.89\n",
      "train epoch: 1466, loss 0.0964\n",
      "test epoch: 1466, loss: 0.84\n",
      "train epoch: 1467, loss 0.0967\n",
      "test epoch: 1467, loss: 0.90\n",
      "train epoch: 1468, loss 0.0972\n",
      "test epoch: 1468, loss: 0.82\n",
      "train epoch: 1469, loss 0.0978\n",
      "test epoch: 1469, loss: 0.91\n",
      "train epoch: 1470, loss 0.0982\n",
      "test epoch: 1470, loss: 0.82\n",
      "train epoch: 1471, loss 0.0983\n",
      "test epoch: 1471, loss: 0.91\n",
      "train epoch: 1472, loss 0.0987\n",
      "test epoch: 1472, loss: 0.81\n",
      "train epoch: 1473, loss 0.0988\n",
      "test epoch: 1473, loss: 0.92\n",
      "train epoch: 1474, loss 0.0989\n",
      "test epoch: 1474, loss: 0.82\n",
      "train epoch: 1475, loss 0.0988\n",
      "test epoch: 1475, loss: 0.92\n",
      "train epoch: 1476, loss 0.0987\n",
      "test epoch: 1476, loss: 0.82\n",
      "train epoch: 1477, loss 0.0984\n",
      "test epoch: 1477, loss: 0.91\n",
      "train epoch: 1478, loss 0.0982\n",
      "test epoch: 1478, loss: 0.82\n",
      "train epoch: 1479, loss 0.0977\n",
      "test epoch: 1479, loss: 0.89\n",
      "train epoch: 1480, loss 0.0969\n",
      "test epoch: 1480, loss: 0.84\n",
      "train epoch: 1481, loss 0.0962\n",
      "test epoch: 1481, loss: 0.88\n",
      "train epoch: 1482, loss 0.0958\n",
      "test epoch: 1482, loss: 0.88\n",
      "train epoch: 1483, loss 0.0958\n",
      "test epoch: 1483, loss: 0.87\n",
      "train epoch: 1484, loss 0.0958\n",
      "test epoch: 1484, loss: 0.88\n",
      "train epoch: 1485, loss 0.0960\n",
      "test epoch: 1485, loss: 0.83\n",
      "train epoch: 1486, loss 0.0966\n",
      "test epoch: 1486, loss: 0.88\n",
      "train epoch: 1487, loss 0.0970\n",
      "test epoch: 1487, loss: 0.82\n",
      "train epoch: 1488, loss 0.0970\n",
      "test epoch: 1488, loss: 0.90\n",
      "train epoch: 1489, loss 0.0967\n",
      "test epoch: 1489, loss: 0.85\n",
      "train epoch: 1490, loss 0.0966\n",
      "test epoch: 1490, loss: 0.91\n",
      "train epoch: 1491, loss 0.0965\n",
      "test epoch: 1491, loss: 0.85\n",
      "train epoch: 1492, loss 0.0963\n",
      "test epoch: 1492, loss: 0.89\n",
      "train epoch: 1493, loss 0.0963\n",
      "test epoch: 1493, loss: 0.83\n",
      "train epoch: 1494, loss 0.0966\n",
      "test epoch: 1494, loss: 0.88\n",
      "train epoch: 1495, loss 0.0966\n",
      "test epoch: 1495, loss: 0.83\n",
      "train epoch: 1496, loss 0.0963\n",
      "test epoch: 1496, loss: 0.88\n",
      "train epoch: 1497, loss 0.0959\n",
      "test epoch: 1497, loss: 0.86\n",
      "train epoch: 1498, loss 0.0961\n",
      "test epoch: 1498, loss: 0.89\n",
      "train epoch: 1499, loss 0.0961\n",
      "test epoch: 1499, loss: 0.84\n",
      "train epoch: 1500, loss 0.0964\n",
      "test epoch: 1500, loss: 0.89\n",
      "train epoch: 1501, loss 0.0967\n",
      "test epoch: 1501, loss: 0.83\n",
      "train epoch: 1502, loss 0.0969\n",
      "test epoch: 1502, loss: 0.90\n",
      "train epoch: 1503, loss 0.0969\n",
      "test epoch: 1503, loss: 0.84\n",
      "train epoch: 1504, loss 0.0971\n",
      "test epoch: 1504, loss: 0.91\n",
      "train epoch: 1505, loss 0.0974\n",
      "test epoch: 1505, loss: 0.82\n",
      "train epoch: 1506, loss 0.0978\n",
      "test epoch: 1506, loss: 0.91\n",
      "train epoch: 1507, loss 0.0982\n",
      "test epoch: 1507, loss: 0.82\n",
      "train epoch: 1508, loss 0.0987\n",
      "test epoch: 1508, loss: 0.93\n",
      "train epoch: 1509, loss 0.0998\n",
      "test epoch: 1509, loss: 0.80\n",
      "train epoch: 1510, loss 0.1008\n",
      "test epoch: 1510, loss: 0.93\n",
      "train epoch: 1511, loss 0.1021\n",
      "test epoch: 1511, loss: 0.78\n",
      "train epoch: 1512, loss 0.1023\n",
      "test epoch: 1512, loss: 0.93\n",
      "train epoch: 1513, loss 0.1015\n",
      "test epoch: 1513, loss: 0.80\n",
      "train epoch: 1514, loss 0.0995\n",
      "test epoch: 1514, loss: 0.91\n",
      "train epoch: 1515, loss 0.0978\n",
      "test epoch: 1515, loss: 0.84\n",
      "train epoch: 1516, loss 0.0966\n",
      "test epoch: 1516, loss: 0.89\n",
      "train epoch: 1517, loss 0.0958\n",
      "test epoch: 1517, loss: 0.86\n",
      "train epoch: 1518, loss 0.0956\n",
      "test epoch: 1518, loss: 0.85\n",
      "train epoch: 1519, loss 0.0959\n",
      "test epoch: 1519, loss: 0.89\n",
      "train epoch: 1520, loss 0.0966\n",
      "test epoch: 1520, loss: 0.82\n",
      "train epoch: 1521, loss 0.0973\n",
      "test epoch: 1521, loss: 0.91\n",
      "train epoch: 1522, loss 0.0981\n",
      "test epoch: 1522, loss: 0.82\n",
      "train epoch: 1523, loss 0.0987\n",
      "test epoch: 1523, loss: 0.92\n",
      "train epoch: 1524, loss 0.0990\n",
      "test epoch: 1524, loss: 0.81\n",
      "train epoch: 1525, loss 0.0988\n",
      "test epoch: 1525, loss: 0.91\n",
      "train epoch: 1526, loss 0.0983\n",
      "test epoch: 1526, loss: 0.82\n",
      "train epoch: 1527, loss 0.0975\n",
      "test epoch: 1527, loss: 0.89\n",
      "train epoch: 1528, loss 0.0966\n",
      "test epoch: 1528, loss: 0.85\n",
      "train epoch: 1529, loss 0.0958\n",
      "test epoch: 1529, loss: 0.87\n",
      "train epoch: 1530, loss 0.0955\n",
      "test epoch: 1530, loss: 0.88\n",
      "train epoch: 1531, loss 0.0957\n",
      "test epoch: 1531, loss: 0.84\n",
      "train epoch: 1532, loss 0.0963\n",
      "test epoch: 1532, loss: 0.90\n",
      "train epoch: 1533, loss 0.0972\n",
      "test epoch: 1533, loss: 0.82\n",
      "train epoch: 1534, loss 0.0982\n",
      "test epoch: 1534, loss: 0.92\n",
      "train epoch: 1535, loss 0.0993\n",
      "test epoch: 1535, loss: 0.80\n",
      "train epoch: 1536, loss 0.0998\n",
      "test epoch: 1536, loss: 0.92\n",
      "train epoch: 1537, loss 0.0998\n",
      "test epoch: 1537, loss: 0.80\n",
      "train epoch: 1538, loss 0.0990\n",
      "test epoch: 1538, loss: 0.90\n",
      "train epoch: 1539, loss 0.0976\n",
      "test epoch: 1539, loss: 0.84\n",
      "train epoch: 1540, loss 0.0964\n",
      "test epoch: 1540, loss: 0.88\n",
      "train epoch: 1541, loss 0.0957\n",
      "test epoch: 1541, loss: 0.87\n",
      "train epoch: 1542, loss 0.0955\n",
      "test epoch: 1542, loss: 0.85\n",
      "train epoch: 1543, loss 0.0957\n",
      "test epoch: 1543, loss: 0.89\n",
      "train epoch: 1544, loss 0.0962\n",
      "test epoch: 1544, loss: 0.84\n",
      "train epoch: 1545, loss 0.0965\n",
      "test epoch: 1545, loss: 0.90\n",
      "train epoch: 1546, loss 0.0971\n",
      "test epoch: 1546, loss: 0.82\n",
      "train epoch: 1547, loss 0.0977\n",
      "test epoch: 1547, loss: 0.91\n",
      "train epoch: 1548, loss 0.0979\n",
      "test epoch: 1548, loss: 0.82\n",
      "train epoch: 1549, loss 0.0979\n",
      "test epoch: 1549, loss: 0.90\n",
      "train epoch: 1550, loss 0.0974\n",
      "test epoch: 1550, loss: 0.83\n",
      "train epoch: 1551, loss 0.0968\n",
      "test epoch: 1551, loss: 0.88\n",
      "train epoch: 1552, loss 0.0964\n",
      "test epoch: 1552, loss: 0.84\n",
      "train epoch: 1553, loss 0.0959\n",
      "test epoch: 1553, loss: 0.87\n",
      "train epoch: 1554, loss 0.0955\n",
      "test epoch: 1554, loss: 0.87\n",
      "train epoch: 1555, loss 0.0955\n",
      "test epoch: 1555, loss: 0.86\n",
      "train epoch: 1556, loss 0.0958\n",
      "test epoch: 1556, loss: 0.89\n",
      "train epoch: 1557, loss 0.0960\n",
      "test epoch: 1557, loss: 0.84\n",
      "train epoch: 1558, loss 0.0963\n",
      "test epoch: 1558, loss: 0.89\n",
      "train epoch: 1559, loss 0.0969\n",
      "test epoch: 1559, loss: 0.82\n",
      "train epoch: 1560, loss 0.0971\n",
      "test epoch: 1560, loss: 0.89\n",
      "train epoch: 1561, loss 0.0969\n",
      "test epoch: 1561, loss: 0.84\n",
      "train epoch: 1562, loss 0.0967\n",
      "test epoch: 1562, loss: 0.89\n",
      "train epoch: 1563, loss 0.0965\n",
      "test epoch: 1563, loss: 0.83\n",
      "train epoch: 1564, loss 0.0963\n",
      "test epoch: 1564, loss: 0.88\n",
      "train epoch: 1565, loss 0.0960\n",
      "test epoch: 1565, loss: 0.85\n",
      "train epoch: 1566, loss 0.0958\n",
      "test epoch: 1566, loss: 0.88\n",
      "train epoch: 1567, loss 0.0956\n",
      "test epoch: 1567, loss: 0.86\n",
      "train epoch: 1568, loss 0.0955\n",
      "test epoch: 1568, loss: 0.87\n",
      "train epoch: 1569, loss 0.0955\n",
      "test epoch: 1569, loss: 0.86\n",
      "train epoch: 1570, loss 0.0955\n",
      "test epoch: 1570, loss: 0.87\n",
      "train epoch: 1571, loss 0.0955\n",
      "test epoch: 1571, loss: 0.87\n",
      "train epoch: 1572, loss 0.0955\n",
      "test epoch: 1572, loss: 0.87\n",
      "train epoch: 1573, loss 0.0955\n",
      "test epoch: 1573, loss: 0.85\n",
      "train epoch: 1574, loss 0.0955\n",
      "test epoch: 1574, loss: 0.87\n",
      "train epoch: 1575, loss 0.0955\n",
      "test epoch: 1575, loss: 0.86\n",
      "train epoch: 1576, loss 0.0955\n",
      "test epoch: 1576, loss: 0.88\n",
      "train epoch: 1577, loss 0.0956\n",
      "test epoch: 1577, loss: 0.85\n",
      "train epoch: 1578, loss 0.0957\n",
      "test epoch: 1578, loss: 0.88\n",
      "train epoch: 1579, loss 0.0960\n",
      "test epoch: 1579, loss: 0.83\n",
      "train epoch: 1580, loss 0.0965\n",
      "test epoch: 1580, loss: 0.89\n",
      "train epoch: 1581, loss 0.0968\n",
      "test epoch: 1581, loss: 0.83\n",
      "train epoch: 1582, loss 0.0973\n",
      "test epoch: 1582, loss: 0.91\n",
      "train epoch: 1583, loss 0.0982\n",
      "test epoch: 1583, loss: 0.80\n",
      "train epoch: 1584, loss 0.0996\n",
      "test epoch: 1584, loss: 0.93\n",
      "train epoch: 1585, loss 0.1017\n",
      "test epoch: 1585, loss: 0.78\n",
      "train epoch: 1586, loss 0.1036\n",
      "test epoch: 1586, loss: 0.95\n",
      "train epoch: 1587, loss 0.1050\n",
      "test epoch: 1587, loss: 0.78\n",
      "train epoch: 1588, loss 0.1052\n",
      "test epoch: 1588, loss: 0.95\n",
      "train epoch: 1589, loss 0.1054\n",
      "test epoch: 1589, loss: 0.77\n",
      "train epoch: 1590, loss 0.1038\n",
      "test epoch: 1590, loss: 0.91\n",
      "train epoch: 1591, loss 0.1003\n",
      "test epoch: 1591, loss: 0.83\n",
      "train epoch: 1592, loss 0.0969\n",
      "test epoch: 1592, loss: 0.88\n",
      "train epoch: 1593, loss 0.0955\n",
      "test epoch: 1593, loss: 0.91\n",
      "train epoch: 1594, loss 0.0965\n",
      "test epoch: 1594, loss: 0.84\n",
      "train epoch: 1595, loss 0.0986\n",
      "test epoch: 1595, loss: 0.94\n",
      "train epoch: 1596, loss 0.1018\n",
      "test epoch: 1596, loss: 0.76\n",
      "train epoch: 1597, loss 0.1059\n",
      "test epoch: 1597, loss: 0.95\n",
      "train epoch: 1598, loss 0.1062\n",
      "test epoch: 1598, loss: 0.78\n",
      "train epoch: 1599, loss 0.1025\n",
      "test epoch: 1599, loss: 0.92\n",
      "train epoch: 1600, loss 0.0981\n",
      "test epoch: 1600, loss: 0.87\n",
      "train epoch: 1601, loss 0.0963\n",
      "test epoch: 1601, loss: 0.87\n",
      "train epoch: 1602, loss 0.0960\n",
      "test epoch: 1602, loss: 0.90\n",
      "train epoch: 1603, loss 0.0971\n",
      "test epoch: 1603, loss: 0.80\n",
      "train epoch: 1604, loss 0.0994\n",
      "test epoch: 1604, loss: 0.91\n",
      "train epoch: 1605, loss 0.1001\n",
      "test epoch: 1605, loss: 0.81\n",
      "train epoch: 1606, loss 0.0987\n",
      "test epoch: 1606, loss: 0.90\n",
      "train epoch: 1607, loss 0.0968\n",
      "test epoch: 1607, loss: 0.87\n",
      "train epoch: 1608, loss 0.0961\n",
      "test epoch: 1608, loss: 0.88\n",
      "train epoch: 1609, loss 0.0956\n",
      "test epoch: 1609, loss: 0.87\n",
      "train epoch: 1610, loss 0.0956\n",
      "test epoch: 1610, loss: 0.84\n",
      "train epoch: 1611, loss 0.0960\n",
      "test epoch: 1611, loss: 0.88\n",
      "train epoch: 1612, loss 0.0963\n",
      "test epoch: 1612, loss: 0.83\n",
      "train epoch: 1613, loss 0.0962\n",
      "test epoch: 1613, loss: 0.88\n",
      "train epoch: 1614, loss 0.0958\n",
      "test epoch: 1614, loss: 0.86\n",
      "train epoch: 1615, loss 0.0957\n",
      "test epoch: 1615, loss: 0.87\n",
      "train epoch: 1616, loss 0.0956\n",
      "test epoch: 1616, loss: 0.87\n",
      "train epoch: 1617, loss 0.0956\n",
      "test epoch: 1617, loss: 0.84\n",
      "train epoch: 1618, loss 0.0960\n",
      "test epoch: 1618, loss: 0.88\n",
      "train epoch: 1619, loss 0.0963\n",
      "test epoch: 1619, loss: 0.83\n",
      "train epoch: 1620, loss 0.0964\n",
      "test epoch: 1620, loss: 0.89\n",
      "train epoch: 1621, loss 0.0963\n",
      "test epoch: 1621, loss: 0.85\n",
      "train epoch: 1622, loss 0.0960\n",
      "test epoch: 1622, loss: 0.89\n",
      "train epoch: 1623, loss 0.0959\n",
      "test epoch: 1623, loss: 0.85\n",
      "train epoch: 1624, loss 0.0956\n",
      "test epoch: 1624, loss: 0.86\n",
      "train epoch: 1625, loss 0.0955\n",
      "test epoch: 1625, loss: 0.86\n",
      "train epoch: 1626, loss 0.0955\n",
      "test epoch: 1626, loss: 0.85\n",
      "train epoch: 1627, loss 0.0956\n",
      "test epoch: 1627, loss: 0.88\n",
      "train epoch: 1628, loss 0.0958\n",
      "test epoch: 1628, loss: 0.84\n",
      "train epoch: 1629, loss 0.0960\n",
      "test epoch: 1629, loss: 0.89\n",
      "train epoch: 1630, loss 0.0960\n",
      "test epoch: 1630, loss: 0.84\n",
      "train epoch: 1631, loss 0.0959\n",
      "test epoch: 1631, loss: 0.88\n",
      "train epoch: 1632, loss 0.0957\n",
      "test epoch: 1632, loss: 0.85\n",
      "train epoch: 1633, loss 0.0956\n",
      "test epoch: 1633, loss: 0.87\n",
      "train epoch: 1634, loss 0.0955\n",
      "test epoch: 1634, loss: 0.85\n",
      "train epoch: 1635, loss 0.0954\n",
      "test epoch: 1635, loss: 0.86\n",
      "train epoch: 1636, loss 0.0954\n",
      "test epoch: 1636, loss: 0.86\n",
      "train epoch: 1637, loss 0.0954\n",
      "test epoch: 1637, loss: 0.86\n",
      "train epoch: 1638, loss 0.0954\n",
      "test epoch: 1638, loss: 0.86\n",
      "train epoch: 1639, loss 0.0954\n",
      "test epoch: 1639, loss: 0.86\n",
      "train epoch: 1640, loss 0.0953\n",
      "test epoch: 1640, loss: 0.87\n",
      "train epoch: 1641, loss 0.0953\n",
      "test epoch: 1641, loss: 0.86\n",
      "train epoch: 1642, loss 0.0954\n",
      "test epoch: 1642, loss: 0.87\n",
      "train epoch: 1643, loss 0.0955\n",
      "test epoch: 1643, loss: 0.84\n",
      "train epoch: 1644, loss 0.0957\n",
      "test epoch: 1644, loss: 0.88\n",
      "train epoch: 1645, loss 0.0959\n",
      "test epoch: 1645, loss: 0.84\n",
      "train epoch: 1646, loss 0.0961\n",
      "test epoch: 1646, loss: 0.90\n",
      "train epoch: 1647, loss 0.0964\n",
      "test epoch: 1647, loss: 0.84\n",
      "train epoch: 1648, loss 0.0966\n",
      "test epoch: 1648, loss: 0.90\n",
      "train epoch: 1649, loss 0.0966\n",
      "test epoch: 1649, loss: 0.83\n",
      "train epoch: 1650, loss 0.0966\n",
      "test epoch: 1650, loss: 0.89\n",
      "train epoch: 1651, loss 0.0966\n",
      "test epoch: 1651, loss: 0.83\n",
      "train epoch: 1652, loss 0.0965\n",
      "test epoch: 1652, loss: 0.89\n",
      "train epoch: 1653, loss 0.0963\n",
      "test epoch: 1653, loss: 0.84\n",
      "train epoch: 1654, loss 0.0963\n",
      "test epoch: 1654, loss: 0.89\n",
      "train epoch: 1655, loss 0.0962\n",
      "test epoch: 1655, loss: 0.83\n",
      "train epoch: 1656, loss 0.0960\n",
      "test epoch: 1656, loss: 0.88\n",
      "train epoch: 1657, loss 0.0958\n",
      "test epoch: 1657, loss: 0.84\n",
      "train epoch: 1658, loss 0.0956\n",
      "test epoch: 1658, loss: 0.87\n",
      "train epoch: 1659, loss 0.0954\n",
      "test epoch: 1659, loss: 0.86\n",
      "train epoch: 1660, loss 0.0954\n",
      "test epoch: 1660, loss: 0.87\n",
      "train epoch: 1661, loss 0.0954\n",
      "test epoch: 1661, loss: 0.86\n",
      "train epoch: 1662, loss 0.0953\n",
      "test epoch: 1662, loss: 0.84\n",
      "train epoch: 1663, loss 0.0956\n",
      "test epoch: 1663, loss: 0.87\n",
      "train epoch: 1664, loss 0.0958\n",
      "test epoch: 1664, loss: 0.84\n",
      "train epoch: 1665, loss 0.0959\n",
      "test epoch: 1665, loss: 0.90\n",
      "train epoch: 1666, loss 0.0963\n",
      "test epoch: 1666, loss: 0.84\n",
      "train epoch: 1667, loss 0.0966\n",
      "test epoch: 1667, loss: 0.90\n",
      "train epoch: 1668, loss 0.0972\n",
      "test epoch: 1668, loss: 0.81\n",
      "train epoch: 1669, loss 0.0980\n",
      "test epoch: 1669, loss: 0.90\n",
      "train epoch: 1670, loss 0.0985\n",
      "test epoch: 1670, loss: 0.81\n",
      "train epoch: 1671, loss 0.0986\n",
      "test epoch: 1671, loss: 0.92\n",
      "train epoch: 1672, loss 0.0988\n",
      "test epoch: 1672, loss: 0.81\n",
      "train epoch: 1673, loss 0.0994\n",
      "test epoch: 1673, loss: 0.93\n",
      "train epoch: 1674, loss 0.1007\n",
      "test epoch: 1674, loss: 0.78\n",
      "train epoch: 1675, loss 0.1017\n",
      "test epoch: 1675, loss: 0.93\n",
      "train epoch: 1676, loss 0.1015\n",
      "test epoch: 1676, loss: 0.79\n",
      "train epoch: 1677, loss 0.1000\n",
      "test epoch: 1677, loss: 0.91\n",
      "train epoch: 1678, loss 0.0981\n",
      "test epoch: 1678, loss: 0.84\n",
      "train epoch: 1679, loss 0.0967\n",
      "test epoch: 1679, loss: 0.89\n",
      "train epoch: 1680, loss 0.0958\n",
      "test epoch: 1680, loss: 0.87\n",
      "train epoch: 1681, loss 0.0954\n",
      "test epoch: 1681, loss: 0.84\n",
      "train epoch: 1682, loss 0.0959\n",
      "test epoch: 1682, loss: 0.89\n",
      "train epoch: 1683, loss 0.0967\n",
      "test epoch: 1683, loss: 0.82\n",
      "train epoch: 1684, loss 0.0973\n",
      "test epoch: 1684, loss: 0.91\n",
      "train epoch: 1685, loss 0.0976\n",
      "test epoch: 1685, loss: 0.82\n",
      "train epoch: 1686, loss 0.0977\n",
      "test epoch: 1686, loss: 0.92\n",
      "train epoch: 1687, loss 0.0977\n",
      "test epoch: 1687, loss: 0.82\n",
      "train epoch: 1688, loss 0.0976\n",
      "test epoch: 1688, loss: 0.91\n",
      "train epoch: 1689, loss 0.0974\n",
      "test epoch: 1689, loss: 0.82\n",
      "train epoch: 1690, loss 0.0971\n",
      "test epoch: 1690, loss: 0.89\n",
      "train epoch: 1691, loss 0.0966\n",
      "test epoch: 1691, loss: 0.83\n",
      "train epoch: 1692, loss 0.0961\n",
      "test epoch: 1692, loss: 0.88\n",
      "train epoch: 1693, loss 0.0957\n",
      "test epoch: 1693, loss: 0.84\n",
      "train epoch: 1694, loss 0.0954\n",
      "test epoch: 1694, loss: 0.86\n",
      "train epoch: 1695, loss 0.0953\n",
      "test epoch: 1695, loss: 0.86\n",
      "train epoch: 1696, loss 0.0952\n",
      "test epoch: 1696, loss: 0.86\n",
      "train epoch: 1697, loss 0.0953\n",
      "test epoch: 1697, loss: 0.87\n",
      "train epoch: 1698, loss 0.0955\n",
      "test epoch: 1698, loss: 0.84\n",
      "train epoch: 1699, loss 0.0958\n",
      "test epoch: 1699, loss: 0.89\n",
      "train epoch: 1700, loss 0.0962\n",
      "test epoch: 1700, loss: 0.83\n",
      "train epoch: 1701, loss 0.0969\n",
      "test epoch: 1701, loss: 0.91\n",
      "train epoch: 1702, loss 0.0978\n",
      "test epoch: 1702, loss: 0.81\n",
      "train epoch: 1703, loss 0.0984\n",
      "test epoch: 1703, loss: 0.91\n",
      "train epoch: 1704, loss 0.0993\n",
      "test epoch: 1704, loss: 0.79\n",
      "train epoch: 1705, loss 0.0995\n",
      "test epoch: 1705, loss: 0.90\n",
      "train epoch: 1706, loss 0.0988\n",
      "test epoch: 1706, loss: 0.81\n",
      "train epoch: 1707, loss 0.0978\n",
      "test epoch: 1707, loss: 0.91\n",
      "train epoch: 1708, loss 0.0969\n",
      "test epoch: 1708, loss: 0.84\n",
      "train epoch: 1709, loss 0.0967\n",
      "test epoch: 1709, loss: 0.89\n",
      "train epoch: 1710, loss 0.0963\n",
      "test epoch: 1710, loss: 0.83\n",
      "train epoch: 1711, loss 0.0963\n",
      "test epoch: 1711, loss: 0.88\n",
      "train epoch: 1712, loss 0.0961\n",
      "test epoch: 1712, loss: 0.84\n",
      "train epoch: 1713, loss 0.0957\n",
      "test epoch: 1713, loss: 0.87\n",
      "train epoch: 1714, loss 0.0954\n",
      "test epoch: 1714, loss: 0.87\n",
      "train epoch: 1715, loss 0.0953\n",
      "test epoch: 1715, loss: 0.87\n",
      "train epoch: 1716, loss 0.0953\n",
      "test epoch: 1716, loss: 0.88\n",
      "train epoch: 1717, loss 0.0954\n",
      "test epoch: 1717, loss: 0.85\n",
      "train epoch: 1718, loss 0.0955\n",
      "test epoch: 1718, loss: 0.88\n",
      "train epoch: 1719, loss 0.0957\n",
      "test epoch: 1719, loss: 0.83\n",
      "train epoch: 1720, loss 0.0959\n",
      "test epoch: 1720, loss: 0.88\n",
      "train epoch: 1721, loss 0.0959\n",
      "test epoch: 1721, loss: 0.84\n",
      "train epoch: 1722, loss 0.0958\n",
      "test epoch: 1722, loss: 0.89\n",
      "train epoch: 1723, loss 0.0958\n",
      "test epoch: 1723, loss: 0.84\n",
      "train epoch: 1724, loss 0.0959\n",
      "test epoch: 1724, loss: 0.89\n",
      "train epoch: 1725, loss 0.0959\n",
      "test epoch: 1725, loss: 0.84\n",
      "train epoch: 1726, loss 0.0960\n",
      "test epoch: 1726, loss: 0.89\n",
      "train epoch: 1727, loss 0.0960\n",
      "test epoch: 1727, loss: 0.83\n",
      "train epoch: 1728, loss 0.0962\n",
      "test epoch: 1728, loss: 0.89\n",
      "train epoch: 1729, loss 0.0965\n",
      "test epoch: 1729, loss: 0.83\n",
      "train epoch: 1730, loss 0.0970\n",
      "test epoch: 1730, loss: 0.90\n",
      "train epoch: 1731, loss 0.0975\n",
      "test epoch: 1731, loss: 0.81\n",
      "train epoch: 1732, loss 0.0978\n",
      "test epoch: 1732, loss: 0.90\n",
      "train epoch: 1733, loss 0.0980\n",
      "test epoch: 1733, loss: 0.81\n",
      "train epoch: 1734, loss 0.0980\n",
      "test epoch: 1734, loss: 0.91\n",
      "train epoch: 1735, loss 0.0981\n",
      "test epoch: 1735, loss: 0.81\n",
      "train epoch: 1736, loss 0.0986\n",
      "test epoch: 1736, loss: 0.92\n",
      "train epoch: 1737, loss 0.0997\n",
      "test epoch: 1737, loss: 0.78\n",
      "train epoch: 1738, loss 0.1009\n",
      "test epoch: 1738, loss: 0.92\n",
      "train epoch: 1739, loss 0.1011\n",
      "test epoch: 1739, loss: 0.79\n",
      "train epoch: 1740, loss 0.1000\n",
      "test epoch: 1740, loss: 0.92\n",
      "train epoch: 1741, loss 0.0985\n",
      "test epoch: 1741, loss: 0.83\n",
      "train epoch: 1742, loss 0.0975\n",
      "test epoch: 1742, loss: 0.90\n",
      "train epoch: 1743, loss 0.0966\n",
      "test epoch: 1743, loss: 0.83\n",
      "train epoch: 1744, loss 0.0962\n",
      "test epoch: 1744, loss: 0.88\n",
      "train epoch: 1745, loss 0.0958\n",
      "test epoch: 1745, loss: 0.85\n",
      "train epoch: 1746, loss 0.0954\n",
      "test epoch: 1746, loss: 0.87\n",
      "train epoch: 1747, loss 0.0952\n",
      "test epoch: 1747, loss: 0.88\n",
      "train epoch: 1748, loss 0.0952\n",
      "test epoch: 1748, loss: 0.87\n",
      "train epoch: 1749, loss 0.0955\n",
      "test epoch: 1749, loss: 0.90\n",
      "train epoch: 1750, loss 0.0958\n",
      "test epoch: 1750, loss: 0.84\n",
      "train epoch: 1751, loss 0.0964\n",
      "test epoch: 1751, loss: 0.90\n",
      "train epoch: 1752, loss 0.0969\n",
      "test epoch: 1752, loss: 0.83\n",
      "train epoch: 1753, loss 0.0971\n",
      "test epoch: 1753, loss: 0.90\n",
      "train epoch: 1754, loss 0.0974\n",
      "test epoch: 1754, loss: 0.82\n",
      "train epoch: 1755, loss 0.0971\n",
      "test epoch: 1755, loss: 0.90\n",
      "train epoch: 1756, loss 0.0966\n",
      "test epoch: 1756, loss: 0.83\n",
      "train epoch: 1757, loss 0.0964\n",
      "test epoch: 1757, loss: 0.88\n",
      "train epoch: 1758, loss 0.0962\n",
      "test epoch: 1758, loss: 0.82\n",
      "train epoch: 1759, loss 0.0963\n",
      "test epoch: 1759, loss: 0.87\n",
      "train epoch: 1760, loss 0.0961\n",
      "test epoch: 1760, loss: 0.83\n",
      "train epoch: 1761, loss 0.0959\n",
      "test epoch: 1761, loss: 0.88\n",
      "train epoch: 1762, loss 0.0957\n",
      "test epoch: 1762, loss: 0.84\n",
      "train epoch: 1763, loss 0.0959\n",
      "test epoch: 1763, loss: 0.88\n",
      "train epoch: 1764, loss 0.0959\n",
      "test epoch: 1764, loss: 0.82\n",
      "train epoch: 1765, loss 0.0962\n",
      "test epoch: 1765, loss: 0.88\n",
      "train epoch: 1766, loss 0.0963\n",
      "test epoch: 1766, loss: 0.82\n",
      "train epoch: 1767, loss 0.0962\n",
      "test epoch: 1767, loss: 0.89\n",
      "train epoch: 1768, loss 0.0961\n",
      "test epoch: 1768, loss: 0.83\n",
      "train epoch: 1769, loss 0.0961\n",
      "test epoch: 1769, loss: 0.88\n",
      "train epoch: 1770, loss 0.0961\n",
      "test epoch: 1770, loss: 0.82\n",
      "train epoch: 1771, loss 0.0963\n",
      "test epoch: 1771, loss: 0.88\n",
      "train epoch: 1772, loss 0.0965\n",
      "test epoch: 1772, loss: 0.82\n",
      "train epoch: 1773, loss 0.0965\n",
      "test epoch: 1773, loss: 0.89\n",
      "train epoch: 1774, loss 0.0963\n",
      "test epoch: 1774, loss: 0.84\n",
      "train epoch: 1775, loss 0.0965\n",
      "test epoch: 1775, loss: 0.89\n",
      "train epoch: 1776, loss 0.0965\n",
      "test epoch: 1776, loss: 0.82\n",
      "train epoch: 1777, loss 0.0968\n",
      "test epoch: 1777, loss: 0.89\n",
      "train epoch: 1778, loss 0.0969\n",
      "test epoch: 1778, loss: 0.82\n",
      "train epoch: 1779, loss 0.0967\n",
      "test epoch: 1779, loss: 0.90\n",
      "train epoch: 1780, loss 0.0966\n",
      "test epoch: 1780, loss: 0.83\n",
      "train epoch: 1781, loss 0.0966\n",
      "test epoch: 1781, loss: 0.91\n",
      "train epoch: 1782, loss 0.0969\n",
      "test epoch: 1782, loss: 0.82\n",
      "train epoch: 1783, loss 0.0973\n",
      "test epoch: 1783, loss: 0.91\n",
      "train epoch: 1784, loss 0.0974\n",
      "test epoch: 1784, loss: 0.82\n",
      "train epoch: 1785, loss 0.0974\n",
      "test epoch: 1785, loss: 0.90\n",
      "train epoch: 1786, loss 0.0976\n",
      "test epoch: 1786, loss: 0.81\n",
      "train epoch: 1787, loss 0.0976\n",
      "test epoch: 1787, loss: 0.90\n",
      "train epoch: 1788, loss 0.0972\n",
      "test epoch: 1788, loss: 0.82\n",
      "train epoch: 1789, loss 0.0967\n",
      "test epoch: 1789, loss: 0.90\n",
      "train epoch: 1790, loss 0.0965\n",
      "test epoch: 1790, loss: 0.83\n",
      "train epoch: 1791, loss 0.0963\n",
      "test epoch: 1791, loss: 0.89\n",
      "train epoch: 1792, loss 0.0963\n",
      "test epoch: 1792, loss: 0.82\n",
      "train epoch: 1793, loss 0.0964\n",
      "test epoch: 1793, loss: 0.88\n",
      "train epoch: 1794, loss 0.0962\n",
      "test epoch: 1794, loss: 0.83\n",
      "train epoch: 1795, loss 0.0959\n",
      "test epoch: 1795, loss: 0.88\n",
      "train epoch: 1796, loss 0.0957\n",
      "test epoch: 1796, loss: 0.84\n",
      "train epoch: 1797, loss 0.0953\n",
      "test epoch: 1797, loss: 0.86\n",
      "train epoch: 1798, loss 0.0952\n",
      "test epoch: 1798, loss: 0.85\n",
      "train epoch: 1799, loss 0.0951\n",
      "test epoch: 1799, loss: 0.86\n",
      "train epoch: 1800, loss 0.0950\n",
      "test epoch: 1800, loss: 0.86\n",
      "train epoch: 1801, loss 0.0949\n",
      "test epoch: 1801, loss: 0.86\n",
      "train epoch: 1802, loss 0.0951\n",
      "test epoch: 1802, loss: 0.86\n",
      "train epoch: 1803, loss 0.0950\n",
      "test epoch: 1803, loss: 0.84\n",
      "train epoch: 1804, loss 0.0952\n",
      "test epoch: 1804, loss: 0.87\n",
      "train epoch: 1805, loss 0.0954\n",
      "test epoch: 1805, loss: 0.83\n",
      "train epoch: 1806, loss 0.0957\n",
      "test epoch: 1806, loss: 0.89\n",
      "train epoch: 1807, loss 0.0963\n",
      "test epoch: 1807, loss: 0.81\n",
      "train epoch: 1808, loss 0.0972\n",
      "test epoch: 1808, loss: 0.91\n",
      "train epoch: 1809, loss 0.0983\n",
      "test epoch: 1809, loss: 0.80\n",
      "train epoch: 1810, loss 0.0997\n",
      "test epoch: 1810, loss: 0.93\n",
      "train epoch: 1811, loss 0.1020\n",
      "test epoch: 1811, loss: 0.76\n",
      "train epoch: 1812, loss 0.1039\n",
      "test epoch: 1812, loss: 0.94\n",
      "train epoch: 1813, loss 0.1048\n",
      "test epoch: 1813, loss: 0.77\n",
      "train epoch: 1814, loss 0.1040\n",
      "test epoch: 1814, loss: 0.94\n",
      "train epoch: 1815, loss 0.1026\n",
      "test epoch: 1815, loss: 0.80\n",
      "train epoch: 1816, loss 0.1012\n",
      "test epoch: 1816, loss: 0.92\n",
      "train epoch: 1817, loss 0.1003\n",
      "test epoch: 1817, loss: 0.79\n",
      "train epoch: 1818, loss 0.0992\n",
      "test epoch: 1818, loss: 0.89\n",
      "train epoch: 1819, loss 0.0975\n",
      "test epoch: 1819, loss: 0.83\n",
      "train epoch: 1820, loss 0.0958\n",
      "test epoch: 1820, loss: 0.86\n",
      "train epoch: 1821, loss 0.0951\n",
      "test epoch: 1821, loss: 0.89\n",
      "train epoch: 1822, loss 0.0954\n",
      "test epoch: 1822, loss: 0.84\n",
      "train epoch: 1823, loss 0.0968\n",
      "test epoch: 1823, loss: 0.92\n",
      "train epoch: 1824, loss 0.0983\n",
      "test epoch: 1824, loss: 0.80\n",
      "train epoch: 1825, loss 0.1000\n",
      "test epoch: 1825, loss: 0.92\n",
      "train epoch: 1826, loss 0.1007\n",
      "test epoch: 1826, loss: 0.78\n",
      "train epoch: 1827, loss 0.0999\n",
      "test epoch: 1827, loss: 0.89\n",
      "train epoch: 1828, loss 0.0978\n",
      "test epoch: 1828, loss: 0.83\n",
      "train epoch: 1829, loss 0.0958\n",
      "test epoch: 1829, loss: 0.87\n",
      "train epoch: 1830, loss 0.0952\n",
      "test epoch: 1830, loss: 0.88\n",
      "train epoch: 1831, loss 0.0955\n",
      "test epoch: 1831, loss: 0.81\n",
      "train epoch: 1832, loss 0.0970\n",
      "test epoch: 1832, loss: 0.90\n",
      "train epoch: 1833, loss 0.0983\n",
      "test epoch: 1833, loss: 0.80\n",
      "train epoch: 1834, loss 0.0987\n",
      "test epoch: 1834, loss: 0.91\n",
      "train epoch: 1835, loss 0.0985\n",
      "test epoch: 1835, loss: 0.82\n",
      "train epoch: 1836, loss 0.0977\n",
      "test epoch: 1836, loss: 0.91\n",
      "train epoch: 1837, loss 0.0968\n",
      "test epoch: 1837, loss: 0.84\n",
      "train epoch: 1838, loss 0.0962\n",
      "test epoch: 1838, loss: 0.88\n",
      "train epoch: 1839, loss 0.0954\n",
      "test epoch: 1839, loss: 0.84\n",
      "train epoch: 1840, loss 0.0953\n",
      "test epoch: 1840, loss: 0.85\n",
      "train epoch: 1841, loss 0.0951\n",
      "test epoch: 1841, loss: 0.86\n",
      "train epoch: 1842, loss 0.0950\n",
      "test epoch: 1842, loss: 0.85\n",
      "train epoch: 1843, loss 0.0951\n",
      "test epoch: 1843, loss: 0.88\n",
      "train epoch: 1844, loss 0.0954\n",
      "test epoch: 1844, loss: 0.85\n",
      "train epoch: 1845, loss 0.0953\n",
      "test epoch: 1845, loss: 0.87\n",
      "train epoch: 1846, loss 0.0953\n",
      "test epoch: 1846, loss: 0.83\n",
      "train epoch: 1847, loss 0.0955\n",
      "test epoch: 1847, loss: 0.86\n",
      "train epoch: 1848, loss 0.0955\n",
      "test epoch: 1848, loss: 0.84\n",
      "train epoch: 1849, loss 0.0953\n",
      "test epoch: 1849, loss: 0.86\n",
      "train epoch: 1850, loss 0.0950\n",
      "test epoch: 1850, loss: 0.86\n",
      "train epoch: 1851, loss 0.0950\n",
      "test epoch: 1851, loss: 0.87\n",
      "train epoch: 1852, loss 0.0952\n",
      "test epoch: 1852, loss: 0.86\n",
      "train epoch: 1853, loss 0.0949\n",
      "test epoch: 1853, loss: 0.84\n",
      "train epoch: 1854, loss 0.0951\n",
      "test epoch: 1854, loss: 0.86\n",
      "train epoch: 1855, loss 0.0953\n",
      "test epoch: 1855, loss: 0.83\n",
      "train epoch: 1856, loss 0.0953\n",
      "test epoch: 1856, loss: 0.87\n",
      "train epoch: 1857, loss 0.0952\n",
      "test epoch: 1857, loss: 0.85\n",
      "train epoch: 1858, loss 0.0952\n",
      "test epoch: 1858, loss: 0.88\n",
      "train epoch: 1859, loss 0.0952\n",
      "test epoch: 1859, loss: 0.84\n",
      "train epoch: 1860, loss 0.0952\n",
      "test epoch: 1860, loss: 0.87\n",
      "train epoch: 1861, loss 0.0952\n",
      "test epoch: 1861, loss: 0.84\n",
      "train epoch: 1862, loss 0.0951\n",
      "test epoch: 1862, loss: 0.87\n",
      "train epoch: 1863, loss 0.0950\n",
      "test epoch: 1863, loss: 0.85\n",
      "train epoch: 1864, loss 0.0949\n",
      "test epoch: 1864, loss: 0.86\n",
      "train epoch: 1865, loss 0.0949\n",
      "test epoch: 1865, loss: 0.85\n",
      "train epoch: 1866, loss 0.0949\n",
      "test epoch: 1866, loss: 0.87\n",
      "train epoch: 1867, loss 0.0948\n",
      "test epoch: 1867, loss: 0.85\n",
      "train epoch: 1868, loss 0.0948\n",
      "test epoch: 1868, loss: 0.87\n",
      "train epoch: 1869, loss 0.0949\n",
      "test epoch: 1869, loss: 0.85\n",
      "train epoch: 1870, loss 0.0950\n",
      "test epoch: 1870, loss: 0.88\n",
      "train epoch: 1871, loss 0.0951\n",
      "test epoch: 1871, loss: 0.84\n",
      "train epoch: 1872, loss 0.0954\n",
      "test epoch: 1872, loss: 0.89\n",
      "train epoch: 1873, loss 0.0959\n",
      "test epoch: 1873, loss: 0.82\n",
      "train epoch: 1874, loss 0.0965\n",
      "test epoch: 1874, loss: 0.90\n",
      "train epoch: 1875, loss 0.0974\n",
      "test epoch: 1875, loss: 0.81\n",
      "train epoch: 1876, loss 0.0982\n",
      "test epoch: 1876, loss: 0.92\n",
      "train epoch: 1877, loss 0.0996\n",
      "test epoch: 1877, loss: 0.78\n",
      "train epoch: 1878, loss 0.1006\n",
      "test epoch: 1878, loss: 0.93\n",
      "train epoch: 1879, loss 0.1012\n",
      "test epoch: 1879, loss: 0.79\n",
      "train epoch: 1880, loss 0.1011\n",
      "test epoch: 1880, loss: 0.93\n",
      "train epoch: 1881, loss 0.1015\n",
      "test epoch: 1881, loss: 0.78\n",
      "train epoch: 1882, loss 0.1009\n",
      "test epoch: 1882, loss: 0.92\n",
      "train epoch: 1883, loss 0.0996\n",
      "test epoch: 1883, loss: 0.81\n",
      "train epoch: 1884, loss 0.0978\n",
      "test epoch: 1884, loss: 0.89\n",
      "train epoch: 1885, loss 0.0964\n",
      "test epoch: 1885, loss: 0.84\n",
      "train epoch: 1886, loss 0.0953\n",
      "test epoch: 1886, loss: 0.86\n",
      "train epoch: 1887, loss 0.0948\n",
      "test epoch: 1887, loss: 0.87\n",
      "train epoch: 1888, loss 0.0949\n",
      "test epoch: 1888, loss: 0.84\n",
      "train epoch: 1889, loss 0.0954\n",
      "test epoch: 1889, loss: 0.90\n",
      "train epoch: 1890, loss 0.0963\n",
      "test epoch: 1890, loss: 0.82\n",
      "train epoch: 1891, loss 0.0971\n",
      "test epoch: 1891, loss: 0.91\n",
      "train epoch: 1892, loss 0.0979\n",
      "test epoch: 1892, loss: 0.80\n",
      "train epoch: 1893, loss 0.0981\n",
      "test epoch: 1893, loss: 0.91\n",
      "train epoch: 1894, loss 0.0977\n",
      "test epoch: 1894, loss: 0.82\n",
      "train epoch: 1895, loss 0.0969\n",
      "test epoch: 1895, loss: 0.89\n",
      "train epoch: 1896, loss 0.0961\n",
      "test epoch: 1896, loss: 0.83\n",
      "train epoch: 1897, loss 0.0953\n",
      "test epoch: 1897, loss: 0.87\n",
      "train epoch: 1898, loss 0.0948\n",
      "test epoch: 1898, loss: 0.86\n",
      "train epoch: 1899, loss 0.0947\n",
      "test epoch: 1899, loss: 0.85\n",
      "train epoch: 1900, loss 0.0949\n",
      "test epoch: 1900, loss: 0.88\n",
      "train epoch: 1901, loss 0.0953\n",
      "test epoch: 1901, loss: 0.82\n",
      "train epoch: 1902, loss 0.0961\n",
      "test epoch: 1902, loss: 0.90\n",
      "train epoch: 1903, loss 0.0970\n",
      "test epoch: 1903, loss: 0.81\n",
      "train epoch: 1904, loss 0.0974\n",
      "test epoch: 1904, loss: 0.91\n",
      "train epoch: 1905, loss 0.0975\n",
      "test epoch: 1905, loss: 0.82\n",
      "train epoch: 1906, loss 0.0974\n",
      "test epoch: 1906, loss: 0.90\n",
      "train epoch: 1907, loss 0.0971\n",
      "test epoch: 1907, loss: 0.81\n",
      "train epoch: 1908, loss 0.0966\n",
      "test epoch: 1908, loss: 0.88\n",
      "train epoch: 1909, loss 0.0958\n",
      "test epoch: 1909, loss: 0.84\n",
      "train epoch: 1910, loss 0.0951\n",
      "test epoch: 1910, loss: 0.87\n",
      "train epoch: 1911, loss 0.0947\n",
      "test epoch: 1911, loss: 0.86\n",
      "train epoch: 1912, loss 0.0947\n",
      "test epoch: 1912, loss: 0.85\n",
      "train epoch: 1913, loss 0.0946\n",
      "test epoch: 1913, loss: 0.86\n",
      "train epoch: 1914, loss 0.0948\n",
      "test epoch: 1914, loss: 0.83\n",
      "train epoch: 1915, loss 0.0950\n",
      "test epoch: 1915, loss: 0.87\n",
      "train epoch: 1916, loss 0.0950\n",
      "test epoch: 1916, loss: 0.84\n",
      "train epoch: 1917, loss 0.0950\n",
      "test epoch: 1917, loss: 0.88\n",
      "train epoch: 1918, loss 0.0950\n",
      "test epoch: 1918, loss: 0.84\n",
      "train epoch: 1919, loss 0.0950\n",
      "test epoch: 1919, loss: 0.88\n",
      "train epoch: 1920, loss 0.0950\n",
      "test epoch: 1920, loss: 0.85\n",
      "train epoch: 1921, loss 0.0949\n",
      "test epoch: 1921, loss: 0.88\n",
      "train epoch: 1922, loss 0.0949\n",
      "test epoch: 1922, loss: 0.84\n",
      "train epoch: 1923, loss 0.0948\n",
      "test epoch: 1923, loss: 0.87\n",
      "train epoch: 1924, loss 0.0948\n",
      "test epoch: 1924, loss: 0.84\n",
      "train epoch: 1925, loss 0.0948\n",
      "test epoch: 1925, loss: 0.87\n",
      "train epoch: 1926, loss 0.0948\n",
      "test epoch: 1926, loss: 0.85\n",
      "train epoch: 1927, loss 0.0948\n",
      "test epoch: 1927, loss: 0.87\n",
      "train epoch: 1928, loss 0.0947\n",
      "test epoch: 1928, loss: 0.84\n",
      "train epoch: 1929, loss 0.0947\n",
      "test epoch: 1929, loss: 0.86\n",
      "train epoch: 1930, loss 0.0947\n",
      "test epoch: 1930, loss: 0.85\n",
      "train epoch: 1931, loss 0.0946\n",
      "test epoch: 1931, loss: 0.86\n",
      "train epoch: 1932, loss 0.0945\n",
      "test epoch: 1932, loss: 0.86\n",
      "train epoch: 1933, loss 0.0946\n",
      "test epoch: 1933, loss: 0.86\n",
      "train epoch: 1934, loss 0.0945\n",
      "test epoch: 1934, loss: 0.85\n",
      "train epoch: 1935, loss 0.0946\n",
      "test epoch: 1935, loss: 0.85\n",
      "train epoch: 1936, loss 0.0945\n",
      "test epoch: 1936, loss: 0.86\n",
      "train epoch: 1937, loss 0.0945\n",
      "test epoch: 1937, loss: 0.86\n",
      "train epoch: 1938, loss 0.0945\n",
      "test epoch: 1938, loss: 0.85\n",
      "train epoch: 1939, loss 0.0945\n",
      "test epoch: 1939, loss: 0.86\n",
      "train epoch: 1940, loss 0.0946\n",
      "test epoch: 1940, loss: 0.84\n",
      "train epoch: 1941, loss 0.0947\n",
      "test epoch: 1941, loss: 0.86\n",
      "train epoch: 1942, loss 0.0949\n",
      "test epoch: 1942, loss: 0.82\n",
      "train epoch: 1943, loss 0.0951\n",
      "test epoch: 1943, loss: 0.88\n",
      "train epoch: 1944, loss 0.0953\n",
      "test epoch: 1944, loss: 0.82\n",
      "train epoch: 1945, loss 0.0957\n",
      "test epoch: 1945, loss: 0.89\n",
      "train epoch: 1946, loss 0.0966\n",
      "test epoch: 1946, loss: 0.80\n",
      "train epoch: 1947, loss 0.0980\n",
      "test epoch: 1947, loss: 0.92\n",
      "train epoch: 1948, loss 0.0997\n",
      "test epoch: 1948, loss: 0.78\n",
      "train epoch: 1949, loss 0.1019\n",
      "test epoch: 1949, loss: 0.95\n",
      "train epoch: 1950, loss 0.1058\n",
      "test epoch: 1950, loss: 0.74\n",
      "train epoch: 1951, loss 0.1093\n",
      "test epoch: 1951, loss: 0.97\n",
      "train epoch: 1952, loss 0.1115\n",
      "test epoch: 1952, loss: 0.74\n",
      "train epoch: 1953, loss 0.1104\n",
      "test epoch: 1953, loss: 0.96\n",
      "train epoch: 1954, loss 0.1069\n",
      "test epoch: 1954, loss: 0.78\n",
      "train epoch: 1955, loss 0.1022\n",
      "test epoch: 1955, loss: 0.91\n",
      "train epoch: 1956, loss 0.0984\n",
      "test epoch: 1956, loss: 0.82\n",
      "train epoch: 1957, loss 0.0957\n",
      "test epoch: 1957, loss: 0.85\n",
      "train epoch: 1958, loss 0.0947\n",
      "test epoch: 1958, loss: 0.88\n",
      "train epoch: 1959, loss 0.0953\n",
      "test epoch: 1959, loss: 0.82\n",
      "train epoch: 1960, loss 0.0969\n",
      "test epoch: 1960, loss: 0.93\n",
      "train epoch: 1961, loss 0.0987\n",
      "test epoch: 1961, loss: 0.80\n",
      "train epoch: 1962, loss 0.1000\n",
      "test epoch: 1962, loss: 0.93\n",
      "train epoch: 1963, loss 0.1009\n",
      "test epoch: 1963, loss: 0.78\n",
      "train epoch: 1964, loss 0.1005\n",
      "test epoch: 1964, loss: 0.90\n",
      "train epoch: 1965, loss 0.0982\n",
      "test epoch: 1965, loss: 0.82\n",
      "train epoch: 1966, loss 0.0956\n",
      "test epoch: 1966, loss: 0.85\n",
      "train epoch: 1967, loss 0.0945\n",
      "test epoch: 1967, loss: 0.89\n",
      "train epoch: 1968, loss 0.0957\n",
      "test epoch: 1968, loss: 0.82\n",
      "train epoch: 1969, loss 0.0968\n",
      "test epoch: 1969, loss: 0.90\n",
      "train epoch: 1970, loss 0.0980\n",
      "test epoch: 1970, loss: 0.79\n",
      "train epoch: 1971, loss 0.0987\n",
      "test epoch: 1971, loss: 0.89\n",
      "train epoch: 1972, loss 0.0978\n",
      "test epoch: 1972, loss: 0.82\n",
      "train epoch: 1973, loss 0.0960\n",
      "test epoch: 1973, loss: 0.87\n",
      "train epoch: 1974, loss 0.0947\n",
      "test epoch: 1974, loss: 0.88\n",
      "train epoch: 1975, loss 0.0948\n",
      "test epoch: 1975, loss: 0.85\n",
      "train epoch: 1976, loss 0.0951\n",
      "test epoch: 1976, loss: 0.89\n",
      "train epoch: 1977, loss 0.0955\n",
      "test epoch: 1977, loss: 0.82\n",
      "train epoch: 1978, loss 0.0960\n",
      "test epoch: 1978, loss: 0.89\n",
      "train epoch: 1979, loss 0.0959\n",
      "test epoch: 1979, loss: 0.83\n",
      "train epoch: 1980, loss 0.0953\n",
      "test epoch: 1980, loss: 0.87\n",
      "train epoch: 1981, loss 0.0947\n",
      "test epoch: 1981, loss: 0.86\n",
      "train epoch: 1982, loss 0.0944\n",
      "test epoch: 1982, loss: 0.84\n",
      "train epoch: 1983, loss 0.0946\n",
      "test epoch: 1983, loss: 0.87\n",
      "train epoch: 1984, loss 0.0950\n",
      "test epoch: 1984, loss: 0.82\n",
      "train epoch: 1985, loss 0.0953\n",
      "test epoch: 1985, loss: 0.88\n",
      "train epoch: 1986, loss 0.0953\n",
      "test epoch: 1986, loss: 0.83\n",
      "train epoch: 1987, loss 0.0952\n",
      "test epoch: 1987, loss: 0.88\n",
      "train epoch: 1988, loss 0.0950\n",
      "test epoch: 1988, loss: 0.84\n",
      "train epoch: 1989, loss 0.0946\n",
      "test epoch: 1989, loss: 0.85\n",
      "train epoch: 1990, loss 0.0945\n",
      "test epoch: 1990, loss: 0.84\n",
      "train epoch: 1991, loss 0.0945\n",
      "test epoch: 1991, loss: 0.84\n",
      "train epoch: 1992, loss 0.0945\n",
      "test epoch: 1992, loss: 0.86\n",
      "train epoch: 1993, loss 0.0945\n",
      "test epoch: 1993, loss: 0.85\n",
      "train epoch: 1994, loss 0.0947\n",
      "test epoch: 1994, loss: 0.88\n",
      "train epoch: 1995, loss 0.0948\n",
      "test epoch: 1995, loss: 0.83\n",
      "train epoch: 1996, loss 0.0949\n",
      "test epoch: 1996, loss: 0.88\n",
      "train epoch: 1997, loss 0.0950\n",
      "test epoch: 1997, loss: 0.83\n",
      "train epoch: 1998, loss 0.0949\n",
      "test epoch: 1998, loss: 0.88\n",
      "train epoch: 1999, loss 0.0947\n",
      "test epoch: 1999, loss: 0.85\n",
      "train epoch: 2000, loss 0.0946\n",
      "test epoch: 2000, loss: 0.87\n",
      "train epoch: 2001, loss 0.0945\n",
      "test epoch: 2001, loss: 0.85\n",
      "train epoch: 2002, loss 0.0944\n",
      "test epoch: 2002, loss: 0.86\n",
      "train epoch: 2003, loss 0.0944\n",
      "test epoch: 2003, loss: 0.85\n",
      "train epoch: 2004, loss 0.0943\n",
      "test epoch: 2004, loss: 0.86\n",
      "train epoch: 2005, loss 0.0943\n",
      "test epoch: 2005, loss: 0.86\n",
      "train epoch: 2006, loss 0.0943\n",
      "test epoch: 2006, loss: 0.85\n",
      "train epoch: 2007, loss 0.0943\n",
      "test epoch: 2007, loss: 0.86\n",
      "train epoch: 2008, loss 0.0944\n",
      "test epoch: 2008, loss: 0.84\n",
      "train epoch: 2009, loss 0.0944\n",
      "test epoch: 2009, loss: 0.86\n",
      "train epoch: 2010, loss 0.0943\n",
      "test epoch: 2010, loss: 0.85\n",
      "train epoch: 2011, loss 0.0944\n",
      "test epoch: 2011, loss: 0.87\n",
      "train epoch: 2012, loss 0.0944\n",
      "test epoch: 2012, loss: 0.84\n",
      "train epoch: 2013, loss 0.0945\n",
      "test epoch: 2013, loss: 0.87\n",
      "train epoch: 2014, loss 0.0947\n",
      "test epoch: 2014, loss: 0.83\n",
      "train epoch: 2015, loss 0.0949\n",
      "test epoch: 2015, loss: 0.88\n",
      "train epoch: 2016, loss 0.0950\n",
      "test epoch: 2016, loss: 0.83\n",
      "train epoch: 2017, loss 0.0952\n",
      "test epoch: 2017, loss: 0.88\n",
      "train epoch: 2018, loss 0.0954\n",
      "test epoch: 2018, loss: 0.82\n",
      "train epoch: 2019, loss 0.0957\n",
      "test epoch: 2019, loss: 0.89\n",
      "train epoch: 2020, loss 0.0960\n",
      "test epoch: 2020, loss: 0.82\n",
      "train epoch: 2021, loss 0.0963\n",
      "test epoch: 2021, loss: 0.90\n",
      "train epoch: 2022, loss 0.0968\n",
      "test epoch: 2022, loss: 0.81\n",
      "train epoch: 2023, loss 0.0973\n",
      "test epoch: 2023, loss: 0.90\n",
      "train epoch: 2024, loss 0.0982\n",
      "test epoch: 2024, loss: 0.78\n",
      "train epoch: 2025, loss 0.0989\n",
      "test epoch: 2025, loss: 0.90\n",
      "train epoch: 2026, loss 0.0989\n",
      "test epoch: 2026, loss: 0.79\n",
      "train epoch: 2027, loss 0.0980\n",
      "test epoch: 2027, loss: 0.90\n",
      "train epoch: 2028, loss 0.0971\n",
      "test epoch: 2028, loss: 0.82\n",
      "train epoch: 2029, loss 0.0970\n",
      "test epoch: 2029, loss: 0.90\n",
      "train epoch: 2030, loss 0.0967\n",
      "test epoch: 2030, loss: 0.80\n",
      "train epoch: 2031, loss 0.0969\n",
      "test epoch: 2031, loss: 0.88\n",
      "train epoch: 2032, loss 0.0966\n",
      "test epoch: 2032, loss: 0.80\n",
      "train epoch: 2033, loss 0.0960\n",
      "test epoch: 2033, loss: 0.87\n",
      "train epoch: 2034, loss 0.0951\n",
      "test epoch: 2034, loss: 0.83\n",
      "train epoch: 2035, loss 0.0946\n",
      "test epoch: 2035, loss: 0.86\n",
      "train epoch: 2036, loss 0.0943\n",
      "test epoch: 2036, loss: 0.85\n",
      "train epoch: 2037, loss 0.0941\n",
      "test epoch: 2037, loss: 0.84\n",
      "train epoch: 2038, loss 0.0942\n",
      "test epoch: 2038, loss: 0.86\n",
      "train epoch: 2039, loss 0.0944\n",
      "test epoch: 2039, loss: 0.83\n",
      "train epoch: 2040, loss 0.0947\n",
      "test epoch: 2040, loss: 0.88\n",
      "train epoch: 2041, loss 0.0952\n",
      "test epoch: 2041, loss: 0.81\n",
      "train epoch: 2042, loss 0.0958\n",
      "test epoch: 2042, loss: 0.89\n",
      "train epoch: 2043, loss 0.0965\n",
      "test epoch: 2043, loss: 0.80\n",
      "train epoch: 2044, loss 0.0969\n",
      "test epoch: 2044, loss: 0.90\n",
      "train epoch: 2045, loss 0.0974\n",
      "test epoch: 2045, loss: 0.80\n",
      "train epoch: 2046, loss 0.0973\n",
      "test epoch: 2046, loss: 0.90\n",
      "train epoch: 2047, loss 0.0969\n",
      "test epoch: 2047, loss: 0.81\n",
      "train epoch: 2048, loss 0.0963\n",
      "test epoch: 2048, loss: 0.89\n",
      "train epoch: 2049, loss 0.0959\n",
      "test epoch: 2049, loss: 0.82\n",
      "train epoch: 2050, loss 0.0955\n",
      "test epoch: 2050, loss: 0.89\n",
      "train epoch: 2051, loss 0.0952\n",
      "test epoch: 2051, loss: 0.84\n",
      "train epoch: 2052, loss 0.0947\n",
      "test epoch: 2052, loss: 0.88\n",
      "train epoch: 2053, loss 0.0944\n",
      "test epoch: 2053, loss: 0.85\n",
      "train epoch: 2054, loss 0.0942\n",
      "test epoch: 2054, loss: 0.86\n",
      "train epoch: 2055, loss 0.0941\n",
      "test epoch: 2055, loss: 0.85\n",
      "train epoch: 2056, loss 0.0941\n",
      "test epoch: 2056, loss: 0.85\n",
      "train epoch: 2057, loss 0.0941\n",
      "test epoch: 2057, loss: 0.86\n",
      "train epoch: 2058, loss 0.0940\n",
      "test epoch: 2058, loss: 0.86\n",
      "train epoch: 2059, loss 0.0940\n",
      "test epoch: 2059, loss: 0.86\n",
      "train epoch: 2060, loss 0.0940\n",
      "test epoch: 2060, loss: 0.84\n",
      "train epoch: 2061, loss 0.0941\n",
      "test epoch: 2061, loss: 0.86\n",
      "train epoch: 2062, loss 0.0942\n",
      "test epoch: 2062, loss: 0.84\n",
      "train epoch: 2063, loss 0.0943\n",
      "test epoch: 2063, loss: 0.88\n",
      "train epoch: 2064, loss 0.0945\n",
      "test epoch: 2064, loss: 0.83\n",
      "train epoch: 2065, loss 0.0948\n",
      "test epoch: 2065, loss: 0.88\n",
      "train epoch: 2066, loss 0.0952\n",
      "test epoch: 2066, loss: 0.81\n",
      "train epoch: 2067, loss 0.0957\n",
      "test epoch: 2067, loss: 0.89\n",
      "train epoch: 2068, loss 0.0962\n",
      "test epoch: 2068, loss: 0.81\n",
      "train epoch: 2069, loss 0.0966\n",
      "test epoch: 2069, loss: 0.91\n",
      "train epoch: 2070, loss 0.0978\n",
      "test epoch: 2070, loss: 0.79\n",
      "train epoch: 2071, loss 0.0990\n",
      "test epoch: 2071, loss: 0.92\n",
      "train epoch: 2072, loss 0.1007\n",
      "test epoch: 2072, loss: 0.77\n",
      "train epoch: 2073, loss 0.1020\n",
      "test epoch: 2073, loss: 0.93\n",
      "train epoch: 2074, loss 0.1035\n",
      "test epoch: 2074, loss: 0.76\n",
      "train epoch: 2075, loss 0.1035\n",
      "test epoch: 2075, loss: 0.93\n",
      "train epoch: 2076, loss 0.1025\n",
      "test epoch: 2076, loss: 0.78\n",
      "train epoch: 2077, loss 0.1001\n",
      "test epoch: 2077, loss: 0.91\n",
      "train epoch: 2078, loss 0.0975\n",
      "test epoch: 2078, loss: 0.82\n",
      "train epoch: 2079, loss 0.0955\n",
      "test epoch: 2079, loss: 0.86\n",
      "train epoch: 2080, loss 0.0943\n",
      "test epoch: 2080, loss: 0.84\n",
      "train epoch: 2081, loss 0.0941\n",
      "test epoch: 2081, loss: 0.83\n",
      "train epoch: 2082, loss 0.0944\n",
      "test epoch: 2082, loss: 0.88\n",
      "train epoch: 2083, loss 0.0950\n",
      "test epoch: 2083, loss: 0.82\n",
      "train epoch: 2084, loss 0.0958\n",
      "test epoch: 2084, loss: 0.90\n",
      "train epoch: 2085, loss 0.0968\n",
      "test epoch: 2085, loss: 0.80\n",
      "train epoch: 2086, loss 0.0977\n",
      "test epoch: 2086, loss: 0.90\n",
      "train epoch: 2087, loss 0.0984\n",
      "test epoch: 2087, loss: 0.78\n",
      "train epoch: 2088, loss 0.0983\n",
      "test epoch: 2088, loss: 0.89\n",
      "train epoch: 2089, loss 0.0971\n",
      "test epoch: 2089, loss: 0.81\n",
      "train epoch: 2090, loss 0.0953\n",
      "test epoch: 2090, loss: 0.87\n",
      "train epoch: 2091, loss 0.0943\n",
      "test epoch: 2091, loss: 0.86\n",
      "train epoch: 2092, loss 0.0940\n",
      "test epoch: 2092, loss: 0.83\n",
      "train epoch: 2093, loss 0.0945\n",
      "test epoch: 2093, loss: 0.88\n",
      "train epoch: 2094, loss 0.0953\n",
      "test epoch: 2094, loss: 0.81\n",
      "train epoch: 2095, loss 0.0959\n",
      "test epoch: 2095, loss: 0.90\n",
      "train epoch: 2096, loss 0.0962\n",
      "test epoch: 2096, loss: 0.82\n",
      "train epoch: 2097, loss 0.0961\n",
      "test epoch: 2097, loss: 0.90\n",
      "train epoch: 2098, loss 0.0959\n",
      "test epoch: 2098, loss: 0.83\n",
      "train epoch: 2099, loss 0.0956\n",
      "test epoch: 2099, loss: 0.89\n",
      "train epoch: 2100, loss 0.0950\n",
      "test epoch: 2100, loss: 0.83\n",
      "train epoch: 2101, loss 0.0945\n",
      "test epoch: 2101, loss: 0.86\n",
      "train epoch: 2102, loss 0.0942\n",
      "test epoch: 2102, loss: 0.85\n",
      "train epoch: 2103, loss 0.0940\n",
      "test epoch: 2103, loss: 0.84\n",
      "train epoch: 2104, loss 0.0940\n",
      "test epoch: 2104, loss: 0.86\n",
      "train epoch: 2105, loss 0.0941\n",
      "test epoch: 2105, loss: 0.83\n",
      "train epoch: 2106, loss 0.0942\n",
      "test epoch: 2106, loss: 0.86\n",
      "train epoch: 2107, loss 0.0942\n",
      "test epoch: 2107, loss: 0.83\n",
      "train epoch: 2108, loss 0.0942\n",
      "test epoch: 2108, loss: 0.87\n",
      "train epoch: 2109, loss 0.0941\n",
      "test epoch: 2109, loss: 0.85\n",
      "train epoch: 2110, loss 0.0941\n",
      "test epoch: 2110, loss: 0.87\n",
      "train epoch: 2111, loss 0.0940\n",
      "test epoch: 2111, loss: 0.84\n",
      "train epoch: 2112, loss 0.0940\n",
      "test epoch: 2112, loss: 0.86\n",
      "train epoch: 2113, loss 0.0939\n",
      "test epoch: 2113, loss: 0.84\n",
      "train epoch: 2114, loss 0.0939\n",
      "test epoch: 2114, loss: 0.85\n",
      "train epoch: 2115, loss 0.0939\n",
      "test epoch: 2115, loss: 0.84\n",
      "train epoch: 2116, loss 0.0938\n",
      "test epoch: 2116, loss: 0.86\n",
      "train epoch: 2117, loss 0.0938\n",
      "test epoch: 2117, loss: 0.85\n",
      "train epoch: 2118, loss 0.0938\n",
      "test epoch: 2118, loss: 0.86\n",
      "train epoch: 2119, loss 0.0938\n",
      "test epoch: 2119, loss: 0.84\n",
      "train epoch: 2120, loss 0.0938\n",
      "test epoch: 2120, loss: 0.86\n",
      "train epoch: 2121, loss 0.0938\n",
      "test epoch: 2121, loss: 0.84\n",
      "train epoch: 2122, loss 0.0939\n",
      "test epoch: 2122, loss: 0.86\n",
      "train epoch: 2123, loss 0.0940\n",
      "test epoch: 2123, loss: 0.83\n",
      "train epoch: 2124, loss 0.0941\n",
      "test epoch: 2124, loss: 0.87\n",
      "train epoch: 2125, loss 0.0944\n",
      "test epoch: 2125, loss: 0.82\n",
      "train epoch: 2126, loss 0.0948\n",
      "test epoch: 2126, loss: 0.89\n",
      "train epoch: 2127, loss 0.0956\n",
      "test epoch: 2127, loss: 0.80\n",
      "train epoch: 2128, loss 0.0965\n",
      "test epoch: 2128, loss: 0.90\n",
      "train epoch: 2129, loss 0.0977\n",
      "test epoch: 2129, loss: 0.79\n",
      "train epoch: 2130, loss 0.0990\n",
      "test epoch: 2130, loss: 0.92\n",
      "train epoch: 2131, loss 0.1010\n",
      "test epoch: 2131, loss: 0.77\n",
      "train epoch: 2132, loss 0.1025\n",
      "test epoch: 2132, loss: 0.93\n",
      "train epoch: 2133, loss 0.1043\n",
      "test epoch: 2133, loss: 0.75\n",
      "train epoch: 2134, loss 0.1042\n",
      "test epoch: 2134, loss: 0.92\n",
      "train epoch: 2135, loss 0.1027\n",
      "test epoch: 2135, loss: 0.77\n",
      "train epoch: 2136, loss 0.0997\n",
      "test epoch: 2136, loss: 0.90\n",
      "train epoch: 2137, loss 0.0968\n",
      "test epoch: 2137, loss: 0.83\n",
      "train epoch: 2138, loss 0.0950\n",
      "test epoch: 2138, loss: 0.86\n",
      "train epoch: 2139, loss 0.0939\n",
      "test epoch: 2139, loss: 0.84\n",
      "train epoch: 2140, loss 0.0938\n",
      "test epoch: 2140, loss: 0.83\n",
      "train epoch: 2141, loss 0.0941\n",
      "test epoch: 2141, loss: 0.87\n",
      "train epoch: 2142, loss 0.0945\n",
      "test epoch: 2142, loss: 0.82\n",
      "train epoch: 2143, loss 0.0949\n",
      "test epoch: 2143, loss: 0.89\n",
      "train epoch: 2144, loss 0.0951\n",
      "test epoch: 2144, loss: 0.82\n",
      "train epoch: 2145, loss 0.0954\n",
      "test epoch: 2145, loss: 0.88\n",
      "train epoch: 2146, loss 0.0956\n",
      "test epoch: 2146, loss: 0.79\n",
      "train epoch: 2147, loss 0.0960\n",
      "test epoch: 2147, loss: 0.87\n",
      "train epoch: 2148, loss 0.0957\n",
      "test epoch: 2148, loss: 0.81\n",
      "train epoch: 2149, loss 0.0950\n",
      "test epoch: 2149, loss: 0.87\n",
      "train epoch: 2150, loss 0.0942\n",
      "test epoch: 2150, loss: 0.85\n",
      "train epoch: 2151, loss 0.0939\n",
      "test epoch: 2151, loss: 0.86\n",
      "train epoch: 2152, loss 0.0938\n",
      "test epoch: 2152, loss: 0.86\n",
      "train epoch: 2153, loss 0.0938\n",
      "test epoch: 2153, loss: 0.83\n",
      "train epoch: 2154, loss 0.0940\n",
      "test epoch: 2154, loss: 0.86\n",
      "train epoch: 2155, loss 0.0942\n",
      "test epoch: 2155, loss: 0.83\n",
      "train epoch: 2156, loss 0.0942\n",
      "test epoch: 2156, loss: 0.88\n",
      "train epoch: 2157, loss 0.0945\n",
      "test epoch: 2157, loss: 0.83\n",
      "train epoch: 2158, loss 0.0944\n",
      "test epoch: 2158, loss: 0.87\n",
      "train epoch: 2159, loss 0.0941\n",
      "test epoch: 2159, loss: 0.83\n",
      "train epoch: 2160, loss 0.0941\n",
      "test epoch: 2160, loss: 0.86\n",
      "train epoch: 2161, loss 0.0940\n",
      "test epoch: 2161, loss: 0.83\n",
      "train epoch: 2162, loss 0.0938\n",
      "test epoch: 2162, loss: 0.85\n",
      "train epoch: 2163, loss 0.0936\n",
      "test epoch: 2163, loss: 0.85\n",
      "train epoch: 2164, loss 0.0936\n",
      "test epoch: 2164, loss: 0.85\n",
      "train epoch: 2165, loss 0.0936\n",
      "test epoch: 2165, loss: 0.84\n",
      "train epoch: 2166, loss 0.0936\n",
      "test epoch: 2166, loss: 0.85\n",
      "train epoch: 2167, loss 0.0936\n",
      "test epoch: 2167, loss: 0.85\n",
      "train epoch: 2168, loss 0.0935\n",
      "test epoch: 2168, loss: 0.85\n",
      "train epoch: 2169, loss 0.0936\n",
      "test epoch: 2169, loss: 0.85\n",
      "train epoch: 2170, loss 0.0935\n",
      "test epoch: 2170, loss: 0.84\n",
      "train epoch: 2171, loss 0.0935\n",
      "test epoch: 2171, loss: 0.85\n",
      "train epoch: 2172, loss 0.0936\n",
      "test epoch: 2172, loss: 0.84\n",
      "train epoch: 2173, loss 0.0937\n",
      "test epoch: 2173, loss: 0.87\n",
      "train epoch: 2174, loss 0.0938\n",
      "test epoch: 2174, loss: 0.84\n",
      "train epoch: 2175, loss 0.0939\n",
      "test epoch: 2175, loss: 0.87\n",
      "train epoch: 2176, loss 0.0942\n",
      "test epoch: 2176, loss: 0.82\n",
      "train epoch: 2177, loss 0.0947\n",
      "test epoch: 2177, loss: 0.88\n",
      "train epoch: 2178, loss 0.0954\n",
      "test epoch: 2178, loss: 0.80\n",
      "train epoch: 2179, loss 0.0963\n",
      "test epoch: 2179, loss: 0.90\n",
      "train epoch: 2180, loss 0.0979\n",
      "test epoch: 2180, loss: 0.77\n",
      "train epoch: 2181, loss 0.0998\n",
      "test epoch: 2181, loss: 0.92\n",
      "train epoch: 2182, loss 0.1019\n",
      "test epoch: 2182, loss: 0.76\n",
      "train epoch: 2183, loss 0.1034\n",
      "test epoch: 2183, loss: 0.94\n",
      "train epoch: 2184, loss 0.1045\n",
      "test epoch: 2184, loss: 0.76\n",
      "train epoch: 2185, loss 0.1043\n",
      "test epoch: 2185, loss: 0.93\n",
      "train epoch: 2186, loss 0.1037\n",
      "test epoch: 2186, loss: 0.76\n",
      "train epoch: 2187, loss 0.1014\n",
      "test epoch: 2187, loss: 0.90\n",
      "train epoch: 2188, loss 0.0981\n",
      "test epoch: 2188, loss: 0.81\n",
      "train epoch: 2189, loss 0.0952\n",
      "test epoch: 2189, loss: 0.86\n",
      "train epoch: 2190, loss 0.0936\n",
      "test epoch: 2190, loss: 0.87\n",
      "train epoch: 2191, loss 0.0938\n",
      "test epoch: 2191, loss: 0.82\n",
      "train epoch: 2192, loss 0.0948\n",
      "test epoch: 2192, loss: 0.89\n",
      "train epoch: 2193, loss 0.0964\n",
      "test epoch: 2193, loss: 0.79\n",
      "train epoch: 2194, loss 0.0978\n",
      "test epoch: 2194, loss: 0.91\n",
      "train epoch: 2195, loss 0.0982\n",
      "test epoch: 2195, loss: 0.80\n",
      "train epoch: 2196, loss 0.0974\n",
      "test epoch: 2196, loss: 0.90\n",
      "train epoch: 2197, loss 0.0963\n",
      "test epoch: 2197, loss: 0.82\n",
      "train epoch: 2198, loss 0.0951\n",
      "test epoch: 2198, loss: 0.87\n",
      "train epoch: 2199, loss 0.0940\n",
      "test epoch: 2199, loss: 0.84\n",
      "train epoch: 2200, loss 0.0935\n",
      "test epoch: 2200, loss: 0.83\n",
      "train epoch: 2201, loss 0.0935\n",
      "test epoch: 2201, loss: 0.86\n",
      "train epoch: 2202, loss 0.0939\n",
      "test epoch: 2202, loss: 0.82\n",
      "train epoch: 2203, loss 0.0943\n",
      "test epoch: 2203, loss: 0.88\n",
      "train epoch: 2204, loss 0.0949\n",
      "test epoch: 2204, loss: 0.81\n",
      "train epoch: 2205, loss 0.0953\n",
      "test epoch: 2205, loss: 0.88\n",
      "train epoch: 2206, loss 0.0957\n",
      "test epoch: 2206, loss: 0.80\n",
      "train epoch: 2207, loss 0.0956\n",
      "test epoch: 2207, loss: 0.87\n",
      "train epoch: 2208, loss 0.0949\n",
      "test epoch: 2208, loss: 0.82\n",
      "train epoch: 2209, loss 0.0942\n",
      "test epoch: 2209, loss: 0.86\n",
      "train epoch: 2210, loss 0.0936\n",
      "test epoch: 2210, loss: 0.84\n",
      "train epoch: 2211, loss 0.0933\n",
      "test epoch: 2211, loss: 0.84\n",
      "train epoch: 2212, loss 0.0934\n",
      "test epoch: 2212, loss: 0.86\n",
      "train epoch: 2213, loss 0.0935\n",
      "test epoch: 2213, loss: 0.84\n",
      "train epoch: 2214, loss 0.0937\n",
      "test epoch: 2214, loss: 0.88\n",
      "train epoch: 2215, loss 0.0942\n",
      "test epoch: 2215, loss: 0.82\n",
      "train epoch: 2216, loss 0.0948\n",
      "test epoch: 2216, loss: 0.88\n",
      "train epoch: 2217, loss 0.0953\n",
      "test epoch: 2217, loss: 0.80\n",
      "train epoch: 2218, loss 0.0954\n",
      "test epoch: 2218, loss: 0.88\n",
      "train epoch: 2219, loss 0.0953\n",
      "test epoch: 2219, loss: 0.81\n",
      "train epoch: 2220, loss 0.0949\n",
      "test epoch: 2220, loss: 0.87\n",
      "train epoch: 2221, loss 0.0946\n",
      "test epoch: 2221, loss: 0.81\n",
      "train epoch: 2222, loss 0.0942\n",
      "test epoch: 2222, loss: 0.86\n",
      "train epoch: 2223, loss 0.0938\n",
      "test epoch: 2223, loss: 0.83\n",
      "train epoch: 2224, loss 0.0936\n",
      "test epoch: 2224, loss: 0.85\n",
      "train epoch: 2225, loss 0.0934\n",
      "test epoch: 2225, loss: 0.83\n",
      "train epoch: 2226, loss 0.0933\n",
      "test epoch: 2226, loss: 0.85\n",
      "train epoch: 2227, loss 0.0932\n",
      "test epoch: 2227, loss: 0.85\n",
      "train epoch: 2228, loss 0.0932\n",
      "test epoch: 2228, loss: 0.84\n",
      "train epoch: 2229, loss 0.0932\n",
      "test epoch: 2229, loss: 0.85\n",
      "train epoch: 2230, loss 0.0933\n",
      "test epoch: 2230, loss: 0.83\n",
      "train epoch: 2231, loss 0.0933\n",
      "test epoch: 2231, loss: 0.86\n",
      "train epoch: 2232, loss 0.0934\n",
      "test epoch: 2232, loss: 0.83\n",
      "train epoch: 2233, loss 0.0934\n",
      "test epoch: 2233, loss: 0.86\n",
      "train epoch: 2234, loss 0.0936\n",
      "test epoch: 2234, loss: 0.83\n",
      "train epoch: 2235, loss 0.0937\n",
      "test epoch: 2235, loss: 0.87\n",
      "train epoch: 2236, loss 0.0939\n",
      "test epoch: 2236, loss: 0.82\n",
      "train epoch: 2237, loss 0.0942\n",
      "test epoch: 2237, loss: 0.87\n",
      "train epoch: 2238, loss 0.0944\n",
      "test epoch: 2238, loss: 0.81\n",
      "train epoch: 2239, loss 0.0948\n",
      "test epoch: 2239, loss: 0.88\n",
      "train epoch: 2240, loss 0.0955\n",
      "test epoch: 2240, loss: 0.79\n",
      "train epoch: 2241, loss 0.0961\n",
      "test epoch: 2241, loss: 0.89\n",
      "train epoch: 2242, loss 0.0965\n",
      "test epoch: 2242, loss: 0.79\n",
      "train epoch: 2243, loss 0.0970\n",
      "test epoch: 2243, loss: 0.90\n",
      "train epoch: 2244, loss 0.0974\n",
      "test epoch: 2244, loss: 0.79\n",
      "train epoch: 2245, loss 0.0977\n",
      "test epoch: 2245, loss: 0.90\n",
      "train epoch: 2246, loss 0.0985\n",
      "test epoch: 2246, loss: 0.77\n",
      "train epoch: 2247, loss 0.0986\n",
      "test epoch: 2247, loss: 0.89\n",
      "train epoch: 2248, loss 0.0978\n",
      "test epoch: 2248, loss: 0.79\n",
      "train epoch: 2249, loss 0.0962\n",
      "test epoch: 2249, loss: 0.88\n",
      "train epoch: 2250, loss 0.0947\n",
      "test epoch: 2250, loss: 0.83\n",
      "train epoch: 2251, loss 0.0940\n",
      "test epoch: 2251, loss: 0.86\n",
      "train epoch: 2252, loss 0.0933\n",
      "test epoch: 2252, loss: 0.83\n",
      "train epoch: 2253, loss 0.0933\n",
      "test epoch: 2253, loss: 0.84\n",
      "train epoch: 2254, loss 0.0933\n",
      "test epoch: 2254, loss: 0.84\n",
      "train epoch: 2255, loss 0.0932\n",
      "test epoch: 2255, loss: 0.83\n",
      "train epoch: 2256, loss 0.0932\n",
      "test epoch: 2256, loss: 0.86\n",
      "train epoch: 2257, loss 0.0933\n",
      "test epoch: 2257, loss: 0.84\n",
      "train epoch: 2258, loss 0.0935\n",
      "test epoch: 2258, loss: 0.86\n",
      "train epoch: 2259, loss 0.0935\n",
      "test epoch: 2259, loss: 0.82\n",
      "train epoch: 2260, loss 0.0938\n",
      "test epoch: 2260, loss: 0.87\n",
      "train epoch: 2261, loss 0.0938\n",
      "test epoch: 2261, loss: 0.83\n",
      "train epoch: 2262, loss 0.0938\n",
      "test epoch: 2262, loss: 0.88\n",
      "train epoch: 2263, loss 0.0939\n",
      "test epoch: 2263, loss: 0.83\n",
      "train epoch: 2264, loss 0.0939\n",
      "test epoch: 2264, loss: 0.88\n",
      "train epoch: 2265, loss 0.0940\n",
      "test epoch: 2265, loss: 0.82\n",
      "train epoch: 2266, loss 0.0943\n",
      "test epoch: 2266, loss: 0.88\n",
      "train epoch: 2267, loss 0.0947\n",
      "test epoch: 2267, loss: 0.80\n",
      "train epoch: 2268, loss 0.0950\n",
      "test epoch: 2268, loss: 0.88\n",
      "train epoch: 2269, loss 0.0956\n",
      "test epoch: 2269, loss: 0.79\n",
      "train epoch: 2270, loss 0.0960\n",
      "test epoch: 2270, loss: 0.89\n",
      "train epoch: 2271, loss 0.0962\n",
      "test epoch: 2271, loss: 0.79\n",
      "train epoch: 2272, loss 0.0965\n",
      "test epoch: 2272, loss: 0.89\n",
      "train epoch: 2273, loss 0.0970\n",
      "test epoch: 2273, loss: 0.78\n",
      "train epoch: 2274, loss 0.0976\n",
      "test epoch: 2274, loss: 0.89\n",
      "train epoch: 2275, loss 0.0985\n",
      "test epoch: 2275, loss: 0.77\n",
      "train epoch: 2276, loss 0.0986\n",
      "test epoch: 2276, loss: 0.90\n",
      "train epoch: 2277, loss 0.0982\n",
      "test epoch: 2277, loss: 0.79\n",
      "train epoch: 2278, loss 0.0975\n",
      "test epoch: 2278, loss: 0.90\n",
      "train epoch: 2279, loss 0.0966\n",
      "test epoch: 2279, loss: 0.80\n",
      "train epoch: 2280, loss 0.0957\n",
      "test epoch: 2280, loss: 0.88\n",
      "train epoch: 2281, loss 0.0952\n",
      "test epoch: 2281, loss: 0.80\n",
      "train epoch: 2282, loss 0.0947\n",
      "test epoch: 2282, loss: 0.86\n",
      "train epoch: 2283, loss 0.0941\n",
      "test epoch: 2283, loss: 0.83\n",
      "train epoch: 2284, loss 0.0935\n",
      "test epoch: 2284, loss: 0.86\n",
      "train epoch: 2285, loss 0.0932\n",
      "test epoch: 2285, loss: 0.85\n",
      "train epoch: 2286, loss 0.0931\n",
      "test epoch: 2286, loss: 0.85\n",
      "train epoch: 2287, loss 0.0929\n",
      "test epoch: 2287, loss: 0.84\n",
      "train epoch: 2288, loss 0.0930\n",
      "test epoch: 2288, loss: 0.84\n",
      "train epoch: 2289, loss 0.0930\n",
      "test epoch: 2289, loss: 0.85\n",
      "train epoch: 2290, loss 0.0929\n",
      "test epoch: 2290, loss: 0.84\n",
      "train epoch: 2291, loss 0.0930\n",
      "test epoch: 2291, loss: 0.86\n",
      "train epoch: 2292, loss 0.0930\n",
      "test epoch: 2292, loss: 0.84\n",
      "train epoch: 2293, loss 0.0930\n",
      "test epoch: 2293, loss: 0.85\n",
      "train epoch: 2294, loss 0.0930\n",
      "test epoch: 2294, loss: 0.83\n",
      "train epoch: 2295, loss 0.0931\n",
      "test epoch: 2295, loss: 0.85\n",
      "train epoch: 2296, loss 0.0931\n",
      "test epoch: 2296, loss: 0.83\n",
      "train epoch: 2297, loss 0.0932\n",
      "test epoch: 2297, loss: 0.86\n",
      "train epoch: 2298, loss 0.0933\n",
      "test epoch: 2298, loss: 0.82\n",
      "train epoch: 2299, loss 0.0936\n",
      "test epoch: 2299, loss: 0.87\n",
      "train epoch: 2300, loss 0.0939\n",
      "test epoch: 2300, loss: 0.81\n",
      "train epoch: 2301, loss 0.0943\n",
      "test epoch: 2301, loss: 0.88\n",
      "train epoch: 2302, loss 0.0955\n",
      "test epoch: 2302, loss: 0.78\n",
      "train epoch: 2303, loss 0.0970\n",
      "test epoch: 2303, loss: 0.90\n",
      "train epoch: 2304, loss 0.0988\n",
      "test epoch: 2304, loss: 0.76\n",
      "train epoch: 2305, loss 0.1011\n",
      "test epoch: 2305, loss: 0.92\n",
      "train epoch: 2306, loss 0.1053\n",
      "test epoch: 2306, loss: 0.71\n",
      "train epoch: 2307, loss 0.1087\n",
      "test epoch: 2307, loss: 0.93\n",
      "train epoch: 2308, loss 0.1096\n",
      "test epoch: 2308, loss: 0.72\n",
      "train epoch: 2309, loss 0.1063\n",
      "test epoch: 2309, loss: 0.91\n",
      "train epoch: 2310, loss 0.1009\n",
      "test epoch: 2310, loss: 0.79\n",
      "train epoch: 2311, loss 0.0965\n",
      "test epoch: 2311, loss: 0.86\n",
      "train epoch: 2312, loss 0.0938\n",
      "test epoch: 2312, loss: 0.83\n",
      "train epoch: 2313, loss 0.0929\n",
      "test epoch: 2313, loss: 0.81\n",
      "train epoch: 2314, loss 0.0935\n",
      "test epoch: 2314, loss: 0.87\n",
      "train epoch: 2315, loss 0.0948\n",
      "test epoch: 2315, loss: 0.79\n",
      "train epoch: 2316, loss 0.0961\n",
      "test epoch: 2316, loss: 0.90\n",
      "train epoch: 2317, loss 0.0972\n",
      "test epoch: 2317, loss: 0.78\n",
      "train epoch: 2318, loss 0.0978\n",
      "test epoch: 2318, loss: 0.89\n",
      "train epoch: 2319, loss 0.0978\n",
      "test epoch: 2319, loss: 0.78\n",
      "train epoch: 2320, loss 0.0966\n",
      "test epoch: 2320, loss: 0.87\n",
      "train epoch: 2321, loss 0.0949\n",
      "test epoch: 2321, loss: 0.82\n",
      "train epoch: 2322, loss 0.0933\n",
      "test epoch: 2322, loss: 0.84\n",
      "train epoch: 2323, loss 0.0929\n",
      "test epoch: 2323, loss: 0.87\n",
      "train epoch: 2324, loss 0.0935\n",
      "test epoch: 2324, loss: 0.81\n",
      "train epoch: 2325, loss 0.0941\n",
      "test epoch: 2325, loss: 0.87\n",
      "train epoch: 2326, loss 0.0951\n",
      "test epoch: 2326, loss: 0.78\n",
      "train epoch: 2327, loss 0.0958\n",
      "test epoch: 2327, loss: 0.87\n",
      "train epoch: 2328, loss 0.0954\n",
      "test epoch: 2328, loss: 0.80\n",
      "train epoch: 2329, loss 0.0946\n",
      "test epoch: 2329, loss: 0.87\n",
      "train epoch: 2330, loss 0.0938\n",
      "test epoch: 2330, loss: 0.84\n",
      "train epoch: 2331, loss 0.0936\n",
      "test epoch: 2331, loss: 0.85\n",
      "train epoch: 2332, loss 0.0930\n",
      "test epoch: 2332, loss: 0.82\n",
      "train epoch: 2333, loss 0.0930\n",
      "test epoch: 2333, loss: 0.83\n",
      "train epoch: 2334, loss 0.0930\n",
      "test epoch: 2334, loss: 0.84\n",
      "train epoch: 2335, loss 0.0929\n",
      "test epoch: 2335, loss: 0.83\n",
      "train epoch: 2336, loss 0.0929\n",
      "test epoch: 2336, loss: 0.86\n",
      "train epoch: 2337, loss 0.0931\n",
      "test epoch: 2337, loss: 0.84\n",
      "train epoch: 2338, loss 0.0930\n",
      "test epoch: 2338, loss: 0.85\n",
      "train epoch: 2339, loss 0.0930\n",
      "test epoch: 2339, loss: 0.83\n",
      "train epoch: 2340, loss 0.0931\n",
      "test epoch: 2340, loss: 0.85\n",
      "train epoch: 2341, loss 0.0931\n",
      "test epoch: 2341, loss: 0.83\n",
      "train epoch: 2342, loss 0.0930\n",
      "test epoch: 2342, loss: 0.85\n",
      "train epoch: 2343, loss 0.0928\n",
      "test epoch: 2343, loss: 0.84\n",
      "train epoch: 2344, loss 0.0930\n",
      "test epoch: 2344, loss: 0.86\n",
      "train epoch: 2345, loss 0.0930\n",
      "test epoch: 2345, loss: 0.84\n",
      "train epoch: 2346, loss 0.0927\n",
      "test epoch: 2346, loss: 0.83\n",
      "train epoch: 2347, loss 0.0929\n",
      "test epoch: 2347, loss: 0.84\n",
      "train epoch: 2348, loss 0.0930\n",
      "test epoch: 2348, loss: 0.82\n",
      "train epoch: 2349, loss 0.0929\n",
      "test epoch: 2349, loss: 0.86\n",
      "train epoch: 2350, loss 0.0929\n",
      "test epoch: 2350, loss: 0.84\n",
      "train epoch: 2351, loss 0.0934\n",
      "test epoch: 2351, loss: 0.87\n",
      "train epoch: 2352, loss 0.0936\n",
      "test epoch: 2352, loss: 0.80\n",
      "train epoch: 2353, loss 0.0943\n",
      "test epoch: 2353, loss: 0.87\n",
      "train epoch: 2354, loss 0.0950\n",
      "test epoch: 2354, loss: 0.79\n",
      "train epoch: 2355, loss 0.0953\n",
      "test epoch: 2355, loss: 0.88\n",
      "train epoch: 2356, loss 0.0951\n",
      "test epoch: 2356, loss: 0.80\n",
      "train epoch: 2357, loss 0.0947\n",
      "test epoch: 2357, loss: 0.87\n",
      "train epoch: 2358, loss 0.0943\n",
      "test epoch: 2358, loss: 0.81\n",
      "train epoch: 2359, loss 0.0938\n",
      "test epoch: 2359, loss: 0.86\n",
      "train epoch: 2360, loss 0.0933\n",
      "test epoch: 2360, loss: 0.83\n",
      "train epoch: 2361, loss 0.0928\n",
      "test epoch: 2361, loss: 0.85\n",
      "train epoch: 2362, loss 0.0926\n",
      "test epoch: 2362, loss: 0.84\n",
      "train epoch: 2363, loss 0.0926\n",
      "test epoch: 2363, loss: 0.84\n",
      "train epoch: 2364, loss 0.0926\n",
      "test epoch: 2364, loss: 0.86\n",
      "train epoch: 2365, loss 0.0927\n",
      "test epoch: 2365, loss: 0.82\n",
      "train epoch: 2366, loss 0.0930\n",
      "test epoch: 2366, loss: 0.87\n",
      "train epoch: 2367, loss 0.0935\n",
      "test epoch: 2367, loss: 0.81\n",
      "train epoch: 2368, loss 0.0940\n",
      "test epoch: 2368, loss: 0.87\n",
      "train epoch: 2369, loss 0.0947\n",
      "test epoch: 2369, loss: 0.78\n",
      "train epoch: 2370, loss 0.0953\n",
      "test epoch: 2370, loss: 0.88\n",
      "train epoch: 2371, loss 0.0957\n",
      "test epoch: 2371, loss: 0.78\n",
      "train epoch: 2372, loss 0.0959\n",
      "test epoch: 2372, loss: 0.88\n",
      "train epoch: 2373, loss 0.0960\n",
      "test epoch: 2373, loss: 0.78\n",
      "train epoch: 2374, loss 0.0960\n",
      "test epoch: 2374, loss: 0.88\n",
      "train epoch: 2375, loss 0.0966\n",
      "test epoch: 2375, loss: 0.77\n",
      "train epoch: 2376, loss 0.0969\n",
      "test epoch: 2376, loss: 0.88\n",
      "train epoch: 2377, loss 0.0963\n",
      "test epoch: 2377, loss: 0.79\n",
      "train epoch: 2378, loss 0.0953\n",
      "test epoch: 2378, loss: 0.87\n",
      "train epoch: 2379, loss 0.0946\n",
      "test epoch: 2379, loss: 0.81\n",
      "train epoch: 2380, loss 0.0938\n",
      "test epoch: 2380, loss: 0.86\n",
      "train epoch: 2381, loss 0.0933\n",
      "test epoch: 2381, loss: 0.81\n",
      "train epoch: 2382, loss 0.0929\n",
      "test epoch: 2382, loss: 0.85\n",
      "train epoch: 2383, loss 0.0926\n",
      "test epoch: 2383, loss: 0.84\n",
      "train epoch: 2384, loss 0.0925\n",
      "test epoch: 2384, loss: 0.85\n",
      "train epoch: 2385, loss 0.0925\n",
      "test epoch: 2385, loss: 0.86\n",
      "train epoch: 2386, loss 0.0925\n",
      "test epoch: 2386, loss: 0.84\n",
      "train epoch: 2387, loss 0.0925\n",
      "test epoch: 2387, loss: 0.85\n",
      "train epoch: 2388, loss 0.0927\n",
      "test epoch: 2388, loss: 0.82\n",
      "train epoch: 2389, loss 0.0928\n",
      "test epoch: 2389, loss: 0.85\n",
      "train epoch: 2390, loss 0.0930\n",
      "test epoch: 2390, loss: 0.81\n",
      "train epoch: 2391, loss 0.0932\n",
      "test epoch: 2391, loss: 0.85\n",
      "train epoch: 2392, loss 0.0935\n",
      "test epoch: 2392, loss: 0.79\n",
      "train epoch: 2393, loss 0.0939\n",
      "test epoch: 2393, loss: 0.86\n",
      "train epoch: 2394, loss 0.0944\n",
      "test epoch: 2394, loss: 0.78\n",
      "train epoch: 2395, loss 0.0948\n",
      "test epoch: 2395, loss: 0.87\n",
      "train epoch: 2396, loss 0.0955\n",
      "test epoch: 2396, loss: 0.77\n",
      "train epoch: 2397, loss 0.0963\n",
      "test epoch: 2397, loss: 0.87\n",
      "train epoch: 2398, loss 0.0980\n",
      "test epoch: 2398, loss: 0.73\n",
      "train epoch: 2399, loss 0.0993\n",
      "test epoch: 2399, loss: 0.88\n",
      "train epoch: 2400, loss 0.0997\n",
      "test epoch: 2400, loss: 0.75\n",
      "train epoch: 2401, loss 0.0989\n",
      "test epoch: 2401, loss: 0.89\n",
      "train epoch: 2402, loss 0.0979\n",
      "test epoch: 2402, loss: 0.78\n",
      "train epoch: 2403, loss 0.0972\n",
      "test epoch: 2403, loss: 0.87\n",
      "train epoch: 2404, loss 0.0967\n",
      "test epoch: 2404, loss: 0.76\n",
      "train epoch: 2405, loss 0.0966\n",
      "test epoch: 2405, loss: 0.86\n",
      "train epoch: 2406, loss 0.0960\n",
      "test epoch: 2406, loss: 0.77\n",
      "train epoch: 2407, loss 0.0948\n",
      "test epoch: 2407, loss: 0.85\n",
      "train epoch: 2408, loss 0.0934\n",
      "test epoch: 2408, loss: 0.82\n",
      "train epoch: 2409, loss 0.0928\n",
      "test epoch: 2409, loss: 0.85\n",
      "train epoch: 2410, loss 0.0928\n",
      "test epoch: 2410, loss: 0.85\n",
      "train epoch: 2411, loss 0.0925\n",
      "test epoch: 2411, loss: 0.81\n",
      "train epoch: 2412, loss 0.0927\n",
      "test epoch: 2412, loss: 0.84\n",
      "train epoch: 2413, loss 0.0932\n",
      "test epoch: 2413, loss: 0.80\n",
      "train epoch: 2414, loss 0.0935\n",
      "test epoch: 2414, loss: 0.86\n",
      "train epoch: 2415, loss 0.0936\n",
      "test epoch: 2415, loss: 0.81\n",
      "train epoch: 2416, loss 0.0935\n",
      "test epoch: 2416, loss: 0.88\n",
      "train epoch: 2417, loss 0.0937\n",
      "test epoch: 2417, loss: 0.81\n",
      "train epoch: 2418, loss 0.0940\n",
      "test epoch: 2418, loss: 0.87\n",
      "train epoch: 2419, loss 0.0944\n",
      "test epoch: 2419, loss: 0.78\n",
      "train epoch: 2420, loss 0.0952\n",
      "test epoch: 2420, loss: 0.87\n",
      "train epoch: 2421, loss 0.0955\n",
      "test epoch: 2421, loss: 0.78\n",
      "train epoch: 2422, loss 0.0950\n",
      "test epoch: 2422, loss: 0.87\n",
      "train epoch: 2423, loss 0.0942\n",
      "test epoch: 2423, loss: 0.81\n",
      "train epoch: 2424, loss 0.0941\n",
      "test epoch: 2424, loss: 0.87\n",
      "train epoch: 2425, loss 0.0936\n",
      "test epoch: 2425, loss: 0.80\n",
      "train epoch: 2426, loss 0.0936\n",
      "test epoch: 2426, loss: 0.85\n",
      "train epoch: 2427, loss 0.0935\n",
      "test epoch: 2427, loss: 0.80\n",
      "train epoch: 2428, loss 0.0932\n",
      "test epoch: 2428, loss: 0.85\n",
      "train epoch: 2429, loss 0.0928\n",
      "test epoch: 2429, loss: 0.82\n",
      "train epoch: 2430, loss 0.0925\n",
      "test epoch: 2430, loss: 0.85\n",
      "train epoch: 2431, loss 0.0925\n",
      "test epoch: 2431, loss: 0.83\n",
      "train epoch: 2432, loss 0.0922\n",
      "test epoch: 2432, loss: 0.83\n",
      "train epoch: 2433, loss 0.0921\n",
      "test epoch: 2433, loss: 0.83\n",
      "train epoch: 2434, loss 0.0922\n",
      "test epoch: 2434, loss: 0.83\n",
      "train epoch: 2435, loss 0.0922\n",
      "test epoch: 2435, loss: 0.85\n",
      "train epoch: 2436, loss 0.0923\n",
      "test epoch: 2436, loss: 0.83\n",
      "train epoch: 2437, loss 0.0925\n",
      "test epoch: 2437, loss: 0.86\n",
      "train epoch: 2438, loss 0.0928\n",
      "test epoch: 2438, loss: 0.80\n",
      "train epoch: 2439, loss 0.0934\n",
      "test epoch: 2439, loss: 0.87\n",
      "train epoch: 2440, loss 0.0941\n",
      "test epoch: 2440, loss: 0.79\n",
      "train epoch: 2441, loss 0.0949\n",
      "test epoch: 2441, loss: 0.88\n",
      "train epoch: 2442, loss 0.0962\n",
      "test epoch: 2442, loss: 0.77\n",
      "train epoch: 2443, loss 0.0978\n",
      "test epoch: 2443, loss: 0.90\n",
      "train epoch: 2444, loss 0.0995\n",
      "test epoch: 2444, loss: 0.74\n",
      "train epoch: 2445, loss 0.1009\n",
      "test epoch: 2445, loss: 0.90\n",
      "train epoch: 2446, loss 0.1017\n",
      "test epoch: 2446, loss: 0.73\n",
      "train epoch: 2447, loss 0.1008\n",
      "test epoch: 2447, loss: 0.88\n",
      "train epoch: 2448, loss 0.0984\n",
      "test epoch: 2448, loss: 0.78\n",
      "train epoch: 2449, loss 0.0954\n",
      "test epoch: 2449, loss: 0.86\n",
      "train epoch: 2450, loss 0.0933\n",
      "test epoch: 2450, loss: 0.82\n",
      "train epoch: 2451, loss 0.0923\n",
      "test epoch: 2451, loss: 0.82\n",
      "train epoch: 2452, loss 0.0921\n",
      "test epoch: 2452, loss: 0.84\n",
      "train epoch: 2453, loss 0.0926\n",
      "test epoch: 2453, loss: 0.79\n",
      "train epoch: 2454, loss 0.0933\n",
      "test epoch: 2454, loss: 0.86\n",
      "train epoch: 2455, loss 0.0941\n",
      "test epoch: 2455, loss: 0.78\n",
      "train epoch: 2456, loss 0.0946\n",
      "test epoch: 2456, loss: 0.88\n",
      "train epoch: 2457, loss 0.0948\n",
      "test epoch: 2457, loss: 0.80\n",
      "train epoch: 2458, loss 0.0951\n",
      "test epoch: 2458, loss: 0.88\n",
      "train epoch: 2459, loss 0.0955\n",
      "test epoch: 2459, loss: 0.77\n",
      "train epoch: 2460, loss 0.0960\n",
      "test epoch: 2460, loss: 0.87\n",
      "train epoch: 2461, loss 0.0958\n",
      "test epoch: 2461, loss: 0.78\n",
      "train epoch: 2462, loss 0.0950\n",
      "test epoch: 2462, loss: 0.86\n",
      "train epoch: 2463, loss 0.0939\n",
      "test epoch: 2463, loss: 0.81\n",
      "train epoch: 2464, loss 0.0931\n",
      "test epoch: 2464, loss: 0.85\n",
      "train epoch: 2465, loss 0.0925\n",
      "test epoch: 2465, loss: 0.81\n",
      "train epoch: 2466, loss 0.0920\n",
      "test epoch: 2466, loss: 0.81\n",
      "train epoch: 2467, loss 0.0921\n",
      "test epoch: 2467, loss: 0.83\n",
      "train epoch: 2468, loss 0.0922\n",
      "test epoch: 2468, loss: 0.80\n",
      "train epoch: 2469, loss 0.0923\n",
      "test epoch: 2469, loss: 0.85\n",
      "train epoch: 2470, loss 0.0926\n",
      "test epoch: 2470, loss: 0.80\n",
      "train epoch: 2471, loss 0.0927\n",
      "test epoch: 2471, loss: 0.85\n",
      "train epoch: 2472, loss 0.0929\n",
      "test epoch: 2472, loss: 0.80\n",
      "train epoch: 2473, loss 0.0929\n",
      "test epoch: 2473, loss: 0.85\n",
      "train epoch: 2474, loss 0.0930\n",
      "test epoch: 2474, loss: 0.80\n",
      "train epoch: 2475, loss 0.0929\n",
      "test epoch: 2475, loss: 0.86\n",
      "train epoch: 2476, loss 0.0928\n",
      "test epoch: 2476, loss: 0.81\n",
      "train epoch: 2477, loss 0.0926\n",
      "test epoch: 2477, loss: 0.86\n",
      "train epoch: 2478, loss 0.0925\n",
      "test epoch: 2478, loss: 0.82\n",
      "train epoch: 2479, loss 0.0924\n",
      "test epoch: 2479, loss: 0.85\n",
      "train epoch: 2480, loss 0.0925\n",
      "test epoch: 2480, loss: 0.80\n",
      "train epoch: 2481, loss 0.0926\n",
      "test epoch: 2481, loss: 0.85\n",
      "train epoch: 2482, loss 0.0926\n",
      "test epoch: 2482, loss: 0.81\n",
      "train epoch: 2483, loss 0.0927\n",
      "test epoch: 2483, loss: 0.85\n",
      "train epoch: 2484, loss 0.0927\n",
      "test epoch: 2484, loss: 0.80\n",
      "train epoch: 2485, loss 0.0928\n",
      "test epoch: 2485, loss: 0.85\n",
      "train epoch: 2486, loss 0.0930\n",
      "test epoch: 2486, loss: 0.79\n",
      "train epoch: 2487, loss 0.0931\n",
      "test epoch: 2487, loss: 0.86\n",
      "train epoch: 2488, loss 0.0932\n",
      "test epoch: 2488, loss: 0.79\n",
      "train epoch: 2489, loss 0.0934\n",
      "test epoch: 2489, loss: 0.86\n",
      "train epoch: 2490, loss 0.0939\n",
      "test epoch: 2490, loss: 0.77\n",
      "train epoch: 2491, loss 0.0944\n",
      "test epoch: 2491, loss: 0.86\n",
      "train epoch: 2492, loss 0.0947\n",
      "test epoch: 2492, loss: 0.78\n",
      "train epoch: 2493, loss 0.0947\n",
      "test epoch: 2493, loss: 0.87\n",
      "train epoch: 2494, loss 0.0949\n",
      "test epoch: 2494, loss: 0.78\n",
      "train epoch: 2495, loss 0.0950\n",
      "test epoch: 2495, loss: 0.87\n",
      "train epoch: 2496, loss 0.0952\n",
      "test epoch: 2496, loss: 0.78\n",
      "train epoch: 2497, loss 0.0953\n",
      "test epoch: 2497, loss: 0.88\n",
      "train epoch: 2498, loss 0.0953\n",
      "test epoch: 2498, loss: 0.78\n",
      "train epoch: 2499, loss 0.0955\n",
      "test epoch: 2499, loss: 0.88\n",
      "train epoch: 2500, loss 0.0960\n",
      "test epoch: 2500, loss: 0.77\n",
      "train epoch: 2501, loss 0.0964\n",
      "test epoch: 2501, loss: 0.88\n",
      "train epoch: 2502, loss 0.0964\n",
      "test epoch: 2502, loss: 0.77\n",
      "train epoch: 2503, loss 0.0961\n",
      "test epoch: 2503, loss: 0.89\n",
      "train epoch: 2504, loss 0.0956\n",
      "test epoch: 2504, loss: 0.79\n",
      "train epoch: 2505, loss 0.0954\n",
      "test epoch: 2505, loss: 0.87\n",
      "train epoch: 2506, loss 0.0954\n",
      "test epoch: 2506, loss: 0.76\n",
      "train epoch: 2507, loss 0.0956\n",
      "test epoch: 2507, loss: 0.86\n",
      "train epoch: 2508, loss 0.0950\n",
      "test epoch: 2508, loss: 0.78\n",
      "train epoch: 2509, loss 0.0938\n",
      "test epoch: 2509, loss: 0.86\n",
      "train epoch: 2510, loss 0.0928\n",
      "test epoch: 2510, loss: 0.81\n",
      "train epoch: 2511, loss 0.0922\n",
      "test epoch: 2511, loss: 0.84\n",
      "train epoch: 2512, loss 0.0918\n",
      "test epoch: 2512, loss: 0.82\n",
      "train epoch: 2513, loss 0.0917\n",
      "test epoch: 2513, loss: 0.82\n",
      "train epoch: 2514, loss 0.0916\n",
      "test epoch: 2514, loss: 0.83\n",
      "train epoch: 2515, loss 0.0916\n",
      "test epoch: 2515, loss: 0.82\n",
      "train epoch: 2516, loss 0.0919\n",
      "test epoch: 2516, loss: 0.85\n",
      "train epoch: 2517, loss 0.0922\n",
      "test epoch: 2517, loss: 0.80\n",
      "train epoch: 2518, loss 0.0927\n",
      "test epoch: 2518, loss: 0.86\n",
      "train epoch: 2519, loss 0.0933\n",
      "test epoch: 2519, loss: 0.79\n",
      "train epoch: 2520, loss 0.0938\n",
      "test epoch: 2520, loss: 0.87\n",
      "train epoch: 2521, loss 0.0944\n",
      "test epoch: 2521, loss: 0.78\n",
      "train epoch: 2522, loss 0.0951\n",
      "test epoch: 2522, loss: 0.87\n",
      "train epoch: 2523, loss 0.0962\n",
      "test epoch: 2523, loss: 0.75\n",
      "train epoch: 2524, loss 0.0973\n",
      "test epoch: 2524, loss: 0.87\n",
      "train epoch: 2525, loss 0.0977\n",
      "test epoch: 2525, loss: 0.75\n",
      "train epoch: 2526, loss 0.0970\n",
      "test epoch: 2526, loss: 0.87\n",
      "train epoch: 2527, loss 0.0957\n",
      "test epoch: 2527, loss: 0.78\n",
      "train epoch: 2528, loss 0.0946\n",
      "test epoch: 2528, loss: 0.85\n",
      "train epoch: 2529, loss 0.0937\n",
      "test epoch: 2529, loss: 0.78\n",
      "train epoch: 2530, loss 0.0932\n",
      "test epoch: 2530, loss: 0.83\n",
      "train epoch: 2531, loss 0.0926\n",
      "test epoch: 2531, loss: 0.80\n",
      "train epoch: 2532, loss 0.0919\n",
      "test epoch: 2532, loss: 0.83\n",
      "train epoch: 2533, loss 0.0915\n",
      "test epoch: 2533, loss: 0.83\n",
      "train epoch: 2534, loss 0.0917\n",
      "test epoch: 2534, loss: 0.82\n",
      "train epoch: 2535, loss 0.0919\n",
      "test epoch: 2535, loss: 0.84\n",
      "train epoch: 2536, loss 0.0921\n",
      "test epoch: 2536, loss: 0.78\n",
      "train epoch: 2537, loss 0.0930\n",
      "test epoch: 2537, loss: 0.85\n",
      "train epoch: 2538, loss 0.0935\n",
      "test epoch: 2538, loss: 0.79\n",
      "train epoch: 2539, loss 0.0937\n",
      "test epoch: 2539, loss: 0.87\n",
      "train epoch: 2540, loss 0.0940\n",
      "test epoch: 2540, loss: 0.79\n",
      "train epoch: 2541, loss 0.0940\n",
      "test epoch: 2541, loss: 0.86\n",
      "train epoch: 2542, loss 0.0945\n",
      "test epoch: 2542, loss: 0.76\n",
      "train epoch: 2543, loss 0.0951\n",
      "test epoch: 2543, loss: 0.86\n",
      "train epoch: 2544, loss 0.0953\n",
      "test epoch: 2544, loss: 0.76\n",
      "train epoch: 2545, loss 0.0948\n",
      "test epoch: 2545, loss: 0.86\n",
      "train epoch: 2546, loss 0.0939\n",
      "test epoch: 2546, loss: 0.79\n",
      "train epoch: 2547, loss 0.0936\n",
      "test epoch: 2547, loss: 0.86\n",
      "train epoch: 2548, loss 0.0932\n",
      "test epoch: 2548, loss: 0.78\n",
      "train epoch: 2549, loss 0.0929\n",
      "test epoch: 2549, loss: 0.84\n",
      "train epoch: 2550, loss 0.0926\n",
      "test epoch: 2550, loss: 0.79\n",
      "train epoch: 2551, loss 0.0921\n",
      "test epoch: 2551, loss: 0.83\n",
      "train epoch: 2552, loss 0.0917\n",
      "test epoch: 2552, loss: 0.81\n",
      "train epoch: 2553, loss 0.0916\n",
      "test epoch: 2553, loss: 0.83\n",
      "train epoch: 2554, loss 0.0914\n",
      "test epoch: 2554, loss: 0.81\n",
      "train epoch: 2555, loss 0.0914\n",
      "test epoch: 2555, loss: 0.81\n",
      "train epoch: 2556, loss 0.0914\n",
      "test epoch: 2556, loss: 0.83\n",
      "train epoch: 2557, loss 0.0914\n",
      "test epoch: 2557, loss: 0.81\n",
      "train epoch: 2558, loss 0.0916\n",
      "test epoch: 2558, loss: 0.85\n",
      "train epoch: 2559, loss 0.0918\n",
      "test epoch: 2559, loss: 0.80\n",
      "train epoch: 2560, loss 0.0921\n",
      "test epoch: 2560, loss: 0.85\n",
      "train epoch: 2561, loss 0.0929\n",
      "test epoch: 2561, loss: 0.78\n",
      "train epoch: 2562, loss 0.0937\n",
      "test epoch: 2562, loss: 0.87\n",
      "train epoch: 2563, loss 0.0949\n",
      "test epoch: 2563, loss: 0.77\n",
      "train epoch: 2564, loss 0.0963\n",
      "test epoch: 2564, loss: 0.89\n",
      "train epoch: 2565, loss 0.0985\n",
      "test epoch: 2565, loss: 0.74\n",
      "train epoch: 2566, loss 0.1007\n",
      "test epoch: 2566, loss: 0.90\n",
      "train epoch: 2567, loss 0.1027\n",
      "test epoch: 2567, loss: 0.72\n",
      "train epoch: 2568, loss 0.1030\n",
      "test epoch: 2568, loss: 0.89\n",
      "train epoch: 2569, loss 0.1021\n",
      "test epoch: 2569, loss: 0.74\n",
      "train epoch: 2570, loss 0.0993\n",
      "test epoch: 2570, loss: 0.87\n",
      "train epoch: 2571, loss 0.0964\n",
      "test epoch: 2571, loss: 0.77\n",
      "train epoch: 2572, loss 0.0934\n",
      "test epoch: 2572, loss: 0.83\n",
      "train epoch: 2573, loss 0.0916\n",
      "test epoch: 2573, loss: 0.82\n",
      "train epoch: 2574, loss 0.0912\n",
      "test epoch: 2574, loss: 0.79\n",
      "train epoch: 2575, loss 0.0918\n",
      "test epoch: 2575, loss: 0.85\n",
      "train epoch: 2576, loss 0.0930\n",
      "test epoch: 2576, loss: 0.77\n",
      "train epoch: 2577, loss 0.0942\n",
      "test epoch: 2577, loss: 0.87\n",
      "train epoch: 2578, loss 0.0952\n",
      "test epoch: 2578, loss: 0.77\n",
      "train epoch: 2579, loss 0.0957\n",
      "test epoch: 2579, loss: 0.87\n",
      "train epoch: 2580, loss 0.0958\n",
      "test epoch: 2580, loss: 0.77\n",
      "train epoch: 2581, loss 0.0951\n",
      "test epoch: 2581, loss: 0.86\n",
      "train epoch: 2582, loss 0.0938\n",
      "test epoch: 2582, loss: 0.79\n",
      "train epoch: 2583, loss 0.0925\n",
      "test epoch: 2583, loss: 0.84\n",
      "train epoch: 2584, loss 0.0916\n",
      "test epoch: 2584, loss: 0.82\n",
      "train epoch: 2585, loss 0.0911\n",
      "test epoch: 2585, loss: 0.81\n",
      "train epoch: 2586, loss 0.0912\n",
      "test epoch: 2586, loss: 0.84\n",
      "train epoch: 2587, loss 0.0916\n",
      "test epoch: 2587, loss: 0.80\n",
      "train epoch: 2588, loss 0.0922\n",
      "test epoch: 2588, loss: 0.86\n",
      "train epoch: 2589, loss 0.0928\n",
      "test epoch: 2589, loss: 0.79\n",
      "train epoch: 2590, loss 0.0932\n",
      "test epoch: 2590, loss: 0.86\n",
      "train epoch: 2591, loss 0.0936\n",
      "test epoch: 2591, loss: 0.78\n",
      "train epoch: 2592, loss 0.0935\n",
      "test epoch: 2592, loss: 0.85\n",
      "train epoch: 2593, loss 0.0934\n",
      "test epoch: 2593, loss: 0.78\n",
      "train epoch: 2594, loss 0.0928\n",
      "test epoch: 2594, loss: 0.84\n",
      "train epoch: 2595, loss 0.0921\n",
      "test epoch: 2595, loss: 0.80\n",
      "train epoch: 2596, loss 0.0915\n",
      "test epoch: 2596, loss: 0.82\n",
      "train epoch: 2597, loss 0.0912\n",
      "test epoch: 2597, loss: 0.80\n",
      "train epoch: 2598, loss 0.0911\n",
      "test epoch: 2598, loss: 0.80\n",
      "train epoch: 2599, loss 0.0911\n",
      "test epoch: 2599, loss: 0.82\n",
      "train epoch: 2600, loss 0.0911\n",
      "test epoch: 2600, loss: 0.80\n",
      "train epoch: 2601, loss 0.0912\n",
      "test epoch: 2601, loss: 0.83\n",
      "train epoch: 2602, loss 0.0914\n",
      "test epoch: 2602, loss: 0.79\n",
      "train epoch: 2603, loss 0.0914\n",
      "test epoch: 2603, loss: 0.82\n",
      "train epoch: 2604, loss 0.0914\n",
      "test epoch: 2604, loss: 0.79\n",
      "train epoch: 2605, loss 0.0914\n",
      "test epoch: 2605, loss: 0.83\n",
      "train epoch: 2606, loss 0.0913\n",
      "test epoch: 2606, loss: 0.80\n",
      "train epoch: 2607, loss 0.0912\n",
      "test epoch: 2607, loss: 0.82\n",
      "train epoch: 2608, loss 0.0912\n",
      "test epoch: 2608, loss: 0.80\n",
      "train epoch: 2609, loss 0.0911\n",
      "test epoch: 2609, loss: 0.83\n",
      "train epoch: 2610, loss 0.0911\n",
      "test epoch: 2610, loss: 0.81\n",
      "train epoch: 2611, loss 0.0910\n",
      "test epoch: 2611, loss: 0.83\n",
      "train epoch: 2612, loss 0.0910\n",
      "test epoch: 2612, loss: 0.81\n",
      "train epoch: 2613, loss 0.0911\n",
      "test epoch: 2613, loss: 0.83\n",
      "train epoch: 2614, loss 0.0910\n",
      "test epoch: 2614, loss: 0.82\n",
      "train epoch: 2615, loss 0.0911\n",
      "test epoch: 2615, loss: 0.84\n",
      "train epoch: 2616, loss 0.0912\n",
      "test epoch: 2616, loss: 0.80\n",
      "train epoch: 2617, loss 0.0914\n",
      "test epoch: 2617, loss: 0.84\n",
      "train epoch: 2618, loss 0.0917\n",
      "test epoch: 2618, loss: 0.79\n",
      "train epoch: 2619, loss 0.0921\n",
      "test epoch: 2619, loss: 0.86\n",
      "train epoch: 2620, loss 0.0927\n",
      "test epoch: 2620, loss: 0.78\n",
      "train epoch: 2621, loss 0.0934\n",
      "test epoch: 2621, loss: 0.86\n",
      "train epoch: 2622, loss 0.0943\n",
      "test epoch: 2622, loss: 0.75\n",
      "train epoch: 2623, loss 0.0953\n",
      "test epoch: 2623, loss: 0.86\n",
      "train epoch: 2624, loss 0.0963\n",
      "test epoch: 2624, loss: 0.74\n",
      "train epoch: 2625, loss 0.0969\n",
      "test epoch: 2625, loss: 0.87\n",
      "train epoch: 2626, loss 0.0976\n",
      "test epoch: 2626, loss: 0.75\n",
      "train epoch: 2627, loss 0.0980\n",
      "test epoch: 2627, loss: 0.88\n",
      "train epoch: 2628, loss 0.0989\n",
      "test epoch: 2628, loss: 0.73\n",
      "train epoch: 2629, loss 0.0992\n",
      "test epoch: 2629, loss: 0.87\n",
      "train epoch: 2630, loss 0.0982\n",
      "test epoch: 2630, loss: 0.75\n",
      "train epoch: 2631, loss 0.0964\n",
      "test epoch: 2631, loss: 0.87\n",
      "train epoch: 2632, loss 0.0950\n",
      "test epoch: 2632, loss: 0.78\n",
      "train epoch: 2633, loss 0.0936\n",
      "test epoch: 2633, loss: 0.85\n",
      "train epoch: 2634, loss 0.0923\n",
      "test epoch: 2634, loss: 0.80\n",
      "train epoch: 2635, loss 0.0914\n",
      "test epoch: 2635, loss: 0.83\n",
      "train epoch: 2636, loss 0.0909\n",
      "test epoch: 2636, loss: 0.82\n",
      "train epoch: 2637, loss 0.0907\n",
      "test epoch: 2637, loss: 0.81\n",
      "train epoch: 2638, loss 0.0909\n",
      "test epoch: 2638, loss: 0.84\n",
      "train epoch: 2639, loss 0.0914\n",
      "test epoch: 2639, loss: 0.79\n",
      "train epoch: 2640, loss 0.0920\n",
      "test epoch: 2640, loss: 0.86\n",
      "train epoch: 2641, loss 0.0930\n",
      "test epoch: 2641, loss: 0.77\n",
      "train epoch: 2642, loss 0.0943\n",
      "test epoch: 2642, loss: 0.87\n",
      "train epoch: 2643, loss 0.0956\n",
      "test epoch: 2643, loss: 0.75\n",
      "train epoch: 2644, loss 0.0967\n",
      "test epoch: 2644, loss: 0.86\n",
      "train epoch: 2645, loss 0.0978\n",
      "test epoch: 2645, loss: 0.73\n",
      "train epoch: 2646, loss 0.0974\n",
      "test epoch: 2646, loss: 0.85\n",
      "train epoch: 2647, loss 0.0954\n",
      "test epoch: 2647, loss: 0.77\n",
      "train epoch: 2648, loss 0.0931\n",
      "test epoch: 2648, loss: 0.84\n",
      "train epoch: 2649, loss 0.0917\n",
      "test epoch: 2649, loss: 0.81\n",
      "train epoch: 2650, loss 0.0909\n",
      "test epoch: 2650, loss: 0.80\n",
      "train epoch: 2651, loss 0.0910\n",
      "test epoch: 2651, loss: 0.83\n",
      "train epoch: 2652, loss 0.0918\n",
      "test epoch: 2652, loss: 0.77\n",
      "train epoch: 2653, loss 0.0926\n",
      "test epoch: 2653, loss: 0.85\n",
      "train epoch: 2654, loss 0.0932\n",
      "test epoch: 2654, loss: 0.78\n",
      "train epoch: 2655, loss 0.0938\n",
      "test epoch: 2655, loss: 0.86\n",
      "train epoch: 2656, loss 0.0945\n",
      "test epoch: 2656, loss: 0.76\n",
      "train epoch: 2657, loss 0.0949\n",
      "test epoch: 2657, loss: 0.86\n",
      "train epoch: 2658, loss 0.0949\n",
      "test epoch: 2658, loss: 0.76\n",
      "train epoch: 2659, loss 0.0940\n",
      "test epoch: 2659, loss: 0.85\n",
      "train epoch: 2660, loss 0.0933\n",
      "test epoch: 2660, loss: 0.78\n",
      "train epoch: 2661, loss 0.0923\n",
      "test epoch: 2661, loss: 0.83\n",
      "train epoch: 2662, loss 0.0915\n",
      "test epoch: 2662, loss: 0.79\n",
      "train epoch: 2663, loss 0.0909\n",
      "test epoch: 2663, loss: 0.81\n",
      "train epoch: 2664, loss 0.0907\n",
      "test epoch: 2664, loss: 0.81\n",
      "train epoch: 2665, loss 0.0906\n",
      "test epoch: 2665, loss: 0.80\n",
      "train epoch: 2666, loss 0.0908\n",
      "test epoch: 2666, loss: 0.82\n",
      "train epoch: 2667, loss 0.0910\n",
      "test epoch: 2667, loss: 0.78\n",
      "train epoch: 2668, loss 0.0910\n",
      "test epoch: 2668, loss: 0.81\n",
      "train epoch: 2669, loss 0.0912\n",
      "test epoch: 2669, loss: 0.78\n",
      "train epoch: 2670, loss 0.0911\n",
      "test epoch: 2670, loss: 0.82\n",
      "train epoch: 2671, loss 0.0909\n",
      "test epoch: 2671, loss: 0.79\n",
      "train epoch: 2672, loss 0.0909\n",
      "test epoch: 2672, loss: 0.82\n",
      "train epoch: 2673, loss 0.0909\n",
      "test epoch: 2673, loss: 0.80\n",
      "train epoch: 2674, loss 0.0906\n",
      "test epoch: 2674, loss: 0.80\n",
      "train epoch: 2675, loss 0.0907\n",
      "test epoch: 2675, loss: 0.79\n",
      "train epoch: 2676, loss 0.0908\n",
      "test epoch: 2676, loss: 0.80\n",
      "train epoch: 2677, loss 0.0907\n",
      "test epoch: 2677, loss: 0.81\n",
      "train epoch: 2678, loss 0.0905\n",
      "test epoch: 2678, loss: 0.82\n",
      "train epoch: 2679, loss 0.0906\n",
      "test epoch: 2679, loss: 0.84\n",
      "train epoch: 2680, loss 0.0909\n",
      "test epoch: 2680, loss: 0.82\n",
      "train epoch: 2681, loss 0.0907\n",
      "test epoch: 2681, loss: 0.82\n",
      "train epoch: 2682, loss 0.0907\n",
      "test epoch: 2682, loss: 0.80\n",
      "train epoch: 2683, loss 0.0909\n",
      "test epoch: 2683, loss: 0.81\n",
      "train epoch: 2684, loss 0.0908\n",
      "test epoch: 2684, loss: 0.80\n",
      "train epoch: 2685, loss 0.0908\n",
      "test epoch: 2685, loss: 0.83\n",
      "train epoch: 2686, loss 0.0908\n",
      "test epoch: 2686, loss: 0.80\n",
      "train epoch: 2687, loss 0.0910\n",
      "test epoch: 2687, loss: 0.83\n",
      "train epoch: 2688, loss 0.0912\n",
      "test epoch: 2688, loss: 0.77\n",
      "train epoch: 2689, loss 0.0917\n",
      "test epoch: 2689, loss: 0.83\n",
      "train epoch: 2690, loss 0.0922\n",
      "test epoch: 2690, loss: 0.76\n",
      "train epoch: 2691, loss 0.0928\n",
      "test epoch: 2691, loss: 0.85\n",
      "train epoch: 2692, loss 0.0937\n",
      "test epoch: 2692, loss: 0.75\n",
      "train epoch: 2693, loss 0.0949\n",
      "test epoch: 2693, loss: 0.86\n",
      "train epoch: 2694, loss 0.0967\n",
      "test epoch: 2694, loss: 0.72\n",
      "train epoch: 2695, loss 0.0988\n",
      "test epoch: 2695, loss: 0.88\n",
      "train epoch: 2696, loss 0.1015\n",
      "test epoch: 2696, loss: 0.71\n",
      "train epoch: 2697, loss 0.1042\n",
      "test epoch: 2697, loss: 0.91\n",
      "train epoch: 2698, loss 0.1077\n",
      "test epoch: 2698, loss: 0.70\n",
      "train epoch: 2699, loss 0.1109\n",
      "test epoch: 2699, loss: 0.92\n",
      "train epoch: 2700, loss 0.1147\n",
      "test epoch: 2700, loss: 0.67\n",
      "train epoch: 2701, loss 0.1129\n",
      "test epoch: 2701, loss: 0.88\n",
      "train epoch: 2702, loss 0.1045\n",
      "test epoch: 2702, loss: 0.74\n",
      "train epoch: 2703, loss 0.0947\n",
      "test epoch: 2703, loss: 0.80\n",
      "train epoch: 2704, loss 0.0905\n",
      "test epoch: 2704, loss: 0.85\n",
      "train epoch: 2705, loss 0.0934\n",
      "test epoch: 2705, loss: 0.73\n",
      "train epoch: 2706, loss 0.1011\n",
      "test epoch: 2706, loss: 0.90\n",
      "train epoch: 2707, loss 0.1110\n",
      "test epoch: 2707, loss: 0.65\n",
      "train epoch: 2708, loss 0.1164\n",
      "test epoch: 2708, loss: 0.88\n",
      "train epoch: 2709, loss 0.1097\n",
      "test epoch: 2709, loss: 0.72\n",
      "train epoch: 2710, loss 0.0960\n",
      "test epoch: 2710, loss: 0.78\n",
      "train epoch: 2711, loss 0.0907\n",
      "test epoch: 2711, loss: 0.87\n",
      "train epoch: 2712, loss 0.0972\n",
      "test epoch: 2712, loss: 0.73\n",
      "train epoch: 2713, loss 0.1069\n",
      "test epoch: 2713, loss: 0.93\n",
      "train epoch: 2714, loss 0.1124\n",
      "test epoch: 2714, loss: 0.68\n",
      "train epoch: 2715, loss 0.1084\n",
      "test epoch: 2715, loss: 0.84\n",
      "train epoch: 2716, loss 0.0962\n",
      "test epoch: 2716, loss: 0.79\n",
      "train epoch: 2717, loss 0.0914\n",
      "test epoch: 2717, loss: 0.73\n",
      "train epoch: 2718, loss 0.0981\n",
      "test epoch: 2718, loss: 0.91\n",
      "train epoch: 2719, loss 0.1039\n",
      "test epoch: 2719, loss: 0.74\n",
      "train epoch: 2720, loss 0.1027\n",
      "test epoch: 2720, loss: 0.87\n",
      "train epoch: 2721, loss 0.0965\n",
      "test epoch: 2721, loss: 0.77\n",
      "train epoch: 2722, loss 0.0915\n",
      "test epoch: 2722, loss: 0.77\n",
      "train epoch: 2723, loss 0.0921\n",
      "test epoch: 2723, loss: 0.85\n",
      "train epoch: 2724, loss 0.0962\n",
      "test epoch: 2724, loss: 0.74\n",
      "train epoch: 2725, loss 0.0963\n",
      "test epoch: 2725, loss: 0.85\n",
      "train epoch: 2726, loss 0.0927\n",
      "test epoch: 2726, loss: 0.82\n",
      "train epoch: 2727, loss 0.0909\n",
      "test epoch: 2727, loss: 0.78\n",
      "train epoch: 2728, loss 0.0920\n",
      "test epoch: 2728, loss: 0.84\n",
      "train epoch: 2729, loss 0.0939\n",
      "test epoch: 2729, loss: 0.74\n",
      "train epoch: 2730, loss 0.0939\n",
      "test epoch: 2730, loss: 0.81\n",
      "train epoch: 2731, loss 0.0917\n",
      "test epoch: 2731, loss: 0.80\n",
      "train epoch: 2732, loss 0.0905\n",
      "test epoch: 2732, loss: 0.77\n",
      "train epoch: 2733, loss 0.0919\n",
      "test epoch: 2733, loss: 0.85\n",
      "train epoch: 2734, loss 0.0933\n",
      "test epoch: 2734, loss: 0.78\n",
      "train epoch: 2735, loss 0.0923\n",
      "test epoch: 2735, loss: 0.81\n",
      "train epoch: 2736, loss 0.0907\n",
      "test epoch: 2736, loss: 0.79\n",
      "train epoch: 2737, loss 0.0907\n",
      "test epoch: 2737, loss: 0.77\n",
      "train epoch: 2738, loss 0.0914\n",
      "test epoch: 2738, loss: 0.82\n",
      "train epoch: 2739, loss 0.0916\n",
      "test epoch: 2739, loss: 0.78\n",
      "train epoch: 2740, loss 0.0910\n",
      "test epoch: 2740, loss: 0.82\n",
      "train epoch: 2741, loss 0.0905\n",
      "test epoch: 2741, loss: 0.82\n",
      "train epoch: 2742, loss 0.0908\n",
      "test epoch: 2742, loss: 0.80\n",
      "train epoch: 2743, loss 0.0910\n",
      "test epoch: 2743, loss: 0.82\n",
      "train epoch: 2744, loss 0.0908\n",
      "test epoch: 2744, loss: 0.78\n",
      "train epoch: 2745, loss 0.0909\n",
      "test epoch: 2745, loss: 0.80\n",
      "train epoch: 2746, loss 0.0906\n",
      "test epoch: 2746, loss: 0.81\n",
      "train epoch: 2747, loss 0.0903\n",
      "test epoch: 2747, loss: 0.81\n",
      "train epoch: 2748, loss 0.0907\n",
      "test epoch: 2748, loss: 0.84\n",
      "train epoch: 2749, loss 0.0910\n",
      "test epoch: 2749, loss: 0.80\n",
      "train epoch: 2750, loss 0.0907\n",
      "test epoch: 2750, loss: 0.82\n",
      "train epoch: 2751, loss 0.0904\n",
      "test epoch: 2751, loss: 0.80\n",
      "train epoch: 2752, loss 0.0903\n",
      "test epoch: 2752, loss: 0.80\n",
      "train epoch: 2753, loss 0.0903\n",
      "test epoch: 2753, loss: 0.82\n",
      "train epoch: 2754, loss 0.0904\n",
      "test epoch: 2754, loss: 0.80\n",
      "train epoch: 2755, loss 0.0904\n",
      "test epoch: 2755, loss: 0.81\n",
      "train epoch: 2756, loss 0.0902\n",
      "test epoch: 2756, loss: 0.80\n",
      "train epoch: 2757, loss 0.0902\n",
      "test epoch: 2757, loss: 0.80\n",
      "train epoch: 2758, loss 0.0902\n",
      "test epoch: 2758, loss: 0.82\n",
      "train epoch: 2759, loss 0.0903\n",
      "test epoch: 2759, loss: 0.80\n",
      "train epoch: 2760, loss 0.0903\n",
      "test epoch: 2760, loss: 0.82\n",
      "train epoch: 2761, loss 0.0902\n",
      "test epoch: 2761, loss: 0.80\n",
      "train epoch: 2762, loss 0.0902\n",
      "test epoch: 2762, loss: 0.81\n",
      "train epoch: 2763, loss 0.0901\n",
      "test epoch: 2763, loss: 0.82\n",
      "train epoch: 2764, loss 0.0901\n",
      "test epoch: 2764, loss: 0.80\n",
      "train epoch: 2765, loss 0.0901\n",
      "test epoch: 2765, loss: 0.82\n",
      "train epoch: 2766, loss 0.0901\n",
      "test epoch: 2766, loss: 0.81\n",
      "train epoch: 2767, loss 0.0901\n",
      "test epoch: 2767, loss: 0.82\n",
      "train epoch: 2768, loss 0.0901\n",
      "test epoch: 2768, loss: 0.81\n",
      "train epoch: 2769, loss 0.0900\n",
      "test epoch: 2769, loss: 0.81\n",
      "train epoch: 2770, loss 0.0900\n",
      "test epoch: 2770, loss: 0.81\n",
      "train epoch: 2771, loss 0.0900\n",
      "test epoch: 2771, loss: 0.79\n",
      "train epoch: 2772, loss 0.0901\n",
      "test epoch: 2772, loss: 0.81\n",
      "train epoch: 2773, loss 0.0900\n",
      "test epoch: 2773, loss: 0.80\n",
      "train epoch: 2774, loss 0.0900\n",
      "test epoch: 2774, loss: 0.81\n",
      "train epoch: 2775, loss 0.0900\n",
      "test epoch: 2775, loss: 0.80\n",
      "train epoch: 2776, loss 0.0899\n",
      "test epoch: 2776, loss: 0.80\n",
      "train epoch: 2777, loss 0.0900\n",
      "test epoch: 2777, loss: 0.81\n",
      "train epoch: 2778, loss 0.0899\n",
      "test epoch: 2778, loss: 0.80\n",
      "train epoch: 2779, loss 0.0899\n",
      "test epoch: 2779, loss: 0.81\n",
      "train epoch: 2780, loss 0.0899\n",
      "test epoch: 2780, loss: 0.80\n",
      "train epoch: 2781, loss 0.0899\n",
      "test epoch: 2781, loss: 0.81\n",
      "train epoch: 2782, loss 0.0899\n",
      "test epoch: 2782, loss: 0.80\n",
      "train epoch: 2783, loss 0.0899\n",
      "test epoch: 2783, loss: 0.81\n",
      "train epoch: 2784, loss 0.0899\n",
      "test epoch: 2784, loss: 0.81\n",
      "train epoch: 2785, loss 0.0899\n",
      "test epoch: 2785, loss: 0.81\n",
      "train epoch: 2786, loss 0.0898\n",
      "test epoch: 2786, loss: 0.81\n",
      "train epoch: 2787, loss 0.0899\n",
      "test epoch: 2787, loss: 0.81\n",
      "train epoch: 2788, loss 0.0899\n",
      "test epoch: 2788, loss: 0.80\n",
      "train epoch: 2789, loss 0.0898\n",
      "test epoch: 2789, loss: 0.80\n",
      "train epoch: 2790, loss 0.0898\n",
      "test epoch: 2790, loss: 0.80\n",
      "train epoch: 2791, loss 0.0899\n",
      "test epoch: 2791, loss: 0.80\n",
      "train epoch: 2792, loss 0.0898\n",
      "test epoch: 2792, loss: 0.81\n",
      "train epoch: 2793, loss 0.0898\n",
      "test epoch: 2793, loss: 0.81\n",
      "train epoch: 2794, loss 0.0898\n",
      "test epoch: 2794, loss: 0.80\n",
      "train epoch: 2795, loss 0.0898\n",
      "test epoch: 2795, loss: 0.80\n",
      "train epoch: 2796, loss 0.0898\n",
      "test epoch: 2796, loss: 0.80\n",
      "train epoch: 2797, loss 0.0898\n",
      "test epoch: 2797, loss: 0.81\n",
      "train epoch: 2798, loss 0.0897\n",
      "test epoch: 2798, loss: 0.81\n",
      "train epoch: 2799, loss 0.0898\n",
      "test epoch: 2799, loss: 0.81\n",
      "train epoch: 2800, loss 0.0899\n",
      "test epoch: 2800, loss: 0.81\n",
      "train epoch: 2801, loss 0.0897\n",
      "test epoch: 2801, loss: 0.79\n",
      "train epoch: 2802, loss 0.0899\n",
      "test epoch: 2802, loss: 0.81\n",
      "train epoch: 2803, loss 0.0899\n",
      "test epoch: 2803, loss: 0.79\n",
      "train epoch: 2804, loss 0.0900\n",
      "test epoch: 2804, loss: 0.82\n",
      "train epoch: 2805, loss 0.0900\n",
      "test epoch: 2805, loss: 0.79\n",
      "train epoch: 2806, loss 0.0899\n",
      "test epoch: 2806, loss: 0.81\n",
      "train epoch: 2807, loss 0.0898\n",
      "test epoch: 2807, loss: 0.78\n",
      "train epoch: 2808, loss 0.0898\n",
      "test epoch: 2808, loss: 0.80\n",
      "train epoch: 2809, loss 0.0897\n",
      "test epoch: 2809, loss: 0.80\n",
      "train epoch: 2810, loss 0.0896\n",
      "test epoch: 2810, loss: 0.80\n",
      "train epoch: 2811, loss 0.0896\n",
      "test epoch: 2811, loss: 0.80\n",
      "train epoch: 2812, loss 0.0896\n",
      "test epoch: 2812, loss: 0.79\n",
      "train epoch: 2813, loss 0.0897\n",
      "test epoch: 2813, loss: 0.80\n",
      "train epoch: 2814, loss 0.0896\n",
      "test epoch: 2814, loss: 0.80\n",
      "train epoch: 2815, loss 0.0896\n",
      "test epoch: 2815, loss: 0.81\n",
      "train epoch: 2816, loss 0.0896\n",
      "test epoch: 2816, loss: 0.79\n",
      "train epoch: 2817, loss 0.0897\n",
      "test epoch: 2817, loss: 0.81\n",
      "train epoch: 2818, loss 0.0897\n",
      "test epoch: 2818, loss: 0.79\n",
      "train epoch: 2819, loss 0.0898\n",
      "test epoch: 2819, loss: 0.82\n",
      "train epoch: 2820, loss 0.0899\n",
      "test epoch: 2820, loss: 0.79\n",
      "train epoch: 2821, loss 0.0901\n",
      "test epoch: 2821, loss: 0.82\n",
      "train epoch: 2822, loss 0.0903\n",
      "test epoch: 2822, loss: 0.77\n",
      "train epoch: 2823, loss 0.0906\n",
      "test epoch: 2823, loss: 0.82\n",
      "train epoch: 2824, loss 0.0907\n",
      "test epoch: 2824, loss: 0.77\n",
      "train epoch: 2825, loss 0.0907\n",
      "test epoch: 2825, loss: 0.83\n",
      "train epoch: 2826, loss 0.0908\n",
      "test epoch: 2826, loss: 0.77\n",
      "train epoch: 2827, loss 0.0907\n",
      "test epoch: 2827, loss: 0.82\n",
      "train epoch: 2828, loss 0.0906\n",
      "test epoch: 2828, loss: 0.77\n",
      "train epoch: 2829, loss 0.0906\n",
      "test epoch: 2829, loss: 0.82\n",
      "train epoch: 2830, loss 0.0904\n",
      "test epoch: 2830, loss: 0.78\n",
      "train epoch: 2831, loss 0.0902\n",
      "test epoch: 2831, loss: 0.83\n",
      "train epoch: 2832, loss 0.0902\n",
      "test epoch: 2832, loss: 0.79\n",
      "train epoch: 2833, loss 0.0899\n",
      "test epoch: 2833, loss: 0.81\n",
      "train epoch: 2834, loss 0.0899\n",
      "test epoch: 2834, loss: 0.78\n",
      "train epoch: 2835, loss 0.0900\n",
      "test epoch: 2835, loss: 0.81\n",
      "train epoch: 2836, loss 0.0899\n",
      "test epoch: 2836, loss: 0.78\n",
      "train epoch: 2837, loss 0.0901\n",
      "test epoch: 2837, loss: 0.83\n",
      "train epoch: 2838, loss 0.0904\n",
      "test epoch: 2838, loss: 0.78\n",
      "train epoch: 2839, loss 0.0904\n",
      "test epoch: 2839, loss: 0.82\n",
      "train epoch: 2840, loss 0.0909\n",
      "test epoch: 2840, loss: 0.75\n",
      "train epoch: 2841, loss 0.0918\n",
      "test epoch: 2841, loss: 0.82\n",
      "train epoch: 2842, loss 0.0926\n",
      "test epoch: 2842, loss: 0.74\n",
      "train epoch: 2843, loss 0.0932\n",
      "test epoch: 2843, loss: 0.85\n",
      "train epoch: 2844, loss 0.0943\n",
      "test epoch: 2844, loss: 0.74\n",
      "train epoch: 2845, loss 0.0968\n",
      "test epoch: 2845, loss: 0.87\n",
      "train epoch: 2846, loss 0.1011\n",
      "test epoch: 2846, loss: 0.68\n",
      "train epoch: 2847, loss 0.1067\n",
      "test epoch: 2847, loss: 0.89\n",
      "train epoch: 2848, loss 0.1123\n",
      "test epoch: 2848, loss: 0.65\n",
      "train epoch: 2849, loss 0.1132\n",
      "test epoch: 2849, loss: 0.91\n",
      "train epoch: 2850, loss 0.1137\n",
      "test epoch: 2850, loss: 0.65\n",
      "train epoch: 2851, loss 0.1181\n",
      "test epoch: 2851, loss: 0.91\n",
      "train epoch: 2852, loss 0.1207\n",
      "test epoch: 2852, loss: 0.65\n",
      "train epoch: 2853, loss 0.1111\n",
      "test epoch: 2853, loss: 0.83\n",
      "train epoch: 2854, loss 0.0977\n",
      "test epoch: 2854, loss: 0.75\n",
      "train epoch: 2855, loss 0.0898\n",
      "test epoch: 2855, loss: 0.72\n",
      "train epoch: 2856, loss 0.0927\n",
      "test epoch: 2856, loss: 0.85\n",
      "train epoch: 2857, loss 0.1013\n",
      "test epoch: 2857, loss: 0.68\n",
      "train epoch: 2858, loss 0.1061\n",
      "test epoch: 2858, loss: 0.86\n",
      "train epoch: 2859, loss 0.1040\n",
      "test epoch: 2859, loss: 0.70\n",
      "train epoch: 2860, loss 0.0955\n",
      "test epoch: 2860, loss: 0.77\n",
      "train epoch: 2861, loss 0.0897\n",
      "test epoch: 2861, loss: 0.81\n",
      "train epoch: 2862, loss 0.0920\n",
      "test epoch: 2862, loss: 0.72\n",
      "train epoch: 2863, loss 0.0981\n",
      "test epoch: 2863, loss: 0.87\n",
      "train epoch: 2864, loss 0.1009\n",
      "test epoch: 2864, loss: 0.71\n",
      "train epoch: 2865, loss 0.0969\n",
      "test epoch: 2865, loss: 0.80\n",
      "train epoch: 2866, loss 0.0912\n",
      "test epoch: 2866, loss: 0.78\n",
      "train epoch: 2867, loss 0.0900\n",
      "test epoch: 2867, loss: 0.73\n",
      "train epoch: 2868, loss 0.0932\n",
      "test epoch: 2868, loss: 0.85\n",
      "train epoch: 2869, loss 0.0952\n",
      "test epoch: 2869, loss: 0.75\n",
      "train epoch: 2870, loss 0.0942\n",
      "test epoch: 2870, loss: 0.83\n",
      "train epoch: 2871, loss 0.0913\n",
      "test epoch: 2871, loss: 0.78\n",
      "train epoch: 2872, loss 0.0894\n",
      "test epoch: 2872, loss: 0.76\n",
      "train epoch: 2873, loss 0.0904\n",
      "test epoch: 2873, loss: 0.82\n",
      "train epoch: 2874, loss 0.0921\n",
      "test epoch: 2874, loss: 0.75\n",
      "train epoch: 2875, loss 0.0921\n",
      "test epoch: 2875, loss: 0.82\n",
      "train epoch: 2876, loss 0.0907\n",
      "test epoch: 2876, loss: 0.79\n",
      "train epoch: 2877, loss 0.0895\n",
      "test epoch: 2877, loss: 0.77\n",
      "train epoch: 2878, loss 0.0896\n",
      "test epoch: 2878, loss: 0.80\n",
      "train epoch: 2879, loss 0.0907\n",
      "test epoch: 2879, loss: 0.74\n",
      "train epoch: 2880, loss 0.0909\n",
      "test epoch: 2880, loss: 0.79\n",
      "train epoch: 2881, loss 0.0900\n",
      "test epoch: 2881, loss: 0.78\n",
      "train epoch: 2882, loss 0.0893\n",
      "test epoch: 2882, loss: 0.78\n",
      "train epoch: 2883, loss 0.0894\n",
      "test epoch: 2883, loss: 0.80\n",
      "train epoch: 2884, loss 0.0899\n",
      "test epoch: 2884, loss: 0.75\n",
      "train epoch: 2885, loss 0.0902\n",
      "test epoch: 2885, loss: 0.80\n",
      "train epoch: 2886, loss 0.0899\n",
      "test epoch: 2886, loss: 0.77\n",
      "train epoch: 2887, loss 0.0893\n",
      "test epoch: 2887, loss: 0.78\n",
      "train epoch: 2888, loss 0.0892\n",
      "test epoch: 2888, loss: 0.80\n",
      "train epoch: 2889, loss 0.0896\n",
      "test epoch: 2889, loss: 0.76\n",
      "train epoch: 2890, loss 0.0900\n",
      "test epoch: 2890, loss: 0.80\n",
      "train epoch: 2891, loss 0.0900\n",
      "test epoch: 2891, loss: 0.77\n",
      "train epoch: 2892, loss 0.0895\n",
      "test epoch: 2892, loss: 0.79\n",
      "train epoch: 2893, loss 0.0892\n",
      "test epoch: 2893, loss: 0.79\n",
      "train epoch: 2894, loss 0.0892\n",
      "test epoch: 2894, loss: 0.78\n",
      "train epoch: 2895, loss 0.0893\n",
      "test epoch: 2895, loss: 0.80\n",
      "train epoch: 2896, loss 0.0895\n",
      "test epoch: 2896, loss: 0.77\n",
      "train epoch: 2897, loss 0.0894\n",
      "test epoch: 2897, loss: 0.80\n",
      "train epoch: 2898, loss 0.0891\n",
      "test epoch: 2898, loss: 0.80\n",
      "train epoch: 2899, loss 0.0892\n",
      "test epoch: 2899, loss: 0.80\n",
      "train epoch: 2900, loss 0.0893\n",
      "test epoch: 2900, loss: 0.81\n",
      "train epoch: 2901, loss 0.0893\n",
      "test epoch: 2901, loss: 0.77\n",
      "train epoch: 2902, loss 0.0897\n",
      "test epoch: 2902, loss: 0.80\n",
      "train epoch: 2903, loss 0.0897\n",
      "test epoch: 2903, loss: 0.78\n",
      "train epoch: 2904, loss 0.0893\n",
      "test epoch: 2904, loss: 0.80\n",
      "train epoch: 2905, loss 0.0890\n",
      "test epoch: 2905, loss: 0.80\n",
      "train epoch: 2906, loss 0.0891\n",
      "test epoch: 2906, loss: 0.78\n",
      "train epoch: 2907, loss 0.0894\n",
      "test epoch: 2907, loss: 0.80\n",
      "train epoch: 2908, loss 0.0895\n",
      "test epoch: 2908, loss: 0.75\n",
      "train epoch: 2909, loss 0.0897\n",
      "test epoch: 2909, loss: 0.78\n",
      "train epoch: 2910, loss 0.0893\n",
      "test epoch: 2910, loss: 0.77\n",
      "train epoch: 2911, loss 0.0890\n",
      "test epoch: 2911, loss: 0.77\n",
      "train epoch: 2912, loss 0.0890\n",
      "test epoch: 2912, loss: 0.78\n",
      "train epoch: 2913, loss 0.0891\n",
      "test epoch: 2913, loss: 0.76\n",
      "train epoch: 2914, loss 0.0892\n",
      "test epoch: 2914, loss: 0.78\n",
      "train epoch: 2915, loss 0.0893\n",
      "test epoch: 2915, loss: 0.76\n",
      "train epoch: 2916, loss 0.0892\n",
      "test epoch: 2916, loss: 0.78\n",
      "train epoch: 2917, loss 0.0890\n",
      "test epoch: 2917, loss: 0.77\n",
      "train epoch: 2918, loss 0.0889\n",
      "test epoch: 2918, loss: 0.77\n",
      "train epoch: 2919, loss 0.0889\n",
      "test epoch: 2919, loss: 0.79\n",
      "train epoch: 2920, loss 0.0890\n",
      "test epoch: 2920, loss: 0.77\n",
      "train epoch: 2921, loss 0.0892\n",
      "test epoch: 2921, loss: 0.79\n",
      "train epoch: 2922, loss 0.0893\n",
      "test epoch: 2922, loss: 0.76\n",
      "train epoch: 2923, loss 0.0893\n",
      "test epoch: 2923, loss: 0.79\n",
      "train epoch: 2924, loss 0.0891\n",
      "test epoch: 2924, loss: 0.77\n",
      "train epoch: 2925, loss 0.0889\n",
      "test epoch: 2925, loss: 0.78\n",
      "train epoch: 2926, loss 0.0888\n",
      "test epoch: 2926, loss: 0.79\n",
      "train epoch: 2927, loss 0.0888\n",
      "test epoch: 2927, loss: 0.77\n",
      "train epoch: 2928, loss 0.0889\n",
      "test epoch: 2928, loss: 0.79\n",
      "train epoch: 2929, loss 0.0890\n",
      "test epoch: 2929, loss: 0.77\n",
      "train epoch: 2930, loss 0.0891\n",
      "test epoch: 2930, loss: 0.80\n",
      "train epoch: 2931, loss 0.0891\n",
      "test epoch: 2931, loss: 0.77\n",
      "train epoch: 2932, loss 0.0890\n",
      "test epoch: 2932, loss: 0.79\n",
      "train epoch: 2933, loss 0.0888\n",
      "test epoch: 2933, loss: 0.78\n",
      "train epoch: 2934, loss 0.0887\n",
      "test epoch: 2934, loss: 0.79\n",
      "train epoch: 2935, loss 0.0887\n",
      "test epoch: 2935, loss: 0.79\n",
      "train epoch: 2936, loss 0.0887\n",
      "test epoch: 2936, loss: 0.78\n",
      "train epoch: 2937, loss 0.0887\n",
      "test epoch: 2937, loss: 0.80\n",
      "train epoch: 2938, loss 0.0887\n",
      "test epoch: 2938, loss: 0.79\n",
      "train epoch: 2939, loss 0.0887\n",
      "test epoch: 2939, loss: 0.79\n",
      "train epoch: 2940, loss 0.0887\n",
      "test epoch: 2940, loss: 0.78\n",
      "train epoch: 2941, loss 0.0887\n",
      "test epoch: 2941, loss: 0.79\n",
      "train epoch: 2942, loss 0.0887\n",
      "test epoch: 2942, loss: 0.78\n",
      "train epoch: 2943, loss 0.0887\n",
      "test epoch: 2943, loss: 0.79\n",
      "train epoch: 2944, loss 0.0888\n",
      "test epoch: 2944, loss: 0.77\n",
      "train epoch: 2945, loss 0.0887\n",
      "test epoch: 2945, loss: 0.79\n",
      "train epoch: 2946, loss 0.0887\n",
      "test epoch: 2946, loss: 0.78\n",
      "train epoch: 2947, loss 0.0886\n",
      "test epoch: 2947, loss: 0.78\n",
      "train epoch: 2948, loss 0.0886\n",
      "test epoch: 2948, loss: 0.78\n",
      "train epoch: 2949, loss 0.0886\n",
      "test epoch: 2949, loss: 0.78\n",
      "train epoch: 2950, loss 0.0885\n",
      "test epoch: 2950, loss: 0.79\n",
      "train epoch: 2951, loss 0.0886\n",
      "test epoch: 2951, loss: 0.78\n",
      "train epoch: 2952, loss 0.0885\n",
      "test epoch: 2952, loss: 0.79\n",
      "train epoch: 2953, loss 0.0885\n",
      "test epoch: 2953, loss: 0.78\n",
      "train epoch: 2954, loss 0.0885\n",
      "test epoch: 2954, loss: 0.79\n",
      "train epoch: 2955, loss 0.0886\n",
      "test epoch: 2955, loss: 0.77\n",
      "train epoch: 2956, loss 0.0886\n",
      "test epoch: 2956, loss: 0.80\n",
      "train epoch: 2957, loss 0.0886\n",
      "test epoch: 2957, loss: 0.78\n",
      "train epoch: 2958, loss 0.0887\n",
      "test epoch: 2958, loss: 0.80\n",
      "train epoch: 2959, loss 0.0889\n",
      "test epoch: 2959, loss: 0.77\n",
      "train epoch: 2960, loss 0.0892\n",
      "test epoch: 2960, loss: 0.81\n",
      "train epoch: 2961, loss 0.0896\n",
      "test epoch: 2961, loss: 0.75\n",
      "train epoch: 2962, loss 0.0903\n",
      "test epoch: 2962, loss: 0.82\n",
      "train epoch: 2963, loss 0.0915\n",
      "test epoch: 2963, loss: 0.72\n",
      "train epoch: 2964, loss 0.0928\n",
      "test epoch: 2964, loss: 0.83\n",
      "train epoch: 2965, loss 0.0939\n",
      "test epoch: 2965, loss: 0.71\n",
      "train epoch: 2966, loss 0.0951\n",
      "test epoch: 2966, loss: 0.83\n",
      "train epoch: 2967, loss 0.0967\n",
      "test epoch: 2967, loss: 0.69\n",
      "train epoch: 2968, loss 0.0981\n",
      "test epoch: 2968, loss: 0.83\n",
      "train epoch: 2969, loss 0.1000\n",
      "test epoch: 2969, loss: 0.67\n",
      "train epoch: 2970, loss 0.1000\n",
      "test epoch: 2970, loss: 0.82\n",
      "train epoch: 2971, loss 0.0973\n",
      "test epoch: 2971, loss: 0.70\n",
      "train epoch: 2972, loss 0.0936\n",
      "test epoch: 2972, loss: 0.80\n",
      "train epoch: 2973, loss 0.0909\n",
      "test epoch: 2973, loss: 0.75\n",
      "train epoch: 2974, loss 0.0892\n",
      "test epoch: 2974, loss: 0.76\n",
      "train epoch: 2975, loss 0.0884\n",
      "test epoch: 2975, loss: 0.76\n",
      "train epoch: 2976, loss 0.0885\n",
      "test epoch: 2976, loss: 0.74\n",
      "train epoch: 2977, loss 0.0890\n",
      "test epoch: 2977, loss: 0.78\n",
      "train epoch: 2978, loss 0.0895\n",
      "test epoch: 2978, loss: 0.73\n",
      "train epoch: 2979, loss 0.0901\n",
      "test epoch: 2979, loss: 0.80\n",
      "train epoch: 2980, loss 0.0906\n",
      "test epoch: 2980, loss: 0.74\n",
      "train epoch: 2981, loss 0.0912\n",
      "test epoch: 2981, loss: 0.81\n",
      "train epoch: 2982, loss 0.0916\n",
      "test epoch: 2982, loss: 0.72\n",
      "train epoch: 2983, loss 0.0920\n",
      "test epoch: 2983, loss: 0.80\n",
      "train epoch: 2984, loss 0.0917\n",
      "test epoch: 2984, loss: 0.73\n",
      "train epoch: 2985, loss 0.0908\n",
      "test epoch: 2985, loss: 0.80\n",
      "train epoch: 2986, loss 0.0899\n",
      "test epoch: 2986, loss: 0.76\n",
      "train epoch: 2987, loss 0.0893\n",
      "test epoch: 2987, loss: 0.79\n",
      "train epoch: 2988, loss 0.0889\n",
      "test epoch: 2988, loss: 0.76\n",
      "train epoch: 2989, loss 0.0887\n",
      "test epoch: 2989, loss: 0.78\n",
      "train epoch: 2990, loss 0.0885\n",
      "test epoch: 2990, loss: 0.77\n",
      "train epoch: 2991, loss 0.0883\n",
      "test epoch: 2991, loss: 0.78\n",
      "train epoch: 2992, loss 0.0883\n",
      "test epoch: 2992, loss: 0.79\n",
      "train epoch: 2993, loss 0.0883\n",
      "test epoch: 2993, loss: 0.77\n",
      "train epoch: 2994, loss 0.0884\n",
      "test epoch: 2994, loss: 0.79\n",
      "train epoch: 2995, loss 0.0886\n",
      "test epoch: 2995, loss: 0.76\n",
      "train epoch: 2996, loss 0.0888\n",
      "test epoch: 2996, loss: 0.80\n",
      "train epoch: 2997, loss 0.0894\n",
      "test epoch: 2997, loss: 0.74\n",
      "train epoch: 2998, loss 0.0898\n",
      "test epoch: 2998, loss: 0.80\n",
      "train epoch: 2999, loss 0.0900\n",
      "test epoch: 2999, loss: 0.73\n",
      "train epoch: 3000, loss 0.0899\n",
      "test epoch: 3000, loss: 0.79\n",
      "train epoch: 3001, loss 0.0901\n",
      "test epoch: 3001, loss: 0.72\n",
      "train epoch: 3002, loss 0.0900\n",
      "test epoch: 3002, loss: 0.79\n",
      "train epoch: 3003, loss 0.0897\n",
      "test epoch: 3003, loss: 0.74\n",
      "train epoch: 3004, loss 0.0895\n",
      "test epoch: 3004, loss: 0.79\n",
      "train epoch: 3005, loss 0.0895\n",
      "test epoch: 3005, loss: 0.73\n",
      "train epoch: 3006, loss 0.0896\n",
      "test epoch: 3006, loss: 0.78\n",
      "train epoch: 3007, loss 0.0897\n",
      "test epoch: 3007, loss: 0.72\n",
      "train epoch: 3008, loss 0.0897\n",
      "test epoch: 3008, loss: 0.78\n",
      "train epoch: 3009, loss 0.0897\n",
      "test epoch: 3009, loss: 0.73\n",
      "train epoch: 3010, loss 0.0898\n",
      "test epoch: 3010, loss: 0.79\n",
      "train epoch: 3011, loss 0.0897\n",
      "test epoch: 3011, loss: 0.74\n",
      "train epoch: 3012, loss 0.0902\n",
      "test epoch: 3012, loss: 0.80\n",
      "train epoch: 3013, loss 0.0905\n",
      "test epoch: 3013, loss: 0.71\n",
      "train epoch: 3014, loss 0.0915\n",
      "test epoch: 3014, loss: 0.80\n",
      "train epoch: 3015, loss 0.0924\n",
      "test epoch: 3015, loss: 0.71\n",
      "train epoch: 3016, loss 0.0929\n",
      "test epoch: 3016, loss: 0.82\n",
      "train epoch: 3017, loss 0.0933\n",
      "test epoch: 3017, loss: 0.72\n",
      "train epoch: 3018, loss 0.0939\n",
      "test epoch: 3018, loss: 0.83\n",
      "train epoch: 3019, loss 0.0947\n",
      "test epoch: 3019, loss: 0.70\n",
      "train epoch: 3020, loss 0.0954\n",
      "test epoch: 3020, loss: 0.83\n",
      "train epoch: 3021, loss 0.0965\n",
      "test epoch: 3021, loss: 0.69\n",
      "train epoch: 3022, loss 0.0963\n",
      "test epoch: 3022, loss: 0.82\n",
      "train epoch: 3023, loss 0.0952\n",
      "test epoch: 3023, loss: 0.71\n",
      "train epoch: 3024, loss 0.0938\n",
      "test epoch: 3024, loss: 0.82\n",
      "train epoch: 3025, loss 0.0925\n",
      "test epoch: 3025, loss: 0.73\n",
      "train epoch: 3026, loss 0.0919\n",
      "test epoch: 3026, loss: 0.80\n",
      "train epoch: 3027, loss 0.0911\n",
      "test epoch: 3027, loss: 0.71\n",
      "train epoch: 3028, loss 0.0906\n",
      "test epoch: 3028, loss: 0.77\n",
      "train epoch: 3029, loss 0.0896\n",
      "test epoch: 3029, loss: 0.74\n",
      "train epoch: 3030, loss 0.0885\n",
      "test epoch: 3030, loss: 0.77\n",
      "train epoch: 3031, loss 0.0881\n",
      "test epoch: 3031, loss: 0.77\n",
      "train epoch: 3032, loss 0.0881\n",
      "test epoch: 3032, loss: 0.75\n",
      "train epoch: 3033, loss 0.0886\n",
      "test epoch: 3033, loss: 0.79\n",
      "train epoch: 3034, loss 0.0894\n",
      "test epoch: 3034, loss: 0.72\n",
      "train epoch: 3035, loss 0.0906\n",
      "test epoch: 3035, loss: 0.80\n",
      "train epoch: 3036, loss 0.0914\n",
      "test epoch: 3036, loss: 0.72\n",
      "train epoch: 3037, loss 0.0916\n",
      "test epoch: 3037, loss: 0.81\n",
      "train epoch: 3038, loss 0.0913\n",
      "test epoch: 3038, loss: 0.73\n",
      "train epoch: 3039, loss 0.0912\n",
      "test epoch: 3039, loss: 0.81\n",
      "train epoch: 3040, loss 0.0910\n",
      "test epoch: 3040, loss: 0.73\n",
      "train epoch: 3041, loss 0.0907\n",
      "test epoch: 3041, loss: 0.79\n",
      "train epoch: 3042, loss 0.0904\n",
      "test epoch: 3042, loss: 0.73\n",
      "train epoch: 3043, loss 0.0896\n",
      "test epoch: 3043, loss: 0.79\n",
      "train epoch: 3044, loss 0.0888\n",
      "test epoch: 3044, loss: 0.76\n",
      "train epoch: 3045, loss 0.0883\n",
      "test epoch: 3045, loss: 0.77\n",
      "train epoch: 3046, loss 0.0879\n",
      "test epoch: 3046, loss: 0.76\n",
      "train epoch: 3047, loss 0.0879\n",
      "test epoch: 3047, loss: 0.75\n",
      "train epoch: 3048, loss 0.0879\n",
      "test epoch: 3048, loss: 0.77\n",
      "train epoch: 3049, loss 0.0879\n",
      "test epoch: 3049, loss: 0.76\n",
      "train epoch: 3050, loss 0.0880\n",
      "test epoch: 3050, loss: 0.78\n",
      "train epoch: 3051, loss 0.0882\n",
      "test epoch: 3051, loss: 0.74\n",
      "train epoch: 3052, loss 0.0883\n",
      "test epoch: 3052, loss: 0.78\n",
      "train epoch: 3053, loss 0.0885\n",
      "test epoch: 3053, loss: 0.74\n",
      "train epoch: 3054, loss 0.0886\n",
      "test epoch: 3054, loss: 0.79\n",
      "train epoch: 3055, loss 0.0888\n",
      "test epoch: 3055, loss: 0.74\n",
      "train epoch: 3056, loss 0.0892\n",
      "test epoch: 3056, loss: 0.80\n",
      "train epoch: 3057, loss 0.0898\n",
      "test epoch: 3057, loss: 0.72\n",
      "train epoch: 3058, loss 0.0907\n",
      "test epoch: 3058, loss: 0.80\n",
      "train epoch: 3059, loss 0.0920\n",
      "test epoch: 3059, loss: 0.70\n",
      "train epoch: 3060, loss 0.0930\n",
      "test epoch: 3060, loss: 0.81\n",
      "train epoch: 3061, loss 0.0940\n",
      "test epoch: 3061, loss: 0.70\n",
      "train epoch: 3062, loss 0.0953\n",
      "test epoch: 3062, loss: 0.82\n",
      "train epoch: 3063, loss 0.0974\n",
      "test epoch: 3063, loss: 0.66\n",
      "train epoch: 3064, loss 0.0993\n",
      "test epoch: 3064, loss: 0.82\n",
      "train epoch: 3065, loss 0.1004\n",
      "test epoch: 3065, loss: 0.66\n",
      "train epoch: 3066, loss 0.0990\n",
      "test epoch: 3066, loss: 0.80\n",
      "train epoch: 3067, loss 0.0962\n",
      "test epoch: 3067, loss: 0.69\n",
      "train epoch: 3068, loss 0.0924\n",
      "test epoch: 3068, loss: 0.76\n",
      "train epoch: 3069, loss 0.0896\n",
      "test epoch: 3069, loss: 0.72\n",
      "train epoch: 3070, loss 0.0879\n",
      "test epoch: 3070, loss: 0.73\n",
      "train epoch: 3071, loss 0.0878\n",
      "test epoch: 3071, loss: 0.77\n",
      "train epoch: 3072, loss 0.0892\n",
      "test epoch: 3072, loss: 0.70\n",
      "train epoch: 3073, loss 0.0911\n",
      "test epoch: 3073, loss: 0.79\n",
      "train epoch: 3074, loss 0.0932\n",
      "test epoch: 3074, loss: 0.68\n",
      "train epoch: 3075, loss 0.0944\n",
      "test epoch: 3075, loss: 0.80\n",
      "train epoch: 3076, loss 0.0945\n",
      "test epoch: 3076, loss: 0.69\n",
      "train epoch: 3077, loss 0.0932\n",
      "test epoch: 3077, loss: 0.78\n",
      "train epoch: 3078, loss 0.0915\n",
      "test epoch: 3078, loss: 0.71\n",
      "train epoch: 3079, loss 0.0895\n",
      "test epoch: 3079, loss: 0.75\n",
      "train epoch: 3080, loss 0.0881\n",
      "test epoch: 3080, loss: 0.75\n",
      "train epoch: 3081, loss 0.0876\n",
      "test epoch: 3081, loss: 0.74\n",
      "train epoch: 3082, loss 0.0882\n",
      "test epoch: 3082, loss: 0.79\n",
      "train epoch: 3083, loss 0.0894\n",
      "test epoch: 3083, loss: 0.73\n",
      "train epoch: 3084, loss 0.0905\n",
      "test epoch: 3084, loss: 0.80\n",
      "train epoch: 3085, loss 0.0918\n",
      "test epoch: 3085, loss: 0.70\n",
      "train epoch: 3086, loss 0.0925\n",
      "test epoch: 3086, loss: 0.79\n",
      "train epoch: 3087, loss 0.0913\n",
      "test epoch: 3087, loss: 0.73\n",
      "train epoch: 3088, loss 0.0896\n",
      "test epoch: 3088, loss: 0.78\n",
      "train epoch: 3089, loss 0.0883\n",
      "test epoch: 3089, loss: 0.77\n",
      "train epoch: 3090, loss 0.0880\n",
      "test epoch: 3090, loss: 0.77\n",
      "train epoch: 3091, loss 0.0878\n",
      "test epoch: 3091, loss: 0.77\n",
      "train epoch: 3092, loss 0.0877\n",
      "test epoch: 3092, loss: 0.74\n",
      "train epoch: 3093, loss 0.0883\n",
      "test epoch: 3093, loss: 0.78\n",
      "train epoch: 3094, loss 0.0887\n",
      "test epoch: 3094, loss: 0.74\n",
      "train epoch: 3095, loss 0.0886\n",
      "test epoch: 3095, loss: 0.79\n",
      "train epoch: 3096, loss 0.0886\n",
      "test epoch: 3096, loss: 0.75\n",
      "train epoch: 3097, loss 0.0887\n",
      "test epoch: 3097, loss: 0.79\n",
      "train epoch: 3098, loss 0.0885\n",
      "test epoch: 3098, loss: 0.74\n",
      "train epoch: 3099, loss 0.0880\n",
      "test epoch: 3099, loss: 0.76\n",
      "train epoch: 3100, loss 0.0881\n",
      "test epoch: 3100, loss: 0.73\n",
      "train epoch: 3101, loss 0.0879\n",
      "test epoch: 3101, loss: 0.75\n",
      "train epoch: 3102, loss 0.0875\n",
      "test epoch: 3102, loss: 0.76\n",
      "train epoch: 3103, loss 0.0874\n",
      "test epoch: 3103, loss: 0.76\n",
      "train epoch: 3104, loss 0.0876\n",
      "test epoch: 3104, loss: 0.76\n",
      "train epoch: 3105, loss 0.0875\n",
      "test epoch: 3105, loss: 0.73\n",
      "train epoch: 3106, loss 0.0876\n",
      "test epoch: 3106, loss: 0.75\n",
      "train epoch: 3107, loss 0.0879\n",
      "test epoch: 3107, loss: 0.73\n",
      "train epoch: 3108, loss 0.0877\n",
      "test epoch: 3108, loss: 0.77\n",
      "train epoch: 3109, loss 0.0875\n",
      "test epoch: 3109, loss: 0.75\n",
      "train epoch: 3110, loss 0.0875\n",
      "test epoch: 3110, loss: 0.77\n",
      "train epoch: 3111, loss 0.0874\n",
      "test epoch: 3111, loss: 0.75\n",
      "train epoch: 3112, loss 0.0874\n",
      "test epoch: 3112, loss: 0.75\n",
      "train epoch: 3113, loss 0.0873\n",
      "test epoch: 3113, loss: 0.76\n",
      "train epoch: 3114, loss 0.0872\n",
      "test epoch: 3114, loss: 0.76\n",
      "train epoch: 3115, loss 0.0874\n",
      "test epoch: 3115, loss: 0.78\n",
      "train epoch: 3116, loss 0.0877\n",
      "test epoch: 3116, loss: 0.75\n",
      "train epoch: 3117, loss 0.0878\n",
      "test epoch: 3117, loss: 0.77\n",
      "train epoch: 3118, loss 0.0877\n",
      "test epoch: 3118, loss: 0.73\n",
      "train epoch: 3119, loss 0.0881\n",
      "test epoch: 3119, loss: 0.77\n",
      "train epoch: 3120, loss 0.0882\n",
      "test epoch: 3120, loss: 0.74\n",
      "train epoch: 3121, loss 0.0881\n",
      "test epoch: 3121, loss: 0.79\n",
      "train epoch: 3122, loss 0.0883\n",
      "test epoch: 3122, loss: 0.75\n",
      "train epoch: 3123, loss 0.0891\n",
      "test epoch: 3123, loss: 0.81\n",
      "train epoch: 3124, loss 0.0898\n",
      "test epoch: 3124, loss: 0.72\n",
      "train epoch: 3125, loss 0.0913\n",
      "test epoch: 3125, loss: 0.81\n",
      "train epoch: 3126, loss 0.0938\n",
      "test epoch: 3126, loss: 0.68\n",
      "train epoch: 3127, loss 0.0969\n",
      "test epoch: 3127, loss: 0.83\n",
      "train epoch: 3128, loss 0.1010\n",
      "test epoch: 3128, loss: 0.65\n",
      "train epoch: 3129, loss 0.1047\n",
      "test epoch: 3129, loss: 0.84\n",
      "train epoch: 3130, loss 0.1085\n",
      "test epoch: 3130, loss: 0.63\n",
      "train epoch: 3131, loss 0.1095\n",
      "test epoch: 3131, loss: 0.83\n",
      "train epoch: 3132, loss 0.1085\n",
      "test epoch: 3132, loss: 0.63\n",
      "train epoch: 3133, loss 0.1019\n",
      "test epoch: 3133, loss: 0.77\n",
      "train epoch: 3134, loss 0.0928\n",
      "test epoch: 3134, loss: 0.71\n",
      "train epoch: 3135, loss 0.0875\n",
      "test epoch: 3135, loss: 0.71\n",
      "train epoch: 3136, loss 0.0890\n",
      "test epoch: 3136, loss: 0.81\n",
      "train epoch: 3137, loss 0.0951\n",
      "test epoch: 3137, loss: 0.65\n",
      "train epoch: 3138, loss 0.1014\n",
      "test epoch: 3138, loss: 0.82\n",
      "train epoch: 3139, loss 0.1042\n",
      "test epoch: 3139, loss: 0.63\n",
      "train epoch: 3140, loss 0.1007\n",
      "test epoch: 3140, loss: 0.76\n",
      "train epoch: 3141, loss 0.0929\n",
      "test epoch: 3141, loss: 0.71\n",
      "train epoch: 3142, loss 0.0876\n",
      "test epoch: 3142, loss: 0.70\n",
      "train epoch: 3143, loss 0.0895\n",
      "test epoch: 3143, loss: 0.81\n",
      "train epoch: 3144, loss 0.0955\n",
      "test epoch: 3144, loss: 0.67\n",
      "train epoch: 3145, loss 0.0992\n",
      "test epoch: 3145, loss: 0.81\n",
      "train epoch: 3146, loss 0.0983\n",
      "test epoch: 3146, loss: 0.67\n",
      "train epoch: 3147, loss 0.0931\n",
      "test epoch: 3147, loss: 0.74\n",
      "train epoch: 3148, loss 0.0881\n",
      "test epoch: 3148, loss: 0.75\n",
      "train epoch: 3149, loss 0.0882\n",
      "test epoch: 3149, loss: 0.69\n",
      "train epoch: 3150, loss 0.0923\n",
      "test epoch: 3150, loss: 0.81\n",
      "train epoch: 3151, loss 0.0959\n",
      "test epoch: 3151, loss: 0.68\n",
      "train epoch: 3152, loss 0.0943\n",
      "test epoch: 3152, loss: 0.77\n",
      "train epoch: 3153, loss 0.0907\n",
      "test epoch: 3153, loss: 0.72\n",
      "train epoch: 3154, loss 0.0877\n",
      "test epoch: 3154, loss: 0.72\n",
      "train epoch: 3155, loss 0.0879\n",
      "test epoch: 3155, loss: 0.79\n",
      "train epoch: 3156, loss 0.0902\n",
      "test epoch: 3156, loss: 0.71\n",
      "train epoch: 3157, loss 0.0921\n",
      "test epoch: 3157, loss: 0.81\n",
      "train epoch: 3158, loss 0.0919\n",
      "test epoch: 3158, loss: 0.71\n",
      "train epoch: 3159, loss 0.0897\n",
      "test epoch: 3159, loss: 0.76\n",
      "train epoch: 3160, loss 0.0875\n",
      "test epoch: 3160, loss: 0.75\n",
      "train epoch: 3161, loss 0.0872\n",
      "test epoch: 3161, loss: 0.72\n",
      "train epoch: 3162, loss 0.0885\n",
      "test epoch: 3162, loss: 0.80\n",
      "train epoch: 3163, loss 0.0898\n",
      "test epoch: 3163, loss: 0.73\n",
      "train epoch: 3164, loss 0.0898\n",
      "test epoch: 3164, loss: 0.78\n",
      "train epoch: 3165, loss 0.0885\n",
      "test epoch: 3165, loss: 0.72\n",
      "train epoch: 3166, loss 0.0878\n",
      "test epoch: 3166, loss: 0.74\n",
      "train epoch: 3167, loss 0.0874\n",
      "test epoch: 3167, loss: 0.76\n",
      "train epoch: 3168, loss 0.0876\n",
      "test epoch: 3168, loss: 0.73\n",
      "train epoch: 3169, loss 0.0880\n",
      "test epoch: 3169, loss: 0.79\n",
      "train epoch: 3170, loss 0.0884\n",
      "test epoch: 3170, loss: 0.75\n",
      "train epoch: 3171, loss 0.0889\n",
      "test epoch: 3171, loss: 0.79\n",
      "train epoch: 3172, loss 0.0881\n",
      "test epoch: 3172, loss: 0.73\n",
      "train epoch: 3173, loss 0.0873\n",
      "test epoch: 3173, loss: 0.74\n",
      "train epoch: 3174, loss 0.0872\n",
      "test epoch: 3174, loss: 0.75\n",
      "train epoch: 3175, loss 0.0873\n",
      "test epoch: 3175, loss: 0.73\n",
      "train epoch: 3176, loss 0.0874\n",
      "test epoch: 3176, loss: 0.78\n",
      "train epoch: 3177, loss 0.0877\n",
      "test epoch: 3177, loss: 0.75\n",
      "train epoch: 3178, loss 0.0883\n",
      "test epoch: 3178, loss: 0.79\n",
      "train epoch: 3179, loss 0.0880\n",
      "test epoch: 3179, loss: 0.73\n",
      "train epoch: 3180, loss 0.0874\n",
      "test epoch: 3180, loss: 0.75\n",
      "train epoch: 3181, loss 0.0871\n",
      "test epoch: 3181, loss: 0.74\n",
      "train epoch: 3182, loss 0.0869\n",
      "test epoch: 3182, loss: 0.74\n",
      "train epoch: 3183, loss 0.0868\n",
      "test epoch: 3183, loss: 0.78\n",
      "train epoch: 3184, loss 0.0873\n",
      "test epoch: 3184, loss: 0.75\n",
      "train epoch: 3185, loss 0.0879\n",
      "test epoch: 3185, loss: 0.79\n",
      "train epoch: 3186, loss 0.0878\n",
      "test epoch: 3186, loss: 0.73\n",
      "train epoch: 3187, loss 0.0875\n",
      "test epoch: 3187, loss: 0.76\n",
      "train epoch: 3188, loss 0.0872\n",
      "test epoch: 3188, loss: 0.74\n",
      "train epoch: 3189, loss 0.0868\n",
      "test epoch: 3189, loss: 0.74\n",
      "train epoch: 3190, loss 0.0867\n",
      "test epoch: 3190, loss: 0.77\n",
      "train epoch: 3191, loss 0.0871\n",
      "test epoch: 3191, loss: 0.75\n",
      "train epoch: 3192, loss 0.0872\n",
      "test epoch: 3192, loss: 0.78\n",
      "train epoch: 3193, loss 0.0871\n",
      "test epoch: 3193, loss: 0.74\n",
      "train epoch: 3194, loss 0.0871\n",
      "test epoch: 3194, loss: 0.77\n",
      "train epoch: 3195, loss 0.0869\n",
      "test epoch: 3195, loss: 0.74\n",
      "train epoch: 3196, loss 0.0866\n",
      "test epoch: 3196, loss: 0.75\n",
      "train epoch: 3197, loss 0.0865\n",
      "test epoch: 3197, loss: 0.76\n",
      "train epoch: 3198, loss 0.0865\n",
      "test epoch: 3198, loss: 0.74\n",
      "train epoch: 3199, loss 0.0867\n",
      "test epoch: 3199, loss: 0.77\n",
      "train epoch: 3200, loss 0.0869\n",
      "test epoch: 3200, loss: 0.74\n",
      "train epoch: 3201, loss 0.0869\n",
      "test epoch: 3201, loss: 0.76\n",
      "train epoch: 3202, loss 0.0867\n",
      "test epoch: 3202, loss: 0.74\n",
      "train epoch: 3203, loss 0.0866\n",
      "test epoch: 3203, loss: 0.76\n",
      "train epoch: 3204, loss 0.0864\n",
      "test epoch: 3204, loss: 0.75\n",
      "train epoch: 3205, loss 0.0864\n",
      "test epoch: 3205, loss: 0.75\n",
      "train epoch: 3206, loss 0.0864\n",
      "test epoch: 3206, loss: 0.76\n",
      "train epoch: 3207, loss 0.0864\n",
      "test epoch: 3207, loss: 0.75\n",
      "train epoch: 3208, loss 0.0864\n",
      "test epoch: 3208, loss: 0.76\n",
      "train epoch: 3209, loss 0.0865\n",
      "test epoch: 3209, loss: 0.74\n",
      "train epoch: 3210, loss 0.0866\n",
      "test epoch: 3210, loss: 0.76\n",
      "train epoch: 3211, loss 0.0867\n",
      "test epoch: 3211, loss: 0.74\n",
      "train epoch: 3212, loss 0.0864\n",
      "test epoch: 3212, loss: 0.74\n",
      "train epoch: 3213, loss 0.0864\n",
      "test epoch: 3213, loss: 0.74\n",
      "train epoch: 3214, loss 0.0864\n",
      "test epoch: 3214, loss: 0.73\n",
      "train epoch: 3215, loss 0.0864\n",
      "test epoch: 3215, loss: 0.75\n",
      "train epoch: 3216, loss 0.0865\n",
      "test epoch: 3216, loss: 0.72\n",
      "train epoch: 3217, loss 0.0866\n",
      "test epoch: 3217, loss: 0.75\n",
      "train epoch: 3218, loss 0.0866\n",
      "test epoch: 3218, loss: 0.72\n",
      "train epoch: 3219, loss 0.0865\n",
      "test epoch: 3219, loss: 0.74\n",
      "train epoch: 3220, loss 0.0864\n",
      "test epoch: 3220, loss: 0.73\n",
      "train epoch: 3221, loss 0.0863\n",
      "test epoch: 3221, loss: 0.74\n",
      "train epoch: 3222, loss 0.0864\n",
      "test epoch: 3222, loss: 0.74\n",
      "train epoch: 3223, loss 0.0862\n",
      "test epoch: 3223, loss: 0.72\n",
      "train epoch: 3224, loss 0.0863\n",
      "test epoch: 3224, loss: 0.73\n",
      "train epoch: 3225, loss 0.0863\n",
      "test epoch: 3225, loss: 0.72\n",
      "train epoch: 3226, loss 0.0863\n",
      "test epoch: 3226, loss: 0.75\n",
      "train epoch: 3227, loss 0.0865\n",
      "test epoch: 3227, loss: 0.73\n",
      "train epoch: 3228, loss 0.0867\n",
      "test epoch: 3228, loss: 0.76\n",
      "train epoch: 3229, loss 0.0870\n",
      "test epoch: 3229, loss: 0.70\n",
      "train epoch: 3230, loss 0.0878\n",
      "test epoch: 3230, loss: 0.77\n",
      "train epoch: 3231, loss 0.0887\n",
      "test epoch: 3231, loss: 0.69\n",
      "train epoch: 3232, loss 0.0897\n",
      "test epoch: 3232, loss: 0.78\n",
      "train epoch: 3233, loss 0.0906\n",
      "test epoch: 3233, loss: 0.69\n",
      "train epoch: 3234, loss 0.0914\n",
      "test epoch: 3234, loss: 0.79\n",
      "train epoch: 3235, loss 0.0924\n",
      "test epoch: 3235, loss: 0.67\n",
      "train epoch: 3236, loss 0.0933\n",
      "test epoch: 3236, loss: 0.79\n",
      "train epoch: 3237, loss 0.0943\n",
      "test epoch: 3237, loss: 0.67\n",
      "train epoch: 3238, loss 0.0943\n",
      "test epoch: 3238, loss: 0.79\n",
      "train epoch: 3239, loss 0.0934\n",
      "test epoch: 3239, loss: 0.68\n",
      "train epoch: 3240, loss 0.0924\n",
      "test epoch: 3240, loss: 0.79\n",
      "train epoch: 3241, loss 0.0914\n",
      "test epoch: 3241, loss: 0.70\n",
      "train epoch: 3242, loss 0.0910\n",
      "test epoch: 3242, loss: 0.78\n",
      "train epoch: 3243, loss 0.0909\n",
      "test epoch: 3243, loss: 0.68\n",
      "train epoch: 3244, loss 0.0905\n",
      "test epoch: 3244, loss: 0.76\n",
      "train epoch: 3245, loss 0.0895\n",
      "test epoch: 3245, loss: 0.69\n",
      "train epoch: 3246, loss 0.0878\n",
      "test epoch: 3246, loss: 0.75\n",
      "train epoch: 3247, loss 0.0865\n",
      "test epoch: 3247, loss: 0.73\n",
      "train epoch: 3248, loss 0.0861\n",
      "test epoch: 3248, loss: 0.73\n",
      "train epoch: 3249, loss 0.0860\n",
      "test epoch: 3249, loss: 0.74\n",
      "train epoch: 3250, loss 0.0865\n",
      "test epoch: 3250, loss: 0.70\n",
      "train epoch: 3251, loss 0.0872\n",
      "test epoch: 3251, loss: 0.76\n",
      "train epoch: 3252, loss 0.0877\n",
      "test epoch: 3252, loss: 0.70\n",
      "train epoch: 3253, loss 0.0881\n",
      "test epoch: 3253, loss: 0.77\n",
      "train epoch: 3254, loss 0.0886\n",
      "test epoch: 3254, loss: 0.69\n",
      "train epoch: 3255, loss 0.0886\n",
      "test epoch: 3255, loss: 0.76\n",
      "train epoch: 3256, loss 0.0882\n",
      "test epoch: 3256, loss: 0.71\n",
      "train epoch: 3257, loss 0.0875\n",
      "test epoch: 3257, loss: 0.76\n",
      "train epoch: 3258, loss 0.0870\n",
      "test epoch: 3258, loss: 0.72\n",
      "train epoch: 3259, loss 0.0865\n",
      "test epoch: 3259, loss: 0.74\n",
      "train epoch: 3260, loss 0.0862\n",
      "test epoch: 3260, loss: 0.72\n",
      "train epoch: 3261, loss 0.0860\n",
      "test epoch: 3261, loss: 0.74\n",
      "train epoch: 3262, loss 0.0858\n",
      "test epoch: 3262, loss: 0.73\n",
      "train epoch: 3263, loss 0.0859\n",
      "test epoch: 3263, loss: 0.74\n",
      "train epoch: 3264, loss 0.0860\n",
      "test epoch: 3264, loss: 0.75\n",
      "train epoch: 3265, loss 0.0860\n",
      "test epoch: 3265, loss: 0.72\n",
      "train epoch: 3266, loss 0.0864\n",
      "test epoch: 3266, loss: 0.76\n",
      "train epoch: 3267, loss 0.0869\n",
      "test epoch: 3267, loss: 0.71\n",
      "train epoch: 3268, loss 0.0878\n",
      "test epoch: 3268, loss: 0.78\n",
      "train epoch: 3269, loss 0.0890\n",
      "test epoch: 3269, loss: 0.69\n",
      "train epoch: 3270, loss 0.0912\n",
      "test epoch: 3270, loss: 0.79\n",
      "train epoch: 3271, loss 0.0944\n",
      "test epoch: 3271, loss: 0.64\n",
      "train epoch: 3272, loss 0.0976\n",
      "test epoch: 3272, loss: 0.80\n",
      "train epoch: 3273, loss 0.1002\n",
      "test epoch: 3273, loss: 0.63\n",
      "train epoch: 3274, loss 0.1008\n",
      "test epoch: 3274, loss: 0.79\n",
      "train epoch: 3275, loss 0.0996\n",
      "test epoch: 3275, loss: 0.63\n",
      "train epoch: 3276, loss 0.0962\n",
      "test epoch: 3276, loss: 0.75\n",
      "train epoch: 3277, loss 0.0915\n",
      "test epoch: 3277, loss: 0.67\n",
      "train epoch: 3278, loss 0.0878\n",
      "test epoch: 3278, loss: 0.71\n",
      "train epoch: 3279, loss 0.0860\n",
      "test epoch: 3279, loss: 0.71\n",
      "train epoch: 3280, loss 0.0859\n",
      "test epoch: 3280, loss: 0.67\n",
      "train epoch: 3281, loss 0.0871\n",
      "test epoch: 3281, loss: 0.73\n",
      "train epoch: 3282, loss 0.0891\n",
      "test epoch: 3282, loss: 0.65\n",
      "train epoch: 3283, loss 0.0907\n",
      "test epoch: 3283, loss: 0.75\n",
      "train epoch: 3284, loss 0.0915\n",
      "test epoch: 3284, loss: 0.65\n",
      "train epoch: 3285, loss 0.0909\n",
      "test epoch: 3285, loss: 0.74\n",
      "train epoch: 3286, loss 0.0893\n",
      "test epoch: 3286, loss: 0.67\n",
      "train epoch: 3287, loss 0.0875\n",
      "test epoch: 3287, loss: 0.72\n",
      "train epoch: 3288, loss 0.0862\n",
      "test epoch: 3288, loss: 0.70\n",
      "train epoch: 3289, loss 0.0857\n",
      "test epoch: 3289, loss: 0.70\n",
      "train epoch: 3290, loss 0.0856\n",
      "test epoch: 3290, loss: 0.72\n",
      "train epoch: 3291, loss 0.0858\n",
      "test epoch: 3291, loss: 0.70\n",
      "train epoch: 3292, loss 0.0860\n",
      "test epoch: 3292, loss: 0.72\n",
      "train epoch: 3293, loss 0.0864\n",
      "test epoch: 3293, loss: 0.67\n",
      "train epoch: 3294, loss 0.0869\n",
      "test epoch: 3294, loss: 0.72\n",
      "train epoch: 3295, loss 0.0868\n",
      "test epoch: 3295, loss: 0.69\n",
      "train epoch: 3296, loss 0.0865\n",
      "test epoch: 3296, loss: 0.73\n",
      "train epoch: 3297, loss 0.0863\n",
      "test epoch: 3297, loss: 0.70\n",
      "train epoch: 3298, loss 0.0860\n",
      "test epoch: 3298, loss: 0.72\n",
      "train epoch: 3299, loss 0.0858\n",
      "test epoch: 3299, loss: 0.70\n",
      "train epoch: 3300, loss 0.0858\n",
      "test epoch: 3300, loss: 0.72\n",
      "train epoch: 3301, loss 0.0857\n",
      "test epoch: 3301, loss: 0.71\n",
      "train epoch: 3302, loss 0.0856\n",
      "test epoch: 3302, loss: 0.71\n",
      "train epoch: 3303, loss 0.0855\n",
      "test epoch: 3303, loss: 0.71\n",
      "train epoch: 3304, loss 0.0855\n",
      "test epoch: 3304, loss: 0.70\n",
      "train epoch: 3305, loss 0.0856\n",
      "test epoch: 3305, loss: 0.71\n",
      "train epoch: 3306, loss 0.0855\n",
      "test epoch: 3306, loss: 0.70\n",
      "train epoch: 3307, loss 0.0855\n",
      "test epoch: 3307, loss: 0.72\n",
      "train epoch: 3308, loss 0.0856\n",
      "test epoch: 3308, loss: 0.71\n",
      "train epoch: 3309, loss 0.0854\n",
      "test epoch: 3309, loss: 0.70\n",
      "train epoch: 3310, loss 0.0855\n",
      "test epoch: 3310, loss: 0.72\n",
      "train epoch: 3311, loss 0.0856\n",
      "test epoch: 3311, loss: 0.70\n",
      "train epoch: 3312, loss 0.0860\n",
      "test epoch: 3312, loss: 0.73\n",
      "train epoch: 3313, loss 0.0863\n",
      "test epoch: 3313, loss: 0.68\n",
      "train epoch: 3314, loss 0.0867\n",
      "test epoch: 3314, loss: 0.72\n",
      "train epoch: 3315, loss 0.0874\n",
      "test epoch: 3315, loss: 0.67\n",
      "train epoch: 3316, loss 0.0877\n",
      "test epoch: 3316, loss: 0.74\n",
      "train epoch: 3317, loss 0.0879\n",
      "test epoch: 3317, loss: 0.68\n",
      "train epoch: 3318, loss 0.0884\n",
      "test epoch: 3318, loss: 0.75\n",
      "train epoch: 3319, loss 0.0892\n",
      "test epoch: 3319, loss: 0.66\n",
      "train epoch: 3320, loss 0.0900\n",
      "test epoch: 3320, loss: 0.75\n",
      "train epoch: 3321, loss 0.0907\n",
      "test epoch: 3321, loss: 0.65\n",
      "train epoch: 3322, loss 0.0910\n",
      "test epoch: 3322, loss: 0.76\n",
      "train epoch: 3323, loss 0.0915\n",
      "test epoch: 3323, loss: 0.65\n",
      "train epoch: 3324, loss 0.0918\n",
      "test epoch: 3324, loss: 0.76\n",
      "train epoch: 3325, loss 0.0912\n",
      "test epoch: 3325, loss: 0.66\n",
      "train epoch: 3326, loss 0.0902\n",
      "test epoch: 3326, loss: 0.75\n",
      "train epoch: 3327, loss 0.0895\n",
      "test epoch: 3327, loss: 0.66\n",
      "train epoch: 3328, loss 0.0889\n",
      "test epoch: 3328, loss: 0.74\n",
      "train epoch: 3329, loss 0.0878\n",
      "test epoch: 3329, loss: 0.68\n",
      "train epoch: 3330, loss 0.0875\n",
      "test epoch: 3330, loss: 0.74\n",
      "train epoch: 3331, loss 0.0874\n",
      "test epoch: 3331, loss: 0.67\n",
      "train epoch: 3332, loss 0.0872\n",
      "test epoch: 3332, loss: 0.72\n",
      "train epoch: 3333, loss 0.0872\n",
      "test epoch: 3333, loss: 0.67\n",
      "train epoch: 3334, loss 0.0867\n",
      "test epoch: 3334, loss: 0.72\n",
      "train epoch: 3335, loss 0.0860\n",
      "test epoch: 3335, loss: 0.70\n",
      "train epoch: 3336, loss 0.0857\n",
      "test epoch: 3336, loss: 0.72\n",
      "train epoch: 3337, loss 0.0858\n",
      "test epoch: 3337, loss: 0.71\n",
      "train epoch: 3338, loss 0.0857\n",
      "test epoch: 3338, loss: 0.70\n",
      "train epoch: 3339, loss 0.0854\n",
      "test epoch: 3339, loss: 0.70\n",
      "train epoch: 3340, loss 0.0856\n",
      "test epoch: 3340, loss: 0.67\n",
      "train epoch: 3341, loss 0.0861\n",
      "test epoch: 3341, loss: 0.71\n",
      "train epoch: 3342, loss 0.0860\n",
      "test epoch: 3342, loss: 0.68\n",
      "train epoch: 3343, loss 0.0863\n",
      "test epoch: 3343, loss: 0.74\n",
      "train epoch: 3344, loss 0.0871\n",
      "test epoch: 3344, loss: 0.69\n",
      "train epoch: 3345, loss 0.0881\n",
      "test epoch: 3345, loss: 0.75\n",
      "train epoch: 3346, loss 0.0896\n",
      "test epoch: 3346, loss: 0.63\n",
      "train epoch: 3347, loss 0.0933\n",
      "test epoch: 3347, loss: 0.75\n",
      "train epoch: 3348, loss 0.0967\n",
      "test epoch: 3348, loss: 0.61\n",
      "train epoch: 3349, loss 0.0977\n",
      "test epoch: 3349, loss: 0.77\n",
      "train epoch: 3350, loss 0.0961\n",
      "test epoch: 3350, loss: 0.65\n",
      "train epoch: 3351, loss 0.0950\n",
      "test epoch: 3351, loss: 0.78\n",
      "train epoch: 3352, loss 0.0941\n",
      "test epoch: 3352, loss: 0.65\n",
      "train epoch: 3353, loss 0.0936\n",
      "test epoch: 3353, loss: 0.76\n",
      "train epoch: 3354, loss 0.0926\n",
      "test epoch: 3354, loss: 0.64\n",
      "train epoch: 3355, loss 0.0913\n",
      "test epoch: 3355, loss: 0.74\n",
      "train epoch: 3356, loss 0.0896\n",
      "test epoch: 3356, loss: 0.66\n",
      "train epoch: 3357, loss 0.0875\n",
      "test epoch: 3357, loss: 0.72\n",
      "train epoch: 3358, loss 0.0861\n",
      "test epoch: 3358, loss: 0.69\n",
      "train epoch: 3359, loss 0.0852\n",
      "test epoch: 3359, loss: 0.69\n",
      "train epoch: 3360, loss 0.0849\n",
      "test epoch: 3360, loss: 0.71\n",
      "train epoch: 3361, loss 0.0852\n",
      "test epoch: 3361, loss: 0.68\n",
      "train epoch: 3362, loss 0.0858\n",
      "test epoch: 3362, loss: 0.73\n",
      "train epoch: 3363, loss 0.0868\n",
      "test epoch: 3363, loss: 0.67\n",
      "train epoch: 3364, loss 0.0876\n",
      "test epoch: 3364, loss: 0.74\n",
      "train epoch: 3365, loss 0.0881\n",
      "test epoch: 3365, loss: 0.66\n",
      "train epoch: 3366, loss 0.0881\n",
      "test epoch: 3366, loss: 0.73\n",
      "train epoch: 3367, loss 0.0879\n",
      "test epoch: 3367, loss: 0.67\n",
      "train epoch: 3368, loss 0.0871\n",
      "test epoch: 3368, loss: 0.73\n",
      "train epoch: 3369, loss 0.0864\n",
      "test epoch: 3369, loss: 0.68\n",
      "train epoch: 3370, loss 0.0858\n",
      "test epoch: 3370, loss: 0.71\n",
      "train epoch: 3371, loss 0.0853\n",
      "test epoch: 3371, loss: 0.68\n",
      "train epoch: 3372, loss 0.0851\n",
      "test epoch: 3372, loss: 0.70\n",
      "train epoch: 3373, loss 0.0850\n",
      "test epoch: 3373, loss: 0.70\n",
      "train epoch: 3374, loss 0.0849\n",
      "test epoch: 3374, loss: 0.69\n",
      "train epoch: 3375, loss 0.0850\n",
      "test epoch: 3375, loss: 0.71\n",
      "train epoch: 3376, loss 0.0851\n",
      "test epoch: 3376, loss: 0.68\n",
      "train epoch: 3377, loss 0.0853\n",
      "test epoch: 3377, loss: 0.71\n",
      "train epoch: 3378, loss 0.0853\n",
      "test epoch: 3378, loss: 0.68\n",
      "train epoch: 3379, loss 0.0854\n",
      "test epoch: 3379, loss: 0.72\n",
      "train epoch: 3380, loss 0.0858\n",
      "test epoch: 3380, loss: 0.67\n",
      "train epoch: 3381, loss 0.0861\n",
      "test epoch: 3381, loss: 0.72\n",
      "train epoch: 3382, loss 0.0867\n",
      "test epoch: 3382, loss: 0.66\n",
      "train epoch: 3383, loss 0.0870\n",
      "test epoch: 3383, loss: 0.73\n",
      "train epoch: 3384, loss 0.0874\n",
      "test epoch: 3384, loss: 0.66\n",
      "train epoch: 3385, loss 0.0877\n",
      "test epoch: 3385, loss: 0.73\n",
      "train epoch: 3386, loss 0.0882\n",
      "test epoch: 3386, loss: 0.65\n",
      "train epoch: 3387, loss 0.0885\n",
      "test epoch: 3387, loss: 0.73\n",
      "train epoch: 3388, loss 0.0888\n",
      "test epoch: 3388, loss: 0.65\n",
      "train epoch: 3389, loss 0.0889\n",
      "test epoch: 3389, loss: 0.74\n",
      "train epoch: 3390, loss 0.0890\n",
      "test epoch: 3390, loss: 0.65\n",
      "train epoch: 3391, loss 0.0891\n",
      "test epoch: 3391, loss: 0.73\n",
      "train epoch: 3392, loss 0.0892\n",
      "test epoch: 3392, loss: 0.64\n",
      "train epoch: 3393, loss 0.0893\n",
      "test epoch: 3393, loss: 0.73\n",
      "train epoch: 3394, loss 0.0890\n",
      "test epoch: 3394, loss: 0.65\n",
      "train epoch: 3395, loss 0.0885\n",
      "test epoch: 3395, loss: 0.73\n",
      "train epoch: 3396, loss 0.0878\n",
      "test epoch: 3396, loss: 0.66\n",
      "train epoch: 3397, loss 0.0871\n",
      "test epoch: 3397, loss: 0.72\n",
      "train epoch: 3398, loss 0.0868\n",
      "test epoch: 3398, loss: 0.65\n",
      "train epoch: 3399, loss 0.0867\n",
      "test epoch: 3399, loss: 0.70\n",
      "train epoch: 3400, loss 0.0864\n",
      "test epoch: 3400, loss: 0.66\n",
      "train epoch: 3401, loss 0.0858\n",
      "test epoch: 3401, loss: 0.71\n",
      "train epoch: 3402, loss 0.0856\n",
      "test epoch: 3402, loss: 0.67\n",
      "train epoch: 3403, loss 0.0853\n",
      "test epoch: 3403, loss: 0.70\n",
      "train epoch: 3404, loss 0.0850\n",
      "test epoch: 3404, loss: 0.69\n",
      "train epoch: 3405, loss 0.0851\n",
      "test epoch: 3405, loss: 0.70\n",
      "train epoch: 3406, loss 0.0851\n",
      "test epoch: 3406, loss: 0.67\n",
      "train epoch: 3407, loss 0.0853\n",
      "test epoch: 3407, loss: 0.71\n",
      "train epoch: 3408, loss 0.0854\n",
      "test epoch: 3408, loss: 0.66\n",
      "train epoch: 3409, loss 0.0855\n",
      "test epoch: 3409, loss: 0.70\n",
      "train epoch: 3410, loss 0.0857\n",
      "test epoch: 3410, loss: 0.66\n",
      "train epoch: 3411, loss 0.0859\n",
      "test epoch: 3411, loss: 0.71\n",
      "train epoch: 3412, loss 0.0861\n",
      "test epoch: 3412, loss: 0.66\n",
      "train epoch: 3413, loss 0.0866\n",
      "test epoch: 3413, loss: 0.72\n",
      "train epoch: 3414, loss 0.0874\n",
      "test epoch: 3414, loss: 0.64\n",
      "train epoch: 3415, loss 0.0885\n",
      "test epoch: 3415, loss: 0.73\n",
      "train epoch: 3416, loss 0.0901\n",
      "test epoch: 3416, loss: 0.62\n",
      "train epoch: 3417, loss 0.0913\n",
      "test epoch: 3417, loss: 0.74\n",
      "train epoch: 3418, loss 0.0919\n",
      "test epoch: 3418, loss: 0.63\n",
      "train epoch: 3419, loss 0.0927\n",
      "test epoch: 3419, loss: 0.75\n",
      "train epoch: 3420, loss 0.0936\n",
      "test epoch: 3420, loss: 0.62\n",
      "train epoch: 3421, loss 0.0944\n",
      "test epoch: 3421, loss: 0.74\n",
      "train epoch: 3422, loss 0.0951\n",
      "test epoch: 3422, loss: 0.61\n",
      "train epoch: 3423, loss 0.0940\n",
      "test epoch: 3423, loss: 0.73\n",
      "train epoch: 3424, loss 0.0911\n",
      "test epoch: 3424, loss: 0.65\n",
      "train epoch: 3425, loss 0.0888\n",
      "test epoch: 3425, loss: 0.72\n",
      "train epoch: 3426, loss 0.0870\n",
      "test epoch: 3426, loss: 0.67\n",
      "train epoch: 3427, loss 0.0855\n",
      "test epoch: 3427, loss: 0.69\n",
      "train epoch: 3428, loss 0.0848\n",
      "test epoch: 3428, loss: 0.67\n",
      "train epoch: 3429, loss 0.0848\n",
      "test epoch: 3429, loss: 0.67\n",
      "train epoch: 3430, loss 0.0846\n",
      "test epoch: 3430, loss: 0.69\n",
      "train epoch: 3431, loss 0.0847\n",
      "test epoch: 3431, loss: 0.68\n",
      "train epoch: 3432, loss 0.0849\n",
      "test epoch: 3432, loss: 0.71\n",
      "train epoch: 3433, loss 0.0850\n",
      "test epoch: 3433, loss: 0.67\n",
      "train epoch: 3434, loss 0.0851\n",
      "test epoch: 3434, loss: 0.70\n",
      "train epoch: 3435, loss 0.0853\n",
      "test epoch: 3435, loss: 0.65\n",
      "train epoch: 3436, loss 0.0856\n",
      "test epoch: 3436, loss: 0.70\n",
      "train epoch: 3437, loss 0.0857\n",
      "test epoch: 3437, loss: 0.66\n",
      "train epoch: 3438, loss 0.0858\n",
      "test epoch: 3438, loss: 0.71\n",
      "train epoch: 3439, loss 0.0862\n",
      "test epoch: 3439, loss: 0.65\n",
      "train epoch: 3440, loss 0.0866\n",
      "test epoch: 3440, loss: 0.72\n",
      "train epoch: 3441, loss 0.0870\n",
      "test epoch: 3441, loss: 0.65\n",
      "train epoch: 3442, loss 0.0877\n",
      "test epoch: 3442, loss: 0.72\n",
      "train epoch: 3443, loss 0.0884\n",
      "test epoch: 3443, loss: 0.64\n",
      "train epoch: 3444, loss 0.0890\n",
      "test epoch: 3444, loss: 0.72\n",
      "train epoch: 3445, loss 0.0898\n",
      "test epoch: 3445, loss: 0.62\n",
      "train epoch: 3446, loss 0.0907\n",
      "test epoch: 3446, loss: 0.73\n",
      "train epoch: 3447, loss 0.0910\n",
      "test epoch: 3447, loss: 0.63\n",
      "train epoch: 3448, loss 0.0914\n",
      "test epoch: 3448, loss: 0.74\n",
      "train epoch: 3449, loss 0.0927\n",
      "test epoch: 3449, loss: 0.61\n",
      "train epoch: 3450, loss 0.0940\n",
      "test epoch: 3450, loss: 0.74\n",
      "train epoch: 3451, loss 0.0954\n",
      "test epoch: 3451, loss: 0.60\n",
      "train epoch: 3452, loss 0.0952\n",
      "test epoch: 3452, loss: 0.73\n",
      "train epoch: 3453, loss 0.0933\n",
      "test epoch: 3453, loss: 0.63\n",
      "train epoch: 3454, loss 0.0906\n",
      "test epoch: 3454, loss: 0.71\n",
      "train epoch: 3455, loss 0.0882\n",
      "test epoch: 3455, loss: 0.64\n",
      "train epoch: 3456, loss 0.0860\n",
      "test epoch: 3456, loss: 0.67\n",
      "train epoch: 3457, loss 0.0847\n",
      "test epoch: 3457, loss: 0.67\n",
      "train epoch: 3458, loss 0.0845\n",
      "test epoch: 3458, loss: 0.66\n",
      "train epoch: 3459, loss 0.0845\n",
      "test epoch: 3459, loss: 0.65\n",
      "train epoch: 3460, loss 0.0857\n",
      "test epoch: 3460, loss: 0.77\n",
      "train epoch: 3461, loss 0.0991\n",
      "test epoch: 3461, loss: 0.53\n",
      "train epoch: 3462, loss 0.1553\n",
      "test epoch: 3462, loss: 0.91\n",
      "train epoch: 3463, loss 0.2319\n",
      "test epoch: 3463, loss: 0.37\n",
      "train epoch: 3464, loss 0.2587\n",
      "test epoch: 3464, loss: 0.73\n",
      "train epoch: 3465, loss 0.1269\n",
      "test epoch: 3465, loss: 0.71\n",
      "train epoch: 3466, loss 0.0947\n",
      "test epoch: 3466, loss: 0.52\n",
      "train epoch: 3467, loss 0.1788\n",
      "test epoch: 3467, loss: 0.92\n",
      "train epoch: 3468, loss 0.1479\n",
      "test epoch: 3468, loss: 0.77\n",
      "train epoch: 3469, loss 0.0960\n",
      "test epoch: 3469, loss: 0.60\n",
      "train epoch: 3470, loss 0.1374\n",
      "test epoch: 3470, loss: 0.94\n",
      "train epoch: 3471, loss 0.1180\n",
      "test epoch: 3471, loss: 0.83\n",
      "train epoch: 3472, loss 0.0905\n",
      "test epoch: 3472, loss: 0.66\n",
      "train epoch: 3473, loss 0.1213\n",
      "test epoch: 3473, loss: 0.97\n",
      "train epoch: 3474, loss 0.1258\n",
      "test epoch: 3474, loss: 0.72\n",
      "train epoch: 3475, loss 0.0999\n",
      "test epoch: 3475, loss: 0.77\n",
      "train epoch: 3476, loss 0.0911\n",
      "test epoch: 3476, loss: 0.90\n",
      "train epoch: 3477, loss 0.1041\n",
      "test epoch: 3477, loss: 0.71\n",
      "train epoch: 3478, loss 0.0979\n",
      "test epoch: 3478, loss: 0.77\n",
      "train epoch: 3479, loss 0.0891\n",
      "test epoch: 3479, loss: 0.86\n",
      "train epoch: 3480, loss 0.0980\n",
      "test epoch: 3480, loss: 0.69\n",
      "train epoch: 3481, loss 0.0966\n",
      "test epoch: 3481, loss: 0.74\n",
      "train epoch: 3482, loss 0.0888\n",
      "test epoch: 3482, loss: 0.81\n",
      "train epoch: 3483, loss 0.0962\n",
      "test epoch: 3483, loss: 0.68\n",
      "train epoch: 3484, loss 0.0936\n",
      "test epoch: 3484, loss: 0.71\n",
      "train epoch: 3485, loss 0.0897\n",
      "test epoch: 3485, loss: 0.80\n",
      "train epoch: 3486, loss 0.0953\n",
      "test epoch: 3486, loss: 0.69\n",
      "train epoch: 3487, loss 0.0909\n",
      "test epoch: 3487, loss: 0.70\n",
      "train epoch: 3488, loss 0.0897\n",
      "test epoch: 3488, loss: 0.80\n",
      "train epoch: 3489, loss 0.0933\n",
      "test epoch: 3489, loss: 0.71\n",
      "train epoch: 3490, loss 0.0894\n",
      "test epoch: 3490, loss: 0.72\n",
      "train epoch: 3491, loss 0.0889\n",
      "test epoch: 3491, loss: 0.80\n",
      "train epoch: 3492, loss 0.0918\n",
      "test epoch: 3492, loss: 0.72\n",
      "train epoch: 3493, loss 0.0894\n",
      "test epoch: 3493, loss: 0.74\n",
      "train epoch: 3494, loss 0.0883\n",
      "test epoch: 3494, loss: 0.80\n",
      "train epoch: 3495, loss 0.0904\n",
      "test epoch: 3495, loss: 0.73\n",
      "train epoch: 3496, loss 0.0888\n",
      "test epoch: 3496, loss: 0.74\n",
      "train epoch: 3497, loss 0.0880\n",
      "test epoch: 3497, loss: 0.79\n",
      "train epoch: 3498, loss 0.0896\n",
      "test epoch: 3498, loss: 0.73\n",
      "train epoch: 3499, loss 0.0884\n",
      "test epoch: 3499, loss: 0.74\n",
      "train epoch: 3500, loss 0.0879\n",
      "test epoch: 3500, loss: 0.79\n",
      "train epoch: 3501, loss 0.0890\n",
      "test epoch: 3501, loss: 0.74\n",
      "train epoch: 3502, loss 0.0882\n",
      "test epoch: 3502, loss: 0.75\n",
      "train epoch: 3503, loss 0.0875\n",
      "test epoch: 3503, loss: 0.78\n",
      "train epoch: 3504, loss 0.0882\n",
      "test epoch: 3504, loss: 0.74\n",
      "train epoch: 3505, loss 0.0878\n",
      "test epoch: 3505, loss: 0.75\n",
      "train epoch: 3506, loss 0.0872\n",
      "test epoch: 3506, loss: 0.78\n",
      "train epoch: 3507, loss 0.0877\n",
      "test epoch: 3507, loss: 0.73\n",
      "train epoch: 3508, loss 0.0878\n",
      "test epoch: 3508, loss: 0.76\n",
      "train epoch: 3509, loss 0.0870\n",
      "test epoch: 3509, loss: 0.77\n",
      "train epoch: 3510, loss 0.0874\n",
      "test epoch: 3510, loss: 0.74\n",
      "train epoch: 3511, loss 0.0874\n",
      "test epoch: 3511, loss: 0.76\n",
      "train epoch: 3512, loss 0.0869\n",
      "test epoch: 3512, loss: 0.77\n",
      "train epoch: 3513, loss 0.0872\n",
      "test epoch: 3513, loss: 0.73\n",
      "train epoch: 3514, loss 0.0875\n",
      "test epoch: 3514, loss: 0.76\n",
      "train epoch: 3515, loss 0.0870\n",
      "test epoch: 3515, loss: 0.76\n",
      "train epoch: 3516, loss 0.0868\n",
      "test epoch: 3516, loss: 0.73\n",
      "train epoch: 3517, loss 0.0872\n",
      "test epoch: 3517, loss: 0.76\n",
      "train epoch: 3518, loss 0.0870\n",
      "test epoch: 3518, loss: 0.75\n",
      "train epoch: 3519, loss 0.0867\n",
      "test epoch: 3519, loss: 0.73\n",
      "train epoch: 3520, loss 0.0870\n",
      "test epoch: 3520, loss: 0.76\n",
      "train epoch: 3521, loss 0.0868\n",
      "test epoch: 3521, loss: 0.75\n",
      "train epoch: 3522, loss 0.0866\n",
      "test epoch: 3522, loss: 0.74\n",
      "train epoch: 3523, loss 0.0867\n",
      "test epoch: 3523, loss: 0.76\n",
      "train epoch: 3524, loss 0.0867\n",
      "test epoch: 3524, loss: 0.74\n",
      "train epoch: 3525, loss 0.0865\n",
      "test epoch: 3525, loss: 0.74\n",
      "train epoch: 3526, loss 0.0864\n",
      "test epoch: 3526, loss: 0.75\n",
      "train epoch: 3527, loss 0.0865\n",
      "test epoch: 3527, loss: 0.73\n",
      "train epoch: 3528, loss 0.0865\n",
      "test epoch: 3528, loss: 0.75\n",
      "train epoch: 3529, loss 0.0864\n",
      "test epoch: 3529, loss: 0.75\n",
      "train epoch: 3530, loss 0.0864\n",
      "test epoch: 3530, loss: 0.73\n",
      "train epoch: 3531, loss 0.0864\n",
      "test epoch: 3531, loss: 0.75\n",
      "train epoch: 3532, loss 0.0863\n",
      "test epoch: 3532, loss: 0.74\n",
      "train epoch: 3533, loss 0.0863\n",
      "test epoch: 3533, loss: 0.73\n",
      "train epoch: 3534, loss 0.0863\n",
      "test epoch: 3534, loss: 0.75\n",
      "train epoch: 3535, loss 0.0862\n",
      "test epoch: 3535, loss: 0.74\n",
      "train epoch: 3536, loss 0.0862\n",
      "test epoch: 3536, loss: 0.74\n",
      "train epoch: 3537, loss 0.0862\n",
      "test epoch: 3537, loss: 0.74\n",
      "train epoch: 3538, loss 0.0861\n",
      "test epoch: 3538, loss: 0.74\n",
      "train epoch: 3539, loss 0.0861\n",
      "test epoch: 3539, loss: 0.74\n",
      "train epoch: 3540, loss 0.0861\n",
      "test epoch: 3540, loss: 0.74\n",
      "train epoch: 3541, loss 0.0860\n",
      "test epoch: 3541, loss: 0.74\n",
      "train epoch: 3542, loss 0.0860\n",
      "test epoch: 3542, loss: 0.74\n",
      "train epoch: 3543, loss 0.0860\n",
      "test epoch: 3543, loss: 0.74\n",
      "train epoch: 3544, loss 0.0860\n",
      "test epoch: 3544, loss: 0.74\n",
      "train epoch: 3545, loss 0.0859\n",
      "test epoch: 3545, loss: 0.74\n",
      "train epoch: 3546, loss 0.0859\n",
      "test epoch: 3546, loss: 0.74\n",
      "train epoch: 3547, loss 0.0859\n",
      "test epoch: 3547, loss: 0.74\n",
      "train epoch: 3548, loss 0.0858\n",
      "test epoch: 3548, loss: 0.74\n",
      "train epoch: 3549, loss 0.0858\n",
      "test epoch: 3549, loss: 0.74\n",
      "train epoch: 3550, loss 0.0858\n",
      "test epoch: 3550, loss: 0.74\n",
      "train epoch: 3551, loss 0.0858\n",
      "test epoch: 3551, loss: 0.74\n",
      "train epoch: 3552, loss 0.0857\n",
      "test epoch: 3552, loss: 0.74\n",
      "train epoch: 3553, loss 0.0857\n",
      "test epoch: 3553, loss: 0.74\n",
      "train epoch: 3554, loss 0.0857\n",
      "test epoch: 3554, loss: 0.74\n",
      "train epoch: 3555, loss 0.0857\n",
      "test epoch: 3555, loss: 0.74\n",
      "train epoch: 3556, loss 0.0857\n",
      "test epoch: 3556, loss: 0.74\n",
      "train epoch: 3557, loss 0.0856\n",
      "test epoch: 3557, loss: 0.74\n",
      "train epoch: 3558, loss 0.0856\n",
      "test epoch: 3558, loss: 0.74\n",
      "train epoch: 3559, loss 0.0856\n",
      "test epoch: 3559, loss: 0.74\n",
      "train epoch: 3560, loss 0.0856\n",
      "test epoch: 3560, loss: 0.74\n",
      "train epoch: 3561, loss 0.0855\n",
      "test epoch: 3561, loss: 0.74\n",
      "train epoch: 3562, loss 0.0855\n",
      "test epoch: 3562, loss: 0.74\n",
      "train epoch: 3563, loss 0.0855\n",
      "test epoch: 3563, loss: 0.74\n",
      "train epoch: 3564, loss 0.0855\n",
      "test epoch: 3564, loss: 0.73\n",
      "train epoch: 3565, loss 0.0855\n",
      "test epoch: 3565, loss: 0.74\n",
      "train epoch: 3566, loss 0.0854\n",
      "test epoch: 3566, loss: 0.73\n",
      "train epoch: 3567, loss 0.0854\n",
      "test epoch: 3567, loss: 0.74\n",
      "train epoch: 3568, loss 0.0854\n",
      "test epoch: 3568, loss: 0.74\n",
      "train epoch: 3569, loss 0.0854\n",
      "test epoch: 3569, loss: 0.74\n",
      "train epoch: 3570, loss 0.0853\n",
      "test epoch: 3570, loss: 0.74\n",
      "train epoch: 3571, loss 0.0853\n",
      "test epoch: 3571, loss: 0.74\n",
      "train epoch: 3572, loss 0.0853\n",
      "test epoch: 3572, loss: 0.73\n",
      "train epoch: 3573, loss 0.0853\n",
      "test epoch: 3573, loss: 0.74\n",
      "train epoch: 3574, loss 0.0853\n",
      "test epoch: 3574, loss: 0.73\n",
      "train epoch: 3575, loss 0.0852\n",
      "test epoch: 3575, loss: 0.73\n",
      "train epoch: 3576, loss 0.0852\n",
      "test epoch: 3576, loss: 0.73\n",
      "train epoch: 3577, loss 0.0852\n",
      "test epoch: 3577, loss: 0.73\n",
      "train epoch: 3578, loss 0.0852\n",
      "test epoch: 3578, loss: 0.73\n",
      "train epoch: 3579, loss 0.0851\n",
      "test epoch: 3579, loss: 0.73\n",
      "train epoch: 3580, loss 0.0851\n",
      "test epoch: 3580, loss: 0.73\n",
      "train epoch: 3581, loss 0.0851\n",
      "test epoch: 3581, loss: 0.73\n",
      "train epoch: 3582, loss 0.0851\n",
      "test epoch: 3582, loss: 0.73\n",
      "train epoch: 3583, loss 0.0851\n",
      "test epoch: 3583, loss: 0.73\n",
      "train epoch: 3584, loss 0.0850\n",
      "test epoch: 3584, loss: 0.73\n",
      "train epoch: 3585, loss 0.0850\n",
      "test epoch: 3585, loss: 0.73\n",
      "train epoch: 3586, loss 0.0850\n",
      "test epoch: 3586, loss: 0.73\n",
      "train epoch: 3587, loss 0.0850\n",
      "test epoch: 3587, loss: 0.73\n",
      "train epoch: 3588, loss 0.0850\n",
      "test epoch: 3588, loss: 0.73\n",
      "train epoch: 3589, loss 0.0849\n",
      "test epoch: 3589, loss: 0.73\n",
      "train epoch: 3590, loss 0.0849\n",
      "test epoch: 3590, loss: 0.73\n",
      "train epoch: 3591, loss 0.0849\n",
      "test epoch: 3591, loss: 0.73\n",
      "train epoch: 3592, loss 0.0849\n",
      "test epoch: 3592, loss: 0.73\n",
      "train epoch: 3593, loss 0.0848\n",
      "test epoch: 3593, loss: 0.73\n",
      "train epoch: 3594, loss 0.0848\n",
      "test epoch: 3594, loss: 0.73\n",
      "train epoch: 3595, loss 0.0848\n",
      "test epoch: 3595, loss: 0.73\n",
      "train epoch: 3596, loss 0.0848\n",
      "test epoch: 3596, loss: 0.73\n",
      "train epoch: 3597, loss 0.0847\n",
      "test epoch: 3597, loss: 0.73\n",
      "train epoch: 3598, loss 0.0847\n",
      "test epoch: 3598, loss: 0.73\n",
      "train epoch: 3599, loss 0.0847\n",
      "test epoch: 3599, loss: 0.73\n",
      "train epoch: 3600, loss 0.0847\n",
      "test epoch: 3600, loss: 0.73\n",
      "train epoch: 3601, loss 0.0846\n",
      "test epoch: 3601, loss: 0.73\n",
      "train epoch: 3602, loss 0.0846\n",
      "test epoch: 3602, loss: 0.73\n",
      "train epoch: 3603, loss 0.0846\n",
      "test epoch: 3603, loss: 0.73\n",
      "train epoch: 3604, loss 0.0846\n",
      "test epoch: 3604, loss: 0.73\n",
      "train epoch: 3605, loss 0.0846\n",
      "test epoch: 3605, loss: 0.73\n",
      "train epoch: 3606, loss 0.0845\n",
      "test epoch: 3606, loss: 0.73\n",
      "train epoch: 3607, loss 0.0845\n",
      "test epoch: 3607, loss: 0.73\n",
      "train epoch: 3608, loss 0.0845\n",
      "test epoch: 3608, loss: 0.73\n",
      "train epoch: 3609, loss 0.0845\n",
      "test epoch: 3609, loss: 0.73\n",
      "train epoch: 3610, loss 0.0845\n",
      "test epoch: 3610, loss: 0.73\n",
      "train epoch: 3611, loss 0.0844\n",
      "test epoch: 3611, loss: 0.73\n",
      "train epoch: 3612, loss 0.0844\n",
      "test epoch: 3612, loss: 0.73\n",
      "train epoch: 3613, loss 0.0844\n",
      "test epoch: 3613, loss: 0.72\n",
      "train epoch: 3614, loss 0.0843\n",
      "test epoch: 3614, loss: 0.73\n",
      "train epoch: 3615, loss 0.0842\n",
      "test epoch: 3615, loss: 0.72\n",
      "train epoch: 3616, loss 0.0841\n",
      "test epoch: 3616, loss: 0.72\n",
      "train epoch: 3617, loss 0.0840\n",
      "test epoch: 3617, loss: 0.71\n",
      "train epoch: 3618, loss 0.0840\n",
      "test epoch: 3618, loss: 0.71\n",
      "train epoch: 3619, loss 0.0840\n",
      "test epoch: 3619, loss: 0.72\n",
      "train epoch: 3620, loss 0.0841\n",
      "test epoch: 3620, loss: 0.71\n",
      "train epoch: 3621, loss 0.0840\n",
      "test epoch: 3621, loss: 0.71\n",
      "train epoch: 3622, loss 0.0839\n",
      "test epoch: 3622, loss: 0.72\n",
      "train epoch: 3623, loss 0.0839\n",
      "test epoch: 3623, loss: 0.71\n",
      "train epoch: 3624, loss 0.0839\n",
      "test epoch: 3624, loss: 0.72\n",
      "train epoch: 3625, loss 0.0839\n",
      "test epoch: 3625, loss: 0.72\n",
      "train epoch: 3626, loss 0.0838\n",
      "test epoch: 3626, loss: 0.72\n",
      "train epoch: 3627, loss 0.0838\n",
      "test epoch: 3627, loss: 0.72\n",
      "train epoch: 3628, loss 0.0837\n",
      "test epoch: 3628, loss: 0.71\n",
      "train epoch: 3629, loss 0.0837\n",
      "test epoch: 3629, loss: 0.72\n",
      "train epoch: 3630, loss 0.0837\n",
      "test epoch: 3630, loss: 0.71\n",
      "train epoch: 3631, loss 0.0836\n",
      "test epoch: 3631, loss: 0.71\n",
      "train epoch: 3632, loss 0.0836\n",
      "test epoch: 3632, loss: 0.72\n",
      "train epoch: 3633, loss 0.0836\n",
      "test epoch: 3633, loss: 0.71\n",
      "train epoch: 3634, loss 0.0835\n",
      "test epoch: 3634, loss: 0.71\n",
      "train epoch: 3635, loss 0.0835\n",
      "test epoch: 3635, loss: 0.72\n",
      "train epoch: 3636, loss 0.0835\n",
      "test epoch: 3636, loss: 0.71\n",
      "train epoch: 3637, loss 0.0835\n",
      "test epoch: 3637, loss: 0.72\n",
      "train epoch: 3638, loss 0.0834\n",
      "test epoch: 3638, loss: 0.71\n",
      "train epoch: 3639, loss 0.0834\n",
      "test epoch: 3639, loss: 0.72\n",
      "train epoch: 3640, loss 0.0834\n",
      "test epoch: 3640, loss: 0.71\n",
      "train epoch: 3641, loss 0.0833\n",
      "test epoch: 3641, loss: 0.71\n",
      "train epoch: 3642, loss 0.0833\n",
      "test epoch: 3642, loss: 0.71\n",
      "train epoch: 3643, loss 0.0833\n",
      "test epoch: 3643, loss: 0.71\n",
      "train epoch: 3644, loss 0.0832\n",
      "test epoch: 3644, loss: 0.71\n",
      "train epoch: 3645, loss 0.0832\n",
      "test epoch: 3645, loss: 0.72\n",
      "train epoch: 3646, loss 0.0833\n",
      "test epoch: 3646, loss: 0.70\n",
      "train epoch: 3647, loss 0.0833\n",
      "test epoch: 3647, loss: 0.72\n",
      "train epoch: 3648, loss 0.0833\n",
      "test epoch: 3648, loss: 0.70\n",
      "train epoch: 3649, loss 0.0832\n",
      "test epoch: 3649, loss: 0.72\n",
      "train epoch: 3650, loss 0.0831\n",
      "test epoch: 3650, loss: 0.71\n",
      "train epoch: 3651, loss 0.0830\n",
      "test epoch: 3651, loss: 0.71\n",
      "train epoch: 3652, loss 0.0830\n",
      "test epoch: 3652, loss: 0.71\n",
      "train epoch: 3653, loss 0.0830\n",
      "test epoch: 3653, loss: 0.70\n",
      "train epoch: 3654, loss 0.0830\n",
      "test epoch: 3654, loss: 0.72\n",
      "train epoch: 3655, loss 0.0832\n",
      "test epoch: 3655, loss: 0.69\n",
      "train epoch: 3656, loss 0.0833\n",
      "test epoch: 3656, loss: 0.72\n",
      "train epoch: 3657, loss 0.0834\n",
      "test epoch: 3657, loss: 0.69\n",
      "train epoch: 3658, loss 0.0833\n",
      "test epoch: 3658, loss: 0.72\n",
      "train epoch: 3659, loss 0.0832\n",
      "test epoch: 3659, loss: 0.69\n",
      "train epoch: 3660, loss 0.0831\n",
      "test epoch: 3660, loss: 0.72\n",
      "train epoch: 3661, loss 0.0830\n",
      "test epoch: 3661, loss: 0.69\n",
      "train epoch: 3662, loss 0.0829\n",
      "test epoch: 3662, loss: 0.71\n",
      "train epoch: 3663, loss 0.0828\n",
      "test epoch: 3663, loss: 0.70\n",
      "train epoch: 3664, loss 0.0827\n",
      "test epoch: 3664, loss: 0.71\n",
      "train epoch: 3665, loss 0.0827\n",
      "test epoch: 3665, loss: 0.71\n",
      "train epoch: 3666, loss 0.0827\n",
      "test epoch: 3666, loss: 0.71\n",
      "train epoch: 3667, loss 0.0826\n",
      "test epoch: 3667, loss: 0.71\n",
      "train epoch: 3668, loss 0.0826\n",
      "test epoch: 3668, loss: 0.70\n",
      "train epoch: 3669, loss 0.0826\n",
      "test epoch: 3669, loss: 0.71\n",
      "train epoch: 3670, loss 0.0826\n",
      "test epoch: 3670, loss: 0.69\n",
      "train epoch: 3671, loss 0.0826\n",
      "test epoch: 3671, loss: 0.71\n",
      "train epoch: 3672, loss 0.0826\n",
      "test epoch: 3672, loss: 0.69\n",
      "train epoch: 3673, loss 0.0825\n",
      "test epoch: 3673, loss: 0.71\n",
      "train epoch: 3674, loss 0.0826\n",
      "test epoch: 3674, loss: 0.69\n",
      "train epoch: 3675, loss 0.0826\n",
      "test epoch: 3675, loss: 0.72\n",
      "train epoch: 3676, loss 0.0827\n",
      "test epoch: 3676, loss: 0.69\n",
      "train epoch: 3677, loss 0.0827\n",
      "test epoch: 3677, loss: 0.72\n",
      "train epoch: 3678, loss 0.0829\n",
      "test epoch: 3678, loss: 0.68\n",
      "train epoch: 3679, loss 0.0830\n",
      "test epoch: 3679, loss: 0.72\n",
      "train epoch: 3680, loss 0.0831\n",
      "test epoch: 3680, loss: 0.67\n",
      "train epoch: 3681, loss 0.0836\n",
      "test epoch: 3681, loss: 0.73\n",
      "train epoch: 3682, loss 0.0840\n",
      "test epoch: 3682, loss: 0.65\n",
      "train epoch: 3683, loss 0.0851\n",
      "test epoch: 3683, loss: 0.75\n",
      "train epoch: 3684, loss 0.0862\n",
      "test epoch: 3684, loss: 0.63\n",
      "train epoch: 3685, loss 0.0883\n",
      "test epoch: 3685, loss: 0.77\n",
      "train epoch: 3686, loss 0.0908\n",
      "test epoch: 3686, loss: 0.61\n",
      "train epoch: 3687, loss 0.0949\n",
      "test epoch: 3687, loss: 0.80\n",
      "train epoch: 3688, loss 0.1001\n",
      "test epoch: 3688, loss: 0.58\n",
      "train epoch: 3689, loss 0.1058\n",
      "test epoch: 3689, loss: 0.83\n",
      "train epoch: 3690, loss 0.1098\n",
      "test epoch: 3690, loss: 0.57\n",
      "train epoch: 3691, loss 0.1116\n",
      "test epoch: 3691, loss: 0.83\n",
      "train epoch: 3692, loss 0.1074\n",
      "test epoch: 3692, loss: 0.59\n",
      "train epoch: 3693, loss 0.0995\n",
      "test epoch: 3693, loss: 0.76\n",
      "train epoch: 3694, loss 0.0898\n",
      "test epoch: 3694, loss: 0.66\n",
      "train epoch: 3695, loss 0.0833\n",
      "test epoch: 3695, loss: 0.68\n",
      "train epoch: 3696, loss 0.0825\n",
      "test epoch: 3696, loss: 0.75\n",
      "train epoch: 3697, loss 0.0862\n",
      "test epoch: 3697, loss: 0.62\n",
      "train epoch: 3698, loss 0.0911\n",
      "test epoch: 3698, loss: 0.78\n",
      "train epoch: 3699, loss 0.0937\n",
      "test epoch: 3699, loss: 0.62\n",
      "train epoch: 3700, loss 0.0917\n",
      "test epoch: 3700, loss: 0.75\n",
      "train epoch: 3701, loss 0.0865\n",
      "test epoch: 3701, loss: 0.67\n",
      "train epoch: 3702, loss 0.0826\n",
      "test epoch: 3702, loss: 0.68\n",
      "train epoch: 3703, loss 0.0823\n",
      "test epoch: 3703, loss: 0.73\n",
      "train epoch: 3704, loss 0.0849\n",
      "test epoch: 3704, loss: 0.63\n",
      "train epoch: 3705, loss 0.0875\n",
      "test epoch: 3705, loss: 0.75\n",
      "train epoch: 3706, loss 0.0874\n",
      "test epoch: 3706, loss: 0.65\n",
      "train epoch: 3707, loss 0.0852\n",
      "test epoch: 3707, loss: 0.71\n",
      "train epoch: 3708, loss 0.0826\n",
      "test epoch: 3708, loss: 0.70\n",
      "train epoch: 3709, loss 0.0820\n",
      "test epoch: 3709, loss: 0.66\n",
      "train epoch: 3710, loss 0.0834\n",
      "test epoch: 3710, loss: 0.74\n",
      "train epoch: 3711, loss 0.0849\n",
      "test epoch: 3711, loss: 0.65\n",
      "train epoch: 3712, loss 0.0852\n",
      "test epoch: 3712, loss: 0.74\n",
      "train epoch: 3713, loss 0.0841\n",
      "test epoch: 3713, loss: 0.68\n",
      "train epoch: 3714, loss 0.0825\n",
      "test epoch: 3714, loss: 0.70\n",
      "train epoch: 3715, loss 0.0818\n",
      "test epoch: 3715, loss: 0.71\n",
      "train epoch: 3716, loss 0.0820\n",
      "test epoch: 3716, loss: 0.67\n",
      "train epoch: 3717, loss 0.0826\n",
      "test epoch: 3717, loss: 0.72\n",
      "train epoch: 3718, loss 0.0828\n",
      "test epoch: 3718, loss: 0.67\n",
      "train epoch: 3719, loss 0.0827\n",
      "test epoch: 3719, loss: 0.72\n",
      "train epoch: 3720, loss 0.0822\n",
      "test epoch: 3720, loss: 0.69\n",
      "train epoch: 3721, loss 0.0817\n",
      "test epoch: 3721, loss: 0.69\n",
      "train epoch: 3722, loss 0.0817\n",
      "test epoch: 3722, loss: 0.72\n",
      "train epoch: 3723, loss 0.0819\n",
      "test epoch: 3723, loss: 0.68\n",
      "train epoch: 3724, loss 0.0824\n",
      "test epoch: 3724, loss: 0.73\n",
      "train epoch: 3725, loss 0.0826\n",
      "test epoch: 3725, loss: 0.68\n",
      "train epoch: 3726, loss 0.0825\n",
      "test epoch: 3726, loss: 0.72\n",
      "train epoch: 3727, loss 0.0821\n",
      "test epoch: 3727, loss: 0.69\n",
      "train epoch: 3728, loss 0.0816\n",
      "test epoch: 3728, loss: 0.70\n",
      "train epoch: 3729, loss 0.0815\n",
      "test epoch: 3729, loss: 0.71\n",
      "train epoch: 3730, loss 0.0816\n",
      "test epoch: 3730, loss: 0.68\n",
      "train epoch: 3731, loss 0.0819\n",
      "test epoch: 3731, loss: 0.72\n",
      "train epoch: 3732, loss 0.0820\n",
      "test epoch: 3732, loss: 0.68\n",
      "train epoch: 3733, loss 0.0821\n",
      "test epoch: 3733, loss: 0.72\n",
      "train epoch: 3734, loss 0.0819\n",
      "test epoch: 3734, loss: 0.68\n",
      "train epoch: 3735, loss 0.0818\n",
      "test epoch: 3735, loss: 0.70\n",
      "train epoch: 3736, loss 0.0815\n",
      "test epoch: 3736, loss: 0.69\n",
      "train epoch: 3737, loss 0.0814\n",
      "test epoch: 3737, loss: 0.69\n",
      "train epoch: 3738, loss 0.0813\n",
      "test epoch: 3738, loss: 0.70\n",
      "train epoch: 3739, loss 0.0814\n",
      "test epoch: 3739, loss: 0.68\n",
      "train epoch: 3740, loss 0.0814\n",
      "test epoch: 3740, loss: 0.71\n",
      "train epoch: 3741, loss 0.0815\n",
      "test epoch: 3741, loss: 0.68\n",
      "train epoch: 3742, loss 0.0816\n",
      "test epoch: 3742, loss: 0.71\n",
      "train epoch: 3743, loss 0.0817\n",
      "test epoch: 3743, loss: 0.67\n",
      "train epoch: 3744, loss 0.0817\n",
      "test epoch: 3744, loss: 0.71\n",
      "train epoch: 3745, loss 0.0816\n",
      "test epoch: 3745, loss: 0.68\n",
      "train epoch: 3746, loss 0.0814\n",
      "test epoch: 3746, loss: 0.70\n",
      "train epoch: 3747, loss 0.0813\n",
      "test epoch: 3747, loss: 0.69\n",
      "train epoch: 3748, loss 0.0811\n",
      "test epoch: 3748, loss: 0.69\n",
      "train epoch: 3749, loss 0.0810\n",
      "test epoch: 3749, loss: 0.70\n",
      "train epoch: 3750, loss 0.0811\n",
      "test epoch: 3750, loss: 0.69\n",
      "train epoch: 3751, loss 0.0811\n",
      "test epoch: 3751, loss: 0.70\n",
      "train epoch: 3752, loss 0.0812\n",
      "test epoch: 3752, loss: 0.68\n",
      "train epoch: 3753, loss 0.0813\n",
      "test epoch: 3753, loss: 0.71\n",
      "train epoch: 3754, loss 0.0815\n",
      "test epoch: 3754, loss: 0.67\n",
      "train epoch: 3755, loss 0.0816\n",
      "test epoch: 3755, loss: 0.71\n",
      "train epoch: 3756, loss 0.0816\n",
      "test epoch: 3756, loss: 0.67\n",
      "train epoch: 3757, loss 0.0815\n",
      "test epoch: 3757, loss: 0.71\n",
      "train epoch: 3758, loss 0.0814\n",
      "test epoch: 3758, loss: 0.67\n",
      "train epoch: 3759, loss 0.0812\n",
      "test epoch: 3759, loss: 0.70\n",
      "train epoch: 3760, loss 0.0810\n",
      "test epoch: 3760, loss: 0.68\n",
      "train epoch: 3761, loss 0.0809\n",
      "test epoch: 3761, loss: 0.69\n",
      "train epoch: 3762, loss 0.0809\n",
      "test epoch: 3762, loss: 0.68\n",
      "train epoch: 3763, loss 0.0808\n",
      "test epoch: 3763, loss: 0.69\n",
      "train epoch: 3764, loss 0.0807\n",
      "test epoch: 3764, loss: 0.69\n",
      "train epoch: 3765, loss 0.0807\n",
      "test epoch: 3765, loss: 0.68\n",
      "train epoch: 3766, loss 0.0807\n",
      "test epoch: 3766, loss: 0.69\n",
      "train epoch: 3767, loss 0.0807\n",
      "test epoch: 3767, loss: 0.68\n",
      "train epoch: 3768, loss 0.0807\n",
      "test epoch: 3768, loss: 0.69\n",
      "train epoch: 3769, loss 0.0807\n",
      "test epoch: 3769, loss: 0.67\n",
      "train epoch: 3770, loss 0.0808\n",
      "test epoch: 3770, loss: 0.70\n",
      "train epoch: 3771, loss 0.0808\n",
      "test epoch: 3771, loss: 0.67\n",
      "train epoch: 3772, loss 0.0810\n",
      "test epoch: 3772, loss: 0.70\n",
      "train epoch: 3773, loss 0.0812\n",
      "test epoch: 3773, loss: 0.66\n",
      "train epoch: 3774, loss 0.0815\n",
      "test epoch: 3774, loss: 0.71\n",
      "train epoch: 3775, loss 0.0819\n",
      "test epoch: 3775, loss: 0.65\n",
      "train epoch: 3776, loss 0.0830\n",
      "test epoch: 3776, loss: 0.73\n",
      "train epoch: 3777, loss 0.0847\n",
      "test epoch: 3777, loss: 0.62\n",
      "train epoch: 3778, loss 0.0873\n",
      "test epoch: 3778, loss: 0.77\n",
      "train epoch: 3779, loss 0.0925\n",
      "test epoch: 3779, loss: 0.58\n",
      "train epoch: 3780, loss 0.1019\n",
      "test epoch: 3780, loss: 0.84\n",
      "train epoch: 3781, loss 0.1168\n",
      "test epoch: 3781, loss: 0.51\n",
      "train epoch: 3782, loss 0.1404\n",
      "test epoch: 3782, loss: 0.91\n",
      "train epoch: 3783, loss 0.1592\n",
      "test epoch: 3783, loss: 0.47\n",
      "train epoch: 3784, loss 0.1680\n",
      "test epoch: 3784, loss: 0.85\n",
      "train epoch: 3785, loss 0.1298\n",
      "test epoch: 3785, loss: 0.59\n",
      "train epoch: 3786, loss 0.0910\n",
      "test epoch: 3786, loss: 0.61\n",
      "train epoch: 3787, loss 0.0871\n",
      "test epoch: 3787, loss: 0.82\n",
      "train epoch: 3788, loss 0.1151\n",
      "test epoch: 3788, loss: 0.53\n",
      "train epoch: 3789, loss 0.1298\n",
      "test epoch: 3789, loss: 0.83\n",
      "train epoch: 3790, loss 0.1077\n",
      "test epoch: 3790, loss: 0.66\n",
      "train epoch: 3791, loss 0.0835\n",
      "test epoch: 3791, loss: 0.65\n",
      "train epoch: 3792, loss 0.0870\n",
      "test epoch: 3792, loss: 0.83\n",
      "train epoch: 3793, loss 0.1048\n",
      "test epoch: 3793, loss: 0.58\n",
      "train epoch: 3794, loss 0.1060\n",
      "test epoch: 3794, loss: 0.76\n",
      "train epoch: 3795, loss 0.0874\n",
      "test epoch: 3795, loss: 0.71\n",
      "train epoch: 3796, loss 0.0813\n",
      "test epoch: 3796, loss: 0.60\n",
      "train epoch: 3797, loss 0.0932\n",
      "test epoch: 3797, loss: 0.78\n",
      "train epoch: 3798, loss 0.0961\n",
      "test epoch: 3798, loss: 0.64\n",
      "train epoch: 3799, loss 0.0847\n",
      "test epoch: 3799, loss: 0.67\n",
      "train epoch: 3800, loss 0.0813\n",
      "test epoch: 3800, loss: 0.77\n",
      "train epoch: 3801, loss 0.0889\n",
      "test epoch: 3801, loss: 0.63\n",
      "train epoch: 3802, loss 0.0898\n",
      "test epoch: 3802, loss: 0.73\n",
      "train epoch: 3803, loss 0.0824\n",
      "test epoch: 3803, loss: 0.73\n",
      "train epoch: 3804, loss 0.0817\n",
      "test epoch: 3804, loss: 0.64\n",
      "train epoch: 3805, loss 0.0869\n",
      "test epoch: 3805, loss: 0.76\n",
      "train epoch: 3806, loss 0.0860\n",
      "test epoch: 3806, loss: 0.68\n",
      "train epoch: 3807, loss 0.0811\n",
      "test epoch: 3807, loss: 0.67\n",
      "train epoch: 3808, loss 0.0816\n",
      "test epoch: 3808, loss: 0.74\n",
      "train epoch: 3809, loss 0.0847\n",
      "test epoch: 3809, loss: 0.65\n",
      "train epoch: 3810, loss 0.0831\n",
      "test epoch: 3810, loss: 0.70\n",
      "train epoch: 3811, loss 0.0805\n",
      "test epoch: 3811, loss: 0.72\n",
      "train epoch: 3812, loss 0.0815\n",
      "test epoch: 3812, loss: 0.66\n",
      "train epoch: 3813, loss 0.0832\n",
      "test epoch: 3813, loss: 0.73\n",
      "train epoch: 3814, loss 0.0819\n",
      "test epoch: 3814, loss: 0.70\n",
      "train epoch: 3815, loss 0.0805\n",
      "test epoch: 3815, loss: 0.67\n",
      "train epoch: 3816, loss 0.0814\n",
      "test epoch: 3816, loss: 0.73\n",
      "train epoch: 3817, loss 0.0823\n",
      "test epoch: 3817, loss: 0.67\n",
      "train epoch: 3818, loss 0.0812\n",
      "test epoch: 3818, loss: 0.69\n",
      "train epoch: 3819, loss 0.0803\n",
      "test epoch: 3819, loss: 0.71\n",
      "train epoch: 3820, loss 0.0812\n",
      "test epoch: 3820, loss: 0.67\n",
      "train epoch: 3821, loss 0.0815\n",
      "test epoch: 3821, loss: 0.71\n",
      "train epoch: 3822, loss 0.0805\n",
      "test epoch: 3822, loss: 0.70\n",
      "train epoch: 3823, loss 0.0802\n",
      "test epoch: 3823, loss: 0.68\n",
      "train epoch: 3824, loss 0.0809\n",
      "test epoch: 3824, loss: 0.72\n",
      "train epoch: 3825, loss 0.0808\n",
      "test epoch: 3825, loss: 0.69\n",
      "train epoch: 3826, loss 0.0802\n",
      "test epoch: 3826, loss: 0.70\n",
      "train epoch: 3827, loss 0.0801\n",
      "test epoch: 3827, loss: 0.71\n",
      "train epoch: 3828, loss 0.0804\n",
      "test epoch: 3828, loss: 0.68\n",
      "train epoch: 3829, loss 0.0805\n",
      "test epoch: 3829, loss: 0.70\n",
      "train epoch: 3830, loss 0.0802\n",
      "test epoch: 3830, loss: 0.70\n",
      "train epoch: 3831, loss 0.0800\n",
      "test epoch: 3831, loss: 0.68\n",
      "train epoch: 3832, loss 0.0802\n",
      "test epoch: 3832, loss: 0.71\n",
      "train epoch: 3833, loss 0.0802\n",
      "test epoch: 3833, loss: 0.69\n",
      "train epoch: 3834, loss 0.0800\n",
      "test epoch: 3834, loss: 0.69\n",
      "train epoch: 3835, loss 0.0800\n",
      "test epoch: 3835, loss: 0.71\n",
      "train epoch: 3836, loss 0.0801\n",
      "test epoch: 3836, loss: 0.69\n",
      "train epoch: 3837, loss 0.0802\n",
      "test epoch: 3837, loss: 0.71\n",
      "train epoch: 3838, loss 0.0800\n",
      "test epoch: 3838, loss: 0.70\n",
      "train epoch: 3839, loss 0.0798\n",
      "test epoch: 3839, loss: 0.69\n",
      "train epoch: 3840, loss 0.0799\n",
      "test epoch: 3840, loss: 0.71\n",
      "train epoch: 3841, loss 0.0800\n",
      "test epoch: 3841, loss: 0.69\n",
      "train epoch: 3842, loss 0.0798\n",
      "test epoch: 3842, loss: 0.69\n",
      "train epoch: 3843, loss 0.0798\n",
      "test epoch: 3843, loss: 0.70\n",
      "train epoch: 3844, loss 0.0798\n",
      "test epoch: 3844, loss: 0.69\n",
      "train epoch: 3845, loss 0.0798\n",
      "test epoch: 3845, loss: 0.70\n",
      "train epoch: 3846, loss 0.0797\n",
      "test epoch: 3846, loss: 0.69\n",
      "train epoch: 3847, loss 0.0796\n",
      "test epoch: 3847, loss: 0.69\n",
      "train epoch: 3848, loss 0.0797\n",
      "test epoch: 3848, loss: 0.70\n",
      "train epoch: 3849, loss 0.0797\n",
      "test epoch: 3849, loss: 0.69\n",
      "train epoch: 3850, loss 0.0797\n",
      "test epoch: 3850, loss: 0.69\n",
      "train epoch: 3851, loss 0.0796\n",
      "test epoch: 3851, loss: 0.70\n",
      "train epoch: 3852, loss 0.0796\n",
      "test epoch: 3852, loss: 0.69\n",
      "train epoch: 3853, loss 0.0796\n",
      "test epoch: 3853, loss: 0.70\n",
      "train epoch: 3854, loss 0.0795\n",
      "test epoch: 3854, loss: 0.69\n",
      "train epoch: 3855, loss 0.0795\n",
      "test epoch: 3855, loss: 0.69\n",
      "train epoch: 3856, loss 0.0795\n",
      "test epoch: 3856, loss: 0.69\n",
      "train epoch: 3857, loss 0.0795\n",
      "test epoch: 3857, loss: 0.68\n",
      "train epoch: 3858, loss 0.0795\n",
      "test epoch: 3858, loss: 0.69\n",
      "train epoch: 3859, loss 0.0794\n",
      "test epoch: 3859, loss: 0.69\n",
      "train epoch: 3860, loss 0.0794\n",
      "test epoch: 3860, loss: 0.69\n",
      "train epoch: 3861, loss 0.0794\n",
      "test epoch: 3861, loss: 0.69\n",
      "train epoch: 3862, loss 0.0794\n",
      "test epoch: 3862, loss: 0.69\n",
      "train epoch: 3863, loss 0.0793\n",
      "test epoch: 3863, loss: 0.69\n",
      "train epoch: 3864, loss 0.0793\n",
      "test epoch: 3864, loss: 0.69\n",
      "train epoch: 3865, loss 0.0793\n",
      "test epoch: 3865, loss: 0.69\n",
      "train epoch: 3866, loss 0.0792\n",
      "test epoch: 3866, loss: 0.69\n",
      "train epoch: 3867, loss 0.0792\n",
      "test epoch: 3867, loss: 0.69\n",
      "train epoch: 3868, loss 0.0792\n",
      "test epoch: 3868, loss: 0.69\n",
      "train epoch: 3869, loss 0.0792\n",
      "test epoch: 3869, loss: 0.69\n",
      "train epoch: 3870, loss 0.0792\n",
      "test epoch: 3870, loss: 0.69\n",
      "train epoch: 3871, loss 0.0792\n",
      "test epoch: 3871, loss: 0.69\n",
      "train epoch: 3872, loss 0.0791\n",
      "test epoch: 3872, loss: 0.68\n",
      "train epoch: 3873, loss 0.0791\n",
      "test epoch: 3873, loss: 0.69\n",
      "train epoch: 3874, loss 0.0791\n",
      "test epoch: 3874, loss: 0.68\n",
      "train epoch: 3875, loss 0.0791\n",
      "test epoch: 3875, loss: 0.68\n",
      "train epoch: 3876, loss 0.0790\n",
      "test epoch: 3876, loss: 0.68\n",
      "train epoch: 3877, loss 0.0790\n",
      "test epoch: 3877, loss: 0.68\n",
      "train epoch: 3878, loss 0.0790\n",
      "test epoch: 3878, loss: 0.68\n",
      "train epoch: 3879, loss 0.0790\n",
      "test epoch: 3879, loss: 0.68\n",
      "train epoch: 3880, loss 0.0790\n",
      "test epoch: 3880, loss: 0.68\n",
      "train epoch: 3881, loss 0.0790\n",
      "test epoch: 3881, loss: 0.68\n",
      "train epoch: 3882, loss 0.0789\n",
      "test epoch: 3882, loss: 0.68\n",
      "train epoch: 3883, loss 0.0789\n",
      "test epoch: 3883, loss: 0.68\n",
      "train epoch: 3884, loss 0.0789\n",
      "test epoch: 3884, loss: 0.68\n",
      "train epoch: 3885, loss 0.0789\n",
      "test epoch: 3885, loss: 0.68\n",
      "train epoch: 3886, loss 0.0789\n",
      "test epoch: 3886, loss: 0.68\n",
      "train epoch: 3887, loss 0.0789\n",
      "test epoch: 3887, loss: 0.68\n",
      "train epoch: 3888, loss 0.0789\n",
      "test epoch: 3888, loss: 0.68\n",
      "train epoch: 3889, loss 0.0788\n",
      "test epoch: 3889, loss: 0.68\n",
      "train epoch: 3890, loss 0.0788\n",
      "test epoch: 3890, loss: 0.68\n",
      "train epoch: 3891, loss 0.0788\n",
      "test epoch: 3891, loss: 0.68\n",
      "train epoch: 3892, loss 0.0788\n",
      "test epoch: 3892, loss: 0.67\n",
      "train epoch: 3893, loss 0.0788\n",
      "test epoch: 3893, loss: 0.68\n",
      "train epoch: 3894, loss 0.0787\n",
      "test epoch: 3894, loss: 0.67\n",
      "train epoch: 3895, loss 0.0787\n",
      "test epoch: 3895, loss: 0.68\n",
      "train epoch: 3896, loss 0.0787\n",
      "test epoch: 3896, loss: 0.67\n",
      "train epoch: 3897, loss 0.0787\n",
      "test epoch: 3897, loss: 0.67\n",
      "train epoch: 3898, loss 0.0786\n",
      "test epoch: 3898, loss: 0.68\n",
      "train epoch: 3899, loss 0.0786\n",
      "test epoch: 3899, loss: 0.67\n",
      "train epoch: 3900, loss 0.0787\n",
      "test epoch: 3900, loss: 0.68\n",
      "train epoch: 3901, loss 0.0786\n",
      "test epoch: 3901, loss: 0.67\n",
      "train epoch: 3902, loss 0.0786\n",
      "test epoch: 3902, loss: 0.67\n",
      "train epoch: 3903, loss 0.0785\n",
      "test epoch: 3903, loss: 0.67\n",
      "train epoch: 3904, loss 0.0785\n",
      "test epoch: 3904, loss: 0.67\n",
      "train epoch: 3905, loss 0.0786\n",
      "test epoch: 3905, loss: 0.67\n",
      "train epoch: 3906, loss 0.0786\n",
      "test epoch: 3906, loss: 0.67\n",
      "train epoch: 3907, loss 0.0784\n",
      "test epoch: 3907, loss: 0.67\n",
      "train epoch: 3908, loss 0.0785\n",
      "test epoch: 3908, loss: 0.67\n",
      "train epoch: 3909, loss 0.0785\n",
      "test epoch: 3909, loss: 0.67\n",
      "train epoch: 3910, loss 0.0785\n",
      "test epoch: 3910, loss: 0.67\n",
      "train epoch: 3911, loss 0.0784\n",
      "test epoch: 3911, loss: 0.67\n",
      "train epoch: 3912, loss 0.0783\n",
      "test epoch: 3912, loss: 0.67\n",
      "train epoch: 3913, loss 0.0783\n",
      "test epoch: 3913, loss: 0.67\n",
      "train epoch: 3914, loss 0.0784\n",
      "test epoch: 3914, loss: 0.67\n",
      "train epoch: 3915, loss 0.0784\n",
      "test epoch: 3915, loss: 0.67\n",
      "train epoch: 3916, loss 0.0783\n",
      "test epoch: 3916, loss: 0.67\n",
      "train epoch: 3917, loss 0.0782\n",
      "test epoch: 3917, loss: 0.67\n",
      "train epoch: 3918, loss 0.0783\n",
      "test epoch: 3918, loss: 0.67\n",
      "train epoch: 3919, loss 0.0782\n",
      "test epoch: 3919, loss: 0.67\n",
      "train epoch: 3920, loss 0.0782\n",
      "test epoch: 3920, loss: 0.67\n",
      "train epoch: 3921, loss 0.0781\n",
      "test epoch: 3921, loss: 0.67\n",
      "train epoch: 3922, loss 0.0781\n",
      "test epoch: 3922, loss: 0.67\n",
      "train epoch: 3923, loss 0.0781\n",
      "test epoch: 3923, loss: 0.67\n",
      "train epoch: 3924, loss 0.0781\n",
      "test epoch: 3924, loss: 0.66\n",
      "train epoch: 3925, loss 0.0780\n",
      "test epoch: 3925, loss: 0.67\n",
      "train epoch: 3926, loss 0.0780\n",
      "test epoch: 3926, loss: 0.66\n",
      "train epoch: 3927, loss 0.0780\n",
      "test epoch: 3927, loss: 0.67\n",
      "train epoch: 3928, loss 0.0780\n",
      "test epoch: 3928, loss: 0.66\n",
      "train epoch: 3929, loss 0.0780\n",
      "test epoch: 3929, loss: 0.67\n",
      "train epoch: 3930, loss 0.0780\n",
      "test epoch: 3930, loss: 0.66\n",
      "train epoch: 3931, loss 0.0779\n",
      "test epoch: 3931, loss: 0.67\n",
      "train epoch: 3932, loss 0.0779\n",
      "test epoch: 3932, loss: 0.66\n",
      "train epoch: 3933, loss 0.0779\n",
      "test epoch: 3933, loss: 0.67\n",
      "train epoch: 3934, loss 0.0780\n",
      "test epoch: 3934, loss: 0.65\n",
      "train epoch: 3935, loss 0.0780\n",
      "test epoch: 3935, loss: 0.67\n",
      "train epoch: 3936, loss 0.0780\n",
      "test epoch: 3936, loss: 0.65\n",
      "train epoch: 3937, loss 0.0780\n",
      "test epoch: 3937, loss: 0.67\n",
      "train epoch: 3938, loss 0.0780\n",
      "test epoch: 3938, loss: 0.65\n",
      "train epoch: 3939, loss 0.0781\n",
      "test epoch: 3939, loss: 0.68\n",
      "train epoch: 3940, loss 0.0781\n",
      "test epoch: 3940, loss: 0.64\n",
      "train epoch: 3941, loss 0.0782\n",
      "test epoch: 3941, loss: 0.69\n",
      "train epoch: 3942, loss 0.0788\n",
      "test epoch: 3942, loss: 0.63\n",
      "train epoch: 3943, loss 0.0798\n",
      "test epoch: 3943, loss: 0.71\n",
      "train epoch: 3944, loss 0.0812\n",
      "test epoch: 3944, loss: 0.61\n",
      "train epoch: 3945, loss 0.0837\n",
      "test epoch: 3945, loss: 0.74\n",
      "train epoch: 3946, loss 0.0890\n",
      "test epoch: 3946, loss: 0.56\n",
      "train epoch: 3947, loss 0.0982\n",
      "test epoch: 3947, loss: 0.79\n",
      "train epoch: 3948, loss 0.1109\n",
      "test epoch: 3948, loss: 0.51\n",
      "train epoch: 3949, loss 0.1295\n",
      "test epoch: 3949, loss: 0.85\n",
      "train epoch: 3950, loss 0.1407\n",
      "test epoch: 3950, loss: 0.48\n",
      "train epoch: 3951, loss 0.1446\n",
      "test epoch: 3951, loss: 0.79\n",
      "train epoch: 3952, loss 0.1159\n",
      "test epoch: 3952, loss: 0.58\n",
      "train epoch: 3953, loss 0.0886\n",
      "test epoch: 3953, loss: 0.63\n",
      "train epoch: 3954, loss 0.0799\n",
      "test epoch: 3954, loss: 0.76\n",
      "train epoch: 3955, loss 0.0968\n",
      "test epoch: 3955, loss: 0.53\n",
      "train epoch: 3956, loss 0.1177\n",
      "test epoch: 3956, loss: 0.83\n",
      "train epoch: 3957, loss 0.1169\n",
      "test epoch: 3957, loss: 0.58\n",
      "train epoch: 3958, loss 0.0987\n",
      "test epoch: 3958, loss: 0.71\n",
      "train epoch: 3959, loss 0.0803\n",
      "test epoch: 3959, loss: 0.71\n",
      "train epoch: 3960, loss 0.0810\n",
      "test epoch: 3960, loss: 0.58\n",
      "train epoch: 3961, loss 0.0943\n",
      "test epoch: 3961, loss: 0.77\n",
      "train epoch: 3962, loss 0.0983\n",
      "test epoch: 3962, loss: 0.58\n",
      "train epoch: 3963, loss 0.0888\n",
      "test epoch: 3963, loss: 0.67\n",
      "train epoch: 3964, loss 0.0786\n",
      "test epoch: 3964, loss: 0.71\n",
      "train epoch: 3965, loss 0.0826\n",
      "test epoch: 3965, loss: 0.58\n",
      "train epoch: 3966, loss 0.0917\n",
      "test epoch: 3966, loss: 0.75\n",
      "train epoch: 3967, loss 0.0897\n",
      "test epoch: 3967, loss: 0.64\n",
      "train epoch: 3968, loss 0.0808\n",
      "test epoch: 3968, loss: 0.66\n",
      "train epoch: 3969, loss 0.0783\n",
      "test epoch: 3969, loss: 0.73\n",
      "train epoch: 3970, loss 0.0836\n",
      "test epoch: 3970, loss: 0.61\n",
      "train epoch: 3971, loss 0.0860\n",
      "test epoch: 3971, loss: 0.72\n",
      "train epoch: 3972, loss 0.0818\n",
      "test epoch: 3972, loss: 0.66\n",
      "train epoch: 3973, loss 0.0776\n",
      "test epoch: 3973, loss: 0.63\n",
      "train epoch: 3974, loss 0.0802\n",
      "test epoch: 3974, loss: 0.72\n",
      "train epoch: 3975, loss 0.0839\n",
      "test epoch: 3975, loss: 0.62\n",
      "train epoch: 3976, loss 0.0818\n",
      "test epoch: 3976, loss: 0.69\n",
      "train epoch: 3977, loss 0.0781\n",
      "test epoch: 3977, loss: 0.70\n",
      "train epoch: 3978, loss 0.0786\n",
      "test epoch: 3978, loss: 0.63\n",
      "train epoch: 3979, loss 0.0816\n",
      "test epoch: 3979, loss: 0.72\n",
      "train epoch: 3980, loss 0.0814\n",
      "test epoch: 3980, loss: 0.65\n",
      "train epoch: 3981, loss 0.0785\n",
      "test epoch: 3981, loss: 0.66\n",
      "train epoch: 3982, loss 0.0776\n",
      "test epoch: 3982, loss: 0.70\n",
      "train epoch: 3983, loss 0.0794\n",
      "test epoch: 3983, loss: 0.63\n",
      "train epoch: 3984, loss 0.0803\n",
      "test epoch: 3984, loss: 0.69\n",
      "train epoch: 3985, loss 0.0786\n",
      "test epoch: 3985, loss: 0.67\n",
      "train epoch: 3986, loss 0.0775\n",
      "test epoch: 3986, loss: 0.65\n",
      "train epoch: 3987, loss 0.0782\n",
      "test epoch: 3987, loss: 0.70\n",
      "train epoch: 3988, loss 0.0791\n",
      "test epoch: 3988, loss: 0.65\n",
      "train epoch: 3989, loss 0.0784\n",
      "test epoch: 3989, loss: 0.68\n",
      "train epoch: 3990, loss 0.0774\n",
      "test epoch: 3990, loss: 0.69\n",
      "train epoch: 3991, loss 0.0779\n",
      "test epoch: 3991, loss: 0.65\n",
      "train epoch: 3992, loss 0.0785\n",
      "test epoch: 3992, loss: 0.69\n",
      "train epoch: 3993, loss 0.0781\n",
      "test epoch: 3993, loss: 0.66\n",
      "train epoch: 3994, loss 0.0774\n",
      "test epoch: 3994, loss: 0.65\n",
      "train epoch: 3995, loss 0.0775\n",
      "test epoch: 3995, loss: 0.68\n",
      "train epoch: 3996, loss 0.0780\n",
      "test epoch: 3996, loss: 0.65\n",
      "train epoch: 3997, loss 0.0779\n",
      "test epoch: 3997, loss: 0.68\n",
      "train epoch: 3998, loss 0.0775\n",
      "test epoch: 3998, loss: 0.67\n",
      "train epoch: 3999, loss 0.0773\n",
      "test epoch: 3999, loss: 0.66\n",
      "train epoch: 4000, loss 0.0774\n",
      "test epoch: 4000, loss: 0.69\n",
      "train epoch: 4001, loss 0.0776\n",
      "test epoch: 4001, loss: 0.66\n",
      "train epoch: 4002, loss 0.0775\n",
      "test epoch: 4002, loss: 0.67\n",
      "train epoch: 4003, loss 0.0772\n",
      "test epoch: 4003, loss: 0.67\n",
      "train epoch: 4004, loss 0.0772\n",
      "test epoch: 4004, loss: 0.65\n",
      "train epoch: 4005, loss 0.0774\n",
      "test epoch: 4005, loss: 0.67\n",
      "train epoch: 4006, loss 0.0773\n",
      "test epoch: 4006, loss: 0.66\n",
      "train epoch: 4007, loss 0.0772\n",
      "test epoch: 4007, loss: 0.67\n",
      "train epoch: 4008, loss 0.0772\n",
      "test epoch: 4008, loss: 0.66\n",
      "train epoch: 4009, loss 0.0771\n",
      "test epoch: 4009, loss: 0.67\n",
      "train epoch: 4010, loss 0.0771\n",
      "test epoch: 4010, loss: 0.67\n",
      "train epoch: 4011, loss 0.0771\n",
      "test epoch: 4011, loss: 0.66\n",
      "train epoch: 4012, loss 0.0771\n",
      "test epoch: 4012, loss: 0.67\n",
      "train epoch: 4013, loss 0.0771\n",
      "test epoch: 4013, loss: 0.66\n",
      "train epoch: 4014, loss 0.0770\n",
      "test epoch: 4014, loss: 0.66\n",
      "train epoch: 4015, loss 0.0769\n",
      "test epoch: 4015, loss: 0.67\n",
      "train epoch: 4016, loss 0.0770\n",
      "test epoch: 4016, loss: 0.66\n",
      "train epoch: 4017, loss 0.0769\n",
      "test epoch: 4017, loss: 0.66\n",
      "train epoch: 4018, loss 0.0769\n",
      "test epoch: 4018, loss: 0.66\n",
      "train epoch: 4019, loss 0.0769\n",
      "test epoch: 4019, loss: 0.67\n",
      "train epoch: 4020, loss 0.0769\n",
      "test epoch: 4020, loss: 0.66\n",
      "train epoch: 4021, loss 0.0770\n",
      "test epoch: 4021, loss: 0.67\n",
      "train epoch: 4022, loss 0.0769\n",
      "test epoch: 4022, loss: 0.66\n",
      "train epoch: 4023, loss 0.0769\n",
      "test epoch: 4023, loss: 0.67\n",
      "train epoch: 4024, loss 0.0769\n",
      "test epoch: 4024, loss: 0.67\n",
      "train epoch: 4025, loss 0.0768\n",
      "test epoch: 4025, loss: 0.66\n",
      "train epoch: 4026, loss 0.0768\n",
      "test epoch: 4026, loss: 0.66\n",
      "train epoch: 4027, loss 0.0768\n",
      "test epoch: 4027, loss: 0.65\n",
      "train epoch: 4028, loss 0.0768\n",
      "test epoch: 4028, loss: 0.66\n",
      "train epoch: 4029, loss 0.0768\n",
      "test epoch: 4029, loss: 0.65\n",
      "train epoch: 4030, loss 0.0767\n",
      "test epoch: 4030, loss: 0.67\n",
      "train epoch: 4031, loss 0.0767\n",
      "test epoch: 4031, loss: 0.66\n",
      "train epoch: 4032, loss 0.0766\n",
      "test epoch: 4032, loss: 0.66\n",
      "train epoch: 4033, loss 0.0766\n",
      "test epoch: 4033, loss: 0.66\n",
      "train epoch: 4034, loss 0.0766\n",
      "test epoch: 4034, loss: 0.65\n",
      "train epoch: 4035, loss 0.0766\n",
      "test epoch: 4035, loss: 0.66\n",
      "train epoch: 4036, loss 0.0766\n",
      "test epoch: 4036, loss: 0.65\n",
      "train epoch: 4037, loss 0.0765\n",
      "test epoch: 4037, loss: 0.65\n",
      "train epoch: 4038, loss 0.0765\n",
      "test epoch: 4038, loss: 0.66\n",
      "train epoch: 4039, loss 0.0767\n",
      "test epoch: 4039, loss: 0.64\n",
      "train epoch: 4040, loss 0.0770\n",
      "test epoch: 4040, loss: 0.67\n",
      "train epoch: 4041, loss 0.0771\n",
      "test epoch: 4041, loss: 0.65\n",
      "train epoch: 4042, loss 0.0769\n",
      "test epoch: 4042, loss: 0.66\n",
      "train epoch: 4043, loss 0.0766\n",
      "test epoch: 4043, loss: 0.66\n",
      "train epoch: 4044, loss 0.0764\n",
      "test epoch: 4044, loss: 0.65\n",
      "train epoch: 4045, loss 0.0765\n",
      "test epoch: 4045, loss: 0.66\n",
      "train epoch: 4046, loss 0.0767\n",
      "test epoch: 4046, loss: 0.64\n",
      "train epoch: 4047, loss 0.0768\n",
      "test epoch: 4047, loss: 0.67\n",
      "train epoch: 4048, loss 0.0768\n",
      "test epoch: 4048, loss: 0.64\n",
      "train epoch: 4049, loss 0.0768\n",
      "test epoch: 4049, loss: 0.66\n",
      "train epoch: 4050, loss 0.0765\n",
      "test epoch: 4050, loss: 0.65\n",
      "train epoch: 4051, loss 0.0763\n",
      "test epoch: 4051, loss: 0.65\n",
      "train epoch: 4052, loss 0.0763\n",
      "test epoch: 4052, loss: 0.66\n",
      "train epoch: 4053, loss 0.0763\n",
      "test epoch: 4053, loss: 0.64\n",
      "train epoch: 4054, loss 0.0765\n",
      "test epoch: 4054, loss: 0.66\n",
      "train epoch: 4055, loss 0.0765\n",
      "test epoch: 4055, loss: 0.64\n",
      "train epoch: 4056, loss 0.0765\n",
      "test epoch: 4056, loss: 0.66\n",
      "train epoch: 4057, loss 0.0765\n",
      "test epoch: 4057, loss: 0.64\n",
      "train epoch: 4058, loss 0.0764\n",
      "test epoch: 4058, loss: 0.66\n",
      "train epoch: 4059, loss 0.0764\n",
      "test epoch: 4059, loss: 0.65\n",
      "train epoch: 4060, loss 0.0762\n",
      "test epoch: 4060, loss: 0.65\n",
      "train epoch: 4061, loss 0.0762\n",
      "test epoch: 4061, loss: 0.65\n",
      "train epoch: 4062, loss 0.0761\n",
      "test epoch: 4062, loss: 0.64\n",
      "train epoch: 4063, loss 0.0761\n",
      "test epoch: 4063, loss: 0.65\n",
      "train epoch: 4064, loss 0.0762\n",
      "test epoch: 4064, loss: 0.64\n",
      "train epoch: 4065, loss 0.0764\n",
      "test epoch: 4065, loss: 0.66\n",
      "train epoch: 4066, loss 0.0765\n",
      "test epoch: 4066, loss: 0.63\n",
      "train epoch: 4067, loss 0.0767\n",
      "test epoch: 4067, loss: 0.67\n",
      "train epoch: 4068, loss 0.0767\n",
      "test epoch: 4068, loss: 0.63\n",
      "train epoch: 4069, loss 0.0765\n",
      "test epoch: 4069, loss: 0.66\n",
      "train epoch: 4070, loss 0.0763\n",
      "test epoch: 4070, loss: 0.64\n",
      "train epoch: 4071, loss 0.0760\n",
      "test epoch: 4071, loss: 0.65\n",
      "train epoch: 4072, loss 0.0759\n",
      "test epoch: 4072, loss: 0.65\n",
      "train epoch: 4073, loss 0.0760\n",
      "test epoch: 4073, loss: 0.65\n",
      "train epoch: 4074, loss 0.0759\n",
      "test epoch: 4074, loss: 0.65\n",
      "train epoch: 4075, loss 0.0759\n",
      "test epoch: 4075, loss: 0.65\n",
      "train epoch: 4076, loss 0.0759\n",
      "test epoch: 4076, loss: 0.64\n",
      "train epoch: 4077, loss 0.0759\n",
      "test epoch: 4077, loss: 0.65\n",
      "train epoch: 4078, loss 0.0759\n",
      "test epoch: 4078, loss: 0.64\n",
      "train epoch: 4079, loss 0.0759\n",
      "test epoch: 4079, loss: 0.65\n",
      "train epoch: 4080, loss 0.0758\n",
      "test epoch: 4080, loss: 0.63\n",
      "train epoch: 4081, loss 0.0760\n",
      "test epoch: 4081, loss: 0.65\n",
      "train epoch: 4082, loss 0.0760\n",
      "test epoch: 4082, loss: 0.63\n",
      "train epoch: 4083, loss 0.0763\n",
      "test epoch: 4083, loss: 0.66\n",
      "train epoch: 4084, loss 0.0765\n",
      "test epoch: 4084, loss: 0.62\n",
      "train epoch: 4085, loss 0.0768\n",
      "test epoch: 4085, loss: 0.66\n",
      "train epoch: 4086, loss 0.0768\n",
      "test epoch: 4086, loss: 0.62\n",
      "train epoch: 4087, loss 0.0768\n",
      "test epoch: 4087, loss: 0.67\n",
      "train epoch: 4088, loss 0.0774\n",
      "test epoch: 4088, loss: 0.61\n",
      "train epoch: 4089, loss 0.0778\n",
      "test epoch: 4089, loss: 0.68\n",
      "train epoch: 4090, loss 0.0789\n",
      "test epoch: 4090, loss: 0.60\n",
      "train epoch: 4091, loss 0.0801\n",
      "test epoch: 4091, loss: 0.69\n",
      "train epoch: 4092, loss 0.0809\n",
      "test epoch: 4092, loss: 0.59\n",
      "train epoch: 4093, loss 0.0822\n",
      "test epoch: 4093, loss: 0.70\n",
      "train epoch: 4094, loss 0.0840\n",
      "test epoch: 4094, loss: 0.57\n",
      "train epoch: 4095, loss 0.0857\n",
      "test epoch: 4095, loss: 0.71\n",
      "train epoch: 4096, loss 0.0864\n",
      "test epoch: 4096, loss: 0.57\n",
      "train epoch: 4097, loss 0.0866\n",
      "test epoch: 4097, loss: 0.71\n",
      "train epoch: 4098, loss 0.0858\n",
      "test epoch: 4098, loss: 0.58\n",
      "train epoch: 4099, loss 0.0855\n",
      "test epoch: 4099, loss: 0.71\n",
      "train epoch: 4100, loss 0.0845\n",
      "test epoch: 4100, loss: 0.58\n",
      "train epoch: 4101, loss 0.0838\n",
      "test epoch: 4101, loss: 0.70\n",
      "train epoch: 4102, loss 0.0823\n",
      "test epoch: 4102, loss: 0.60\n",
      "train epoch: 4103, loss 0.0805\n",
      "test epoch: 4103, loss: 0.68\n",
      "train epoch: 4104, loss 0.0787\n",
      "test epoch: 4104, loss: 0.62\n",
      "train epoch: 4105, loss 0.0770\n",
      "test epoch: 4105, loss: 0.66\n",
      "train epoch: 4106, loss 0.0758\n",
      "test epoch: 4106, loss: 0.64\n",
      "train epoch: 4107, loss 0.0754\n",
      "test epoch: 4107, loss: 0.63\n",
      "train epoch: 4108, loss 0.0759\n",
      "test epoch: 4108, loss: 0.67\n",
      "train epoch: 4109, loss 0.0773\n",
      "test epoch: 4109, loss: 0.60\n",
      "train epoch: 4110, loss 0.0792\n",
      "test epoch: 4110, loss: 0.70\n",
      "train epoch: 4111, loss 0.0817\n",
      "test epoch: 4111, loss: 0.58\n",
      "train epoch: 4112, loss 0.0841\n",
      "test epoch: 4112, loss: 0.71\n",
      "train epoch: 4113, loss 0.0858\n",
      "test epoch: 4113, loss: 0.57\n",
      "train epoch: 4114, loss 0.0867\n",
      "test epoch: 4114, loss: 0.71\n",
      "train epoch: 4115, loss 0.0861\n",
      "test epoch: 4115, loss: 0.57\n",
      "train epoch: 4116, loss 0.0834\n",
      "test epoch: 4116, loss: 0.68\n",
      "train epoch: 4117, loss 0.0799\n",
      "test epoch: 4117, loss: 0.61\n",
      "train epoch: 4118, loss 0.0770\n",
      "test epoch: 4118, loss: 0.65\n",
      "train epoch: 4119, loss 0.0756\n",
      "test epoch: 4119, loss: 0.64\n",
      "train epoch: 4120, loss 0.0754\n",
      "test epoch: 4120, loss: 0.62\n",
      "train epoch: 4121, loss 0.0760\n",
      "test epoch: 4121, loss: 0.65\n",
      "train epoch: 4122, loss 0.0768\n",
      "test epoch: 4122, loss: 0.60\n",
      "train epoch: 4123, loss 0.0773\n",
      "test epoch: 4123, loss: 0.66\n",
      "train epoch: 4124, loss 0.0781\n",
      "test epoch: 4124, loss: 0.59\n",
      "train epoch: 4125, loss 0.0781\n",
      "test epoch: 4125, loss: 0.66\n",
      "train epoch: 4126, loss 0.0781\n",
      "test epoch: 4126, loss: 0.59\n",
      "train epoch: 4127, loss 0.0775\n",
      "test epoch: 4127, loss: 0.65\n",
      "train epoch: 4128, loss 0.0765\n",
      "test epoch: 4128, loss: 0.61\n",
      "train epoch: 4129, loss 0.0756\n",
      "test epoch: 4129, loss: 0.62\n",
      "train epoch: 4130, loss 0.0752\n",
      "test epoch: 4130, loss: 0.63\n",
      "train epoch: 4131, loss 0.0753\n",
      "test epoch: 4131, loss: 0.60\n",
      "train epoch: 4132, loss 0.0760\n",
      "test epoch: 4132, loss: 0.65\n",
      "train epoch: 4133, loss 0.0768\n",
      "test epoch: 4133, loss: 0.59\n",
      "train epoch: 4134, loss 0.0779\n",
      "test epoch: 4134, loss: 0.67\n",
      "train epoch: 4135, loss 0.0791\n",
      "test epoch: 4135, loss: 0.59\n",
      "train epoch: 4136, loss 0.0794\n",
      "test epoch: 4136, loss: 0.67\n",
      "train epoch: 4137, loss 0.0788\n",
      "test epoch: 4137, loss: 0.59\n",
      "train epoch: 4138, loss 0.0779\n",
      "test epoch: 4138, loss: 0.65\n",
      "train epoch: 4139, loss 0.0767\n",
      "test epoch: 4139, loss: 0.60\n",
      "train epoch: 4140, loss 0.0760\n",
      "test epoch: 4140, loss: 0.63\n",
      "train epoch: 4141, loss 0.0754\n",
      "test epoch: 4141, loss: 0.62\n",
      "train epoch: 4142, loss 0.0750\n",
      "test epoch: 4142, loss: 0.61\n",
      "train epoch: 4143, loss 0.0755\n",
      "test epoch: 4143, loss: 0.65\n",
      "train epoch: 4144, loss 0.0763\n",
      "test epoch: 4144, loss: 0.60\n",
      "train epoch: 4145, loss 0.0770\n",
      "test epoch: 4145, loss: 0.65\n",
      "train epoch: 4146, loss 0.0773\n",
      "test epoch: 4146, loss: 0.59\n",
      "train epoch: 4147, loss 0.0772\n",
      "test epoch: 4147, loss: 0.65\n",
      "train epoch: 4148, loss 0.0773\n",
      "test epoch: 4148, loss: 0.59\n",
      "train epoch: 4149, loss 0.0771\n",
      "test epoch: 4149, loss: 0.65\n",
      "train epoch: 4150, loss 0.0772\n",
      "test epoch: 4150, loss: 0.59\n",
      "train epoch: 4151, loss 0.0770\n",
      "test epoch: 4151, loss: 0.65\n",
      "train epoch: 4152, loss 0.0765\n",
      "test epoch: 4152, loss: 0.60\n",
      "train epoch: 4153, loss 0.0759\n",
      "test epoch: 4153, loss: 0.64\n",
      "train epoch: 4154, loss 0.0757\n",
      "test epoch: 4154, loss: 0.60\n",
      "train epoch: 4155, loss 0.0755\n",
      "test epoch: 4155, loss: 0.63\n",
      "train epoch: 4156, loss 0.0752\n",
      "test epoch: 4156, loss: 0.61\n",
      "train epoch: 4157, loss 0.0750\n",
      "test epoch: 4157, loss: 0.62\n",
      "train epoch: 4158, loss 0.0748\n",
      "test epoch: 4158, loss: 0.61\n",
      "train epoch: 4159, loss 0.0749\n",
      "test epoch: 4159, loss: 0.62\n",
      "train epoch: 4160, loss 0.0748\n",
      "test epoch: 4160, loss: 0.61\n",
      "train epoch: 4161, loss 0.0748\n",
      "test epoch: 4161, loss: 0.63\n",
      "train epoch: 4162, loss 0.0748\n",
      "test epoch: 4162, loss: 0.61\n",
      "train epoch: 4163, loss 0.0749\n",
      "test epoch: 4163, loss: 0.63\n",
      "train epoch: 4164, loss 0.0750\n",
      "test epoch: 4164, loss: 0.61\n",
      "train epoch: 4165, loss 0.0751\n",
      "test epoch: 4165, loss: 0.64\n",
      "train epoch: 4166, loss 0.0754\n",
      "test epoch: 4166, loss: 0.60\n",
      "train epoch: 4167, loss 0.0759\n",
      "test epoch: 4167, loss: 0.65\n",
      "train epoch: 4168, loss 0.0772\n",
      "test epoch: 4168, loss: 0.58\n",
      "train epoch: 4169, loss 0.0787\n",
      "test epoch: 4169, loss: 0.67\n",
      "train epoch: 4170, loss 0.0801\n",
      "test epoch: 4170, loss: 0.56\n",
      "train epoch: 4171, loss 0.0826\n",
      "test epoch: 4171, loss: 0.68\n",
      "train epoch: 4172, loss 0.0856\n",
      "test epoch: 4172, loss: 0.54\n",
      "train epoch: 4173, loss 0.0886\n",
      "test epoch: 4173, loss: 0.70\n",
      "train epoch: 4174, loss 0.0901\n",
      "test epoch: 4174, loss: 0.54\n",
      "train epoch: 4175, loss 0.0911\n",
      "test epoch: 4175, loss: 0.71\n",
      "train epoch: 4176, loss 0.0901\n",
      "test epoch: 4176, loss: 0.55\n",
      "train epoch: 4177, loss 0.0897\n",
      "test epoch: 4177, loss: 0.70\n",
      "train epoch: 4178, loss 0.0875\n",
      "test epoch: 4178, loss: 0.56\n",
      "train epoch: 4179, loss 0.0840\n",
      "test epoch: 4179, loss: 0.67\n",
      "train epoch: 4180, loss 0.0799\n",
      "test epoch: 4180, loss: 0.59\n",
      "train epoch: 4181, loss 0.0765\n",
      "test epoch: 4181, loss: 0.63\n",
      "train epoch: 4182, loss 0.0752\n",
      "test epoch: 4182, loss: 0.62\n",
      "train epoch: 4183, loss 0.0746\n",
      "test epoch: 4183, loss: 0.60\n",
      "train epoch: 4184, loss 0.0751\n",
      "test epoch: 4184, loss: 0.64\n",
      "train epoch: 4185, loss 0.0761\n",
      "test epoch: 4185, loss: 0.58\n",
      "train epoch: 4186, loss 0.0769\n",
      "test epoch: 4186, loss: 0.65\n",
      "train epoch: 4187, loss 0.0778\n",
      "test epoch: 4187, loss: 0.57\n",
      "train epoch: 4188, loss 0.0782\n",
      "test epoch: 4188, loss: 0.65\n",
      "train epoch: 4189, loss 0.0782\n",
      "test epoch: 4189, loss: 0.58\n",
      "train epoch: 4190, loss 0.0776\n",
      "test epoch: 4190, loss: 0.65\n",
      "train epoch: 4191, loss 0.0772\n",
      "test epoch: 4191, loss: 0.59\n",
      "train epoch: 4192, loss 0.0763\n",
      "test epoch: 4192, loss: 0.63\n",
      "train epoch: 4193, loss 0.0753\n",
      "test epoch: 4193, loss: 0.61\n",
      "train epoch: 4194, loss 0.0745\n",
      "test epoch: 4194, loss: 0.60\n",
      "train epoch: 4195, loss 0.0744\n",
      "test epoch: 4195, loss: 0.62\n",
      "train epoch: 4196, loss 0.0747\n",
      "test epoch: 4196, loss: 0.59\n",
      "train epoch: 4197, loss 0.0748\n",
      "test epoch: 4197, loss: 0.62\n",
      "train epoch: 4198, loss 0.0749\n",
      "test epoch: 4198, loss: 0.59\n",
      "train epoch: 4199, loss 0.0751\n",
      "test epoch: 4199, loss: 0.63\n",
      "train epoch: 4200, loss 0.0751\n",
      "test epoch: 4200, loss: 0.60\n",
      "train epoch: 4201, loss 0.0749\n",
      "test epoch: 4201, loss: 0.62\n",
      "train epoch: 4202, loss 0.0747\n",
      "test epoch: 4202, loss: 0.60\n",
      "train epoch: 4203, loss 0.0745\n",
      "test epoch: 4203, loss: 0.61\n",
      "train epoch: 4204, loss 0.0743\n",
      "test epoch: 4204, loss: 0.61\n",
      "train epoch: 4205, loss 0.0744\n",
      "test epoch: 4205, loss: 0.59\n",
      "train epoch: 4206, loss 0.0750\n",
      "test epoch: 4206, loss: 0.63\n",
      "train epoch: 4207, loss 0.0762\n",
      "test epoch: 4207, loss: 0.57\n",
      "train epoch: 4208, loss 0.0775\n",
      "test epoch: 4208, loss: 0.65\n",
      "train epoch: 4209, loss 0.0784\n",
      "test epoch: 4209, loss: 0.56\n",
      "train epoch: 4210, loss 0.0792\n",
      "test epoch: 4210, loss: 0.66\n",
      "train epoch: 4211, loss 0.0802\n",
      "test epoch: 4211, loss: 0.55\n",
      "train epoch: 4212, loss 0.0805\n",
      "test epoch: 4212, loss: 0.66\n",
      "train epoch: 4213, loss 0.0804\n",
      "test epoch: 4213, loss: 0.56\n",
      "train epoch: 4214, loss 0.0797\n",
      "test epoch: 4214, loss: 0.65\n",
      "train epoch: 4215, loss 0.0790\n",
      "test epoch: 4215, loss: 0.57\n",
      "train epoch: 4216, loss 0.0777\n",
      "test epoch: 4216, loss: 0.63\n",
      "train epoch: 4217, loss 0.0762\n",
      "test epoch: 4217, loss: 0.59\n",
      "train epoch: 4218, loss 0.0749\n",
      "test epoch: 4218, loss: 0.61\n",
      "train epoch: 4219, loss 0.0742\n",
      "test epoch: 4219, loss: 0.60\n",
      "train epoch: 4220, loss 0.0742\n",
      "test epoch: 4220, loss: 0.59\n",
      "train epoch: 4221, loss 0.0742\n",
      "test epoch: 4221, loss: 0.62\n",
      "train epoch: 4222, loss 0.0747\n",
      "test epoch: 4222, loss: 0.59\n",
      "train epoch: 4223, loss 0.0752\n",
      "test epoch: 4223, loss: 0.63\n",
      "train epoch: 4224, loss 0.0757\n",
      "test epoch: 4224, loss: 0.58\n",
      "train epoch: 4225, loss 0.0759\n",
      "test epoch: 4225, loss: 0.63\n",
      "train epoch: 4226, loss 0.0758\n",
      "test epoch: 4226, loss: 0.58\n",
      "train epoch: 4227, loss 0.0757\n",
      "test epoch: 4227, loss: 0.63\n",
      "train epoch: 4228, loss 0.0757\n",
      "test epoch: 4228, loss: 0.57\n",
      "train epoch: 4229, loss 0.0760\n",
      "test epoch: 4229, loss: 0.63\n",
      "train epoch: 4230, loss 0.0762\n",
      "test epoch: 4230, loss: 0.57\n",
      "train epoch: 4231, loss 0.0766\n",
      "test epoch: 4231, loss: 0.64\n",
      "train epoch: 4232, loss 0.0774\n",
      "test epoch: 4232, loss: 0.57\n",
      "train epoch: 4233, loss 0.0778\n",
      "test epoch: 4233, loss: 0.65\n",
      "train epoch: 4234, loss 0.0779\n",
      "test epoch: 4234, loss: 0.56\n",
      "train epoch: 4235, loss 0.0781\n",
      "test epoch: 4235, loss: 0.65\n",
      "train epoch: 4236, loss 0.0786\n",
      "test epoch: 4236, loss: 0.56\n",
      "train epoch: 4237, loss 0.0791\n",
      "test epoch: 4237, loss: 0.65\n",
      "train epoch: 4238, loss 0.0792\n",
      "test epoch: 4238, loss: 0.55\n",
      "train epoch: 4239, loss 0.0793\n",
      "test epoch: 4239, loss: 0.65\n",
      "train epoch: 4240, loss 0.0792\n",
      "test epoch: 4240, loss: 0.56\n",
      "train epoch: 4241, loss 0.0786\n",
      "test epoch: 4241, loss: 0.65\n",
      "train epoch: 4242, loss 0.0787\n",
      "test epoch: 4242, loss: 0.56\n",
      "train epoch: 4243, loss 0.0787\n",
      "test epoch: 4243, loss: 0.64\n",
      "train epoch: 4244, loss 0.0781\n",
      "test epoch: 4244, loss: 0.56\n",
      "train epoch: 4245, loss 0.0773\n",
      "test epoch: 4245, loss: 0.63\n",
      "train epoch: 4246, loss 0.0767\n",
      "test epoch: 4246, loss: 0.57\n",
      "train epoch: 4247, loss 0.0762\n",
      "test epoch: 4247, loss: 0.63\n",
      "train epoch: 4248, loss 0.0758\n",
      "test epoch: 4248, loss: 0.58\n",
      "train epoch: 4249, loss 0.0753\n",
      "test epoch: 4249, loss: 0.63\n",
      "train epoch: 4250, loss 0.0755\n",
      "test epoch: 4250, loss: 0.58\n",
      "train epoch: 4251, loss 0.0754\n",
      "test epoch: 4251, loss: 0.63\n",
      "train epoch: 4252, loss 0.0751\n",
      "test epoch: 4252, loss: 0.59\n",
      "train epoch: 4253, loss 0.0746\n",
      "test epoch: 4253, loss: 0.61\n",
      "train epoch: 4254, loss 0.0742\n",
      "test epoch: 4254, loss: 0.59\n",
      "train epoch: 4255, loss 0.0742\n",
      "test epoch: 4255, loss: 0.61\n",
      "train epoch: 4256, loss 0.0740\n",
      "test epoch: 4256, loss: 0.59\n",
      "train epoch: 4257, loss 0.0737\n",
      "test epoch: 4257, loss: 0.60\n",
      "train epoch: 4258, loss 0.0738\n",
      "test epoch: 4258, loss: 0.61\n",
      "train epoch: 4259, loss 0.0739\n",
      "test epoch: 4259, loss: 0.59\n",
      "train epoch: 4260, loss 0.0741\n",
      "test epoch: 4260, loss: 0.61\n",
      "train epoch: 4261, loss 0.0742\n",
      "test epoch: 4261, loss: 0.58\n",
      "train epoch: 4262, loss 0.0745\n",
      "test epoch: 4262, loss: 0.62\n",
      "train epoch: 4263, loss 0.0752\n",
      "test epoch: 4263, loss: 0.57\n",
      "train epoch: 4264, loss 0.0762\n",
      "test epoch: 4264, loss: 0.64\n",
      "train epoch: 4265, loss 0.0778\n",
      "test epoch: 4265, loss: 0.55\n",
      "train epoch: 4266, loss 0.0797\n",
      "test epoch: 4266, loss: 0.66\n",
      "train epoch: 4267, loss 0.0823\n",
      "test epoch: 4267, loss: 0.53\n",
      "train epoch: 4268, loss 0.0862\n",
      "test epoch: 4268, loss: 0.69\n",
      "train epoch: 4269, loss 0.0904\n",
      "test epoch: 4269, loss: 0.51\n",
      "train epoch: 4270, loss 0.0957\n",
      "test epoch: 4270, loss: 0.70\n",
      "train epoch: 4271, loss 0.0992\n",
      "test epoch: 4271, loss: 0.49\n",
      "train epoch: 4272, loss 0.1020\n",
      "test epoch: 4272, loss: 0.68\n",
      "train epoch: 4273, loss 0.0973\n",
      "test epoch: 4273, loss: 0.51\n",
      "train epoch: 4274, loss 0.0891\n",
      "test epoch: 4274, loss: 0.63\n",
      "train epoch: 4275, loss 0.0796\n",
      "test epoch: 4275, loss: 0.57\n",
      "train epoch: 4276, loss 0.0742\n",
      "test epoch: 4276, loss: 0.57\n",
      "train epoch: 4277, loss 0.0744\n",
      "test epoch: 4277, loss: 0.63\n",
      "train epoch: 4278, loss 0.0785\n",
      "test epoch: 4278, loss: 0.52\n",
      "train epoch: 4279, loss 0.0839\n",
      "test epoch: 4279, loss: 0.66\n",
      "train epoch: 4280, loss 0.0873\n",
      "test epoch: 4280, loss: 0.51\n",
      "train epoch: 4281, loss 0.0875\n",
      "test epoch: 4281, loss: 0.64\n",
      "train epoch: 4282, loss 0.0826\n",
      "test epoch: 4282, loss: 0.54\n",
      "train epoch: 4283, loss 0.0772\n",
      "test epoch: 4283, loss: 0.58\n",
      "train epoch: 4284, loss 0.0737\n",
      "test epoch: 4284, loss: 0.60\n",
      "train epoch: 4285, loss 0.0749\n",
      "test epoch: 4285, loss: 0.54\n",
      "train epoch: 4286, loss 0.0794\n",
      "test epoch: 4286, loss: 0.65\n",
      "train epoch: 4287, loss 0.0825\n",
      "test epoch: 4287, loss: 0.53\n",
      "train epoch: 4288, loss 0.0825\n",
      "test epoch: 4288, loss: 0.63\n",
      "train epoch: 4289, loss 0.0800\n",
      "test epoch: 4289, loss: 0.55\n",
      "train epoch: 4290, loss 0.0764\n",
      "test epoch: 4290, loss: 0.59\n",
      "train epoch: 4291, loss 0.0739\n",
      "test epoch: 4291, loss: 0.58\n",
      "train epoch: 4292, loss 0.0736\n",
      "test epoch: 4292, loss: 0.56\n",
      "train epoch: 4293, loss 0.0742\n",
      "test epoch: 4293, loss: 0.61\n",
      "train epoch: 4294, loss 0.0753\n",
      "test epoch: 4294, loss: 0.55\n",
      "train epoch: 4295, loss 0.0758\n",
      "test epoch: 4295, loss: 0.61\n",
      "train epoch: 4296, loss 0.0751\n",
      "test epoch: 4296, loss: 0.57\n",
      "train epoch: 4297, loss 0.0741\n",
      "test epoch: 4297, loss: 0.58\n",
      "train epoch: 4298, loss 0.0734\n",
      "test epoch: 4298, loss: 0.58\n",
      "train epoch: 4299, loss 0.0734\n",
      "test epoch: 4299, loss: 0.57\n",
      "train epoch: 4300, loss 0.0737\n",
      "test epoch: 4300, loss: 0.59\n",
      "train epoch: 4301, loss 0.0738\n",
      "test epoch: 4301, loss: 0.56\n",
      "train epoch: 4302, loss 0.0739\n",
      "test epoch: 4302, loss: 0.59\n",
      "train epoch: 4303, loss 0.0737\n",
      "test epoch: 4303, loss: 0.57\n",
      "train epoch: 4304, loss 0.0734\n",
      "test epoch: 4304, loss: 0.58\n",
      "train epoch: 4305, loss 0.0732\n",
      "test epoch: 4305, loss: 0.58\n",
      "train epoch: 4306, loss 0.0733\n",
      "test epoch: 4306, loss: 0.56\n",
      "train epoch: 4307, loss 0.0736\n",
      "test epoch: 4307, loss: 0.59\n",
      "train epoch: 4308, loss 0.0739\n",
      "test epoch: 4308, loss: 0.55\n",
      "train epoch: 4309, loss 0.0743\n",
      "test epoch: 4309, loss: 0.60\n",
      "train epoch: 4310, loss 0.0746\n",
      "test epoch: 4310, loss: 0.55\n",
      "train epoch: 4311, loss 0.0745\n",
      "test epoch: 4311, loss: 0.59\n",
      "train epoch: 4312, loss 0.0739\n",
      "test epoch: 4312, loss: 0.57\n",
      "train epoch: 4313, loss 0.0733\n",
      "test epoch: 4313, loss: 0.57\n",
      "train epoch: 4314, loss 0.0732\n",
      "test epoch: 4314, loss: 0.58\n",
      "train epoch: 4315, loss 0.0734\n",
      "test epoch: 4315, loss: 0.55\n",
      "train epoch: 4316, loss 0.0740\n",
      "test epoch: 4316, loss: 0.59\n",
      "train epoch: 4317, loss 0.0742\n",
      "test epoch: 4317, loss: 0.55\n",
      "train epoch: 4318, loss 0.0746\n",
      "test epoch: 4318, loss: 0.60\n",
      "train epoch: 4319, loss 0.0747\n",
      "test epoch: 4319, loss: 0.55\n",
      "train epoch: 4320, loss 0.0745\n",
      "test epoch: 4320, loss: 0.59\n",
      "train epoch: 4321, loss 0.0738\n",
      "test epoch: 4321, loss: 0.57\n",
      "train epoch: 4322, loss 0.0732\n",
      "test epoch: 4322, loss: 0.57\n",
      "train epoch: 4323, loss 0.0730\n",
      "test epoch: 4323, loss: 0.58\n",
      "train epoch: 4324, loss 0.0733\n",
      "test epoch: 4324, loss: 0.55\n",
      "train epoch: 4325, loss 0.0740\n",
      "test epoch: 4325, loss: 0.59\n",
      "train epoch: 4326, loss 0.0744\n",
      "test epoch: 4326, loss: 0.54\n",
      "train epoch: 4327, loss 0.0751\n",
      "test epoch: 4327, loss: 0.60\n",
      "train epoch: 4328, loss 0.0753\n",
      "test epoch: 4328, loss: 0.55\n",
      "train epoch: 4329, loss 0.0751\n",
      "test epoch: 4329, loss: 0.60\n",
      "train epoch: 4330, loss 0.0744\n",
      "test epoch: 4330, loss: 0.56\n",
      "train epoch: 4331, loss 0.0735\n",
      "test epoch: 4331, loss: 0.58\n",
      "train epoch: 4332, loss 0.0730\n",
      "test epoch: 4332, loss: 0.58\n",
      "train epoch: 4333, loss 0.0730\n",
      "test epoch: 4333, loss: 0.55\n",
      "train epoch: 4334, loss 0.0736\n",
      "test epoch: 4334, loss: 0.59\n",
      "train epoch: 4335, loss 0.0744\n",
      "test epoch: 4335, loss: 0.54\n",
      "train epoch: 4336, loss 0.0756\n",
      "test epoch: 4336, loss: 0.61\n",
      "train epoch: 4337, loss 0.0764\n",
      "test epoch: 4337, loss: 0.54\n",
      "train epoch: 4338, loss 0.0767\n",
      "test epoch: 4338, loss: 0.62\n",
      "train epoch: 4339, loss 0.0772\n",
      "test epoch: 4339, loss: 0.54\n",
      "train epoch: 4340, loss 0.0771\n",
      "test epoch: 4340, loss: 0.61\n",
      "train epoch: 4341, loss 0.0761\n",
      "test epoch: 4341, loss: 0.55\n",
      "train epoch: 4342, loss 0.0748\n",
      "test epoch: 4342, loss: 0.59\n",
      "train epoch: 4343, loss 0.0738\n",
      "test epoch: 4343, loss: 0.56\n",
      "train epoch: 4344, loss 0.0734\n",
      "test epoch: 4344, loss: 0.58\n",
      "train epoch: 4345, loss 0.0730\n",
      "test epoch: 4345, loss: 0.57\n",
      "train epoch: 4346, loss 0.0729\n",
      "test epoch: 4346, loss: 0.59\n",
      "train epoch: 4347, loss 0.0730\n",
      "test epoch: 4347, loss: 0.57\n",
      "train epoch: 4348, loss 0.0730\n",
      "test epoch: 4348, loss: 0.59\n",
      "train epoch: 4349, loss 0.0730\n",
      "test epoch: 4349, loss: 0.56\n",
      "train epoch: 4350, loss 0.0730\n",
      "test epoch: 4350, loss: 0.58\n",
      "train epoch: 4351, loss 0.0728\n",
      "test epoch: 4351, loss: 0.56\n",
      "train epoch: 4352, loss 0.0730\n",
      "test epoch: 4352, loss: 0.58\n",
      "train epoch: 4353, loss 0.0730\n",
      "test epoch: 4353, loss: 0.56\n",
      "train epoch: 4354, loss 0.0727\n",
      "test epoch: 4354, loss: 0.57\n",
      "train epoch: 4355, loss 0.0727\n",
      "test epoch: 4355, loss: 0.57\n",
      "train epoch: 4356, loss 0.0727\n",
      "test epoch: 4356, loss: 0.57\n",
      "train epoch: 4357, loss 0.0727\n",
      "test epoch: 4357, loss: 0.58\n",
      "train epoch: 4358, loss 0.0727\n",
      "test epoch: 4358, loss: 0.56\n",
      "train epoch: 4359, loss 0.0729\n",
      "test epoch: 4359, loss: 0.58\n",
      "train epoch: 4360, loss 0.0732\n",
      "test epoch: 4360, loss: 0.55\n",
      "train epoch: 4361, loss 0.0739\n",
      "test epoch: 4361, loss: 0.60\n",
      "train epoch: 4362, loss 0.0750\n",
      "test epoch: 4362, loss: 0.53\n",
      "train epoch: 4363, loss 0.0764\n",
      "test epoch: 4363, loss: 0.62\n",
      "train epoch: 4364, loss 0.0786\n",
      "test epoch: 4364, loss: 0.52\n",
      "train epoch: 4365, loss 0.0805\n",
      "test epoch: 4365, loss: 0.64\n",
      "train epoch: 4366, loss 0.0830\n",
      "test epoch: 4366, loss: 0.50\n",
      "train epoch: 4367, loss 0.0862\n",
      "test epoch: 4367, loss: 0.65\n",
      "train epoch: 4368, loss 0.0884\n",
      "test epoch: 4368, loss: 0.48\n",
      "train epoch: 4369, loss 0.0911\n",
      "test epoch: 4369, loss: 0.65\n",
      "train epoch: 4370, loss 0.0901\n",
      "test epoch: 4370, loss: 0.49\n",
      "train epoch: 4371, loss 0.0872\n",
      "test epoch: 4371, loss: 0.62\n",
      "train epoch: 4372, loss 0.0815\n",
      "test epoch: 4372, loss: 0.53\n",
      "train epoch: 4373, loss 0.0766\n",
      "test epoch: 4373, loss: 0.59\n",
      "train epoch: 4374, loss 0.0737\n",
      "test epoch: 4374, loss: 0.57\n",
      "train epoch: 4375, loss 0.0727\n",
      "test epoch: 4375, loss: 0.56\n",
      "train epoch: 4376, loss 0.0734\n",
      "test epoch: 4376, loss: 0.61\n",
      "train epoch: 4377, loss 0.0750\n",
      "test epoch: 4377, loss: 0.53\n",
      "train epoch: 4378, loss 0.0767\n",
      "test epoch: 4378, loss: 0.62\n",
      "train epoch: 4379, loss 0.0779\n",
      "test epoch: 4379, loss: 0.52\n",
      "train epoch: 4380, loss 0.0782\n",
      "test epoch: 4380, loss: 0.61\n",
      "train epoch: 4381, loss 0.0770\n",
      "test epoch: 4381, loss: 0.54\n",
      "train epoch: 4382, loss 0.0755\n",
      "test epoch: 4382, loss: 0.59\n",
      "train epoch: 4383, loss 0.0740\n",
      "test epoch: 4383, loss: 0.56\n",
      "train epoch: 4384, loss 0.0729\n",
      "test epoch: 4384, loss: 0.57\n",
      "train epoch: 4385, loss 0.0725\n",
      "test epoch: 4385, loss: 0.58\n",
      "train epoch: 4386, loss 0.0726\n",
      "test epoch: 4386, loss: 0.55\n",
      "train epoch: 4387, loss 0.0734\n",
      "test epoch: 4387, loss: 0.60\n",
      "train epoch: 4388, loss 0.0744\n",
      "test epoch: 4388, loss: 0.54\n",
      "train epoch: 4389, loss 0.0755\n",
      "test epoch: 4389, loss: 0.61\n",
      "train epoch: 4390, loss 0.0759\n",
      "test epoch: 4390, loss: 0.54\n",
      "train epoch: 4391, loss 0.0758\n",
      "test epoch: 4391, loss: 0.61\n",
      "train epoch: 4392, loss 0.0755\n",
      "test epoch: 4392, loss: 0.55\n",
      "train epoch: 4393, loss 0.0746\n",
      "test epoch: 4393, loss: 0.59\n",
      "train epoch: 4394, loss 0.0734\n",
      "test epoch: 4394, loss: 0.56\n",
      "train epoch: 4395, loss 0.0725\n",
      "test epoch: 4395, loss: 0.57\n",
      "train epoch: 4396, loss 0.0723\n",
      "test epoch: 4396, loss: 0.57\n",
      "train epoch: 4397, loss 0.0722\n",
      "test epoch: 4397, loss: 0.57\n",
      "train epoch: 4398, loss 0.0723\n",
      "test epoch: 4398, loss: 0.57\n",
      "train epoch: 4399, loss 0.0723\n",
      "test epoch: 4399, loss: 0.58\n",
      "train epoch: 4400, loss 0.0723\n",
      "test epoch: 4400, loss: 0.57\n",
      "train epoch: 4401, loss 0.0722\n",
      "test epoch: 4401, loss: 0.57\n",
      "train epoch: 4402, loss 0.0722\n",
      "test epoch: 4402, loss: 0.57\n",
      "train epoch: 4403, loss 0.0722\n",
      "test epoch: 4403, loss: 0.57\n",
      "train epoch: 4404, loss 0.0722\n",
      "test epoch: 4404, loss: 0.58\n",
      "train epoch: 4405, loss 0.0723\n",
      "test epoch: 4405, loss: 0.57\n",
      "train epoch: 4406, loss 0.0722\n",
      "test epoch: 4406, loss: 0.57\n",
      "train epoch: 4407, loss 0.0722\n",
      "test epoch: 4407, loss: 0.57\n",
      "train epoch: 4408, loss 0.0721\n",
      "test epoch: 4408, loss: 0.56\n",
      "train epoch: 4409, loss 0.0723\n",
      "test epoch: 4409, loss: 0.57\n",
      "train epoch: 4410, loss 0.0723\n",
      "test epoch: 4410, loss: 0.56\n",
      "train epoch: 4411, loss 0.0722\n",
      "test epoch: 4411, loss: 0.59\n",
      "train epoch: 4412, loss 0.0725\n",
      "test epoch: 4412, loss: 0.56\n",
      "train epoch: 4413, loss 0.0729\n",
      "test epoch: 4413, loss: 0.59\n",
      "train epoch: 4414, loss 0.0730\n",
      "test epoch: 4414, loss: 0.56\n",
      "train epoch: 4415, loss 0.0729\n",
      "test epoch: 4415, loss: 0.59\n",
      "train epoch: 4416, loss 0.0727\n",
      "test epoch: 4416, loss: 0.55\n",
      "train epoch: 4417, loss 0.0729\n",
      "test epoch: 4417, loss: 0.58\n",
      "train epoch: 4418, loss 0.0730\n",
      "test epoch: 4418, loss: 0.55\n",
      "train epoch: 4419, loss 0.0726\n",
      "test epoch: 4419, loss: 0.58\n",
      "train epoch: 4420, loss 0.0724\n",
      "test epoch: 4420, loss: 0.56\n",
      "train epoch: 4421, loss 0.0725\n",
      "test epoch: 4421, loss: 0.59\n",
      "train epoch: 4422, loss 0.0725\n",
      "test epoch: 4422, loss: 0.56\n",
      "train epoch: 4423, loss 0.0724\n",
      "test epoch: 4423, loss: 0.58\n",
      "train epoch: 4424, loss 0.0722\n",
      "test epoch: 4424, loss: 0.56\n",
      "train epoch: 4425, loss 0.0722\n",
      "test epoch: 4425, loss: 0.58\n",
      "train epoch: 4426, loss 0.0722\n",
      "test epoch: 4426, loss: 0.56\n",
      "train epoch: 4427, loss 0.0721\n",
      "test epoch: 4427, loss: 0.58\n",
      "train epoch: 4428, loss 0.0722\n",
      "test epoch: 4428, loss: 0.56\n",
      "train epoch: 4429, loss 0.0724\n",
      "test epoch: 4429, loss: 0.59\n",
      "train epoch: 4430, loss 0.0725\n",
      "test epoch: 4430, loss: 0.56\n",
      "train epoch: 4431, loss 0.0725\n",
      "test epoch: 4431, loss: 0.58\n",
      "train epoch: 4432, loss 0.0725\n",
      "test epoch: 4432, loss: 0.55\n",
      "train epoch: 4433, loss 0.0729\n",
      "test epoch: 4433, loss: 0.59\n",
      "train epoch: 4434, loss 0.0732\n",
      "test epoch: 4434, loss: 0.55\n",
      "train epoch: 4435, loss 0.0732\n",
      "test epoch: 4435, loss: 0.60\n",
      "train epoch: 4436, loss 0.0735\n",
      "test epoch: 4436, loss: 0.55\n",
      "train epoch: 4437, loss 0.0741\n",
      "test epoch: 4437, loss: 0.61\n",
      "train epoch: 4438, loss 0.0746\n",
      "test epoch: 4438, loss: 0.54\n",
      "train epoch: 4439, loss 0.0752\n",
      "test epoch: 4439, loss: 0.62\n",
      "train epoch: 4440, loss 0.0773\n",
      "test epoch: 4440, loss: 0.52\n",
      "train epoch: 4441, loss 0.0801\n",
      "test epoch: 4441, loss: 0.64\n",
      "train epoch: 4442, loss 0.0845\n",
      "test epoch: 4442, loss: 0.48\n",
      "train epoch: 4443, loss 0.0903\n",
      "test epoch: 4443, loss: 0.66\n",
      "train epoch: 4444, loss 0.0966\n",
      "test epoch: 4444, loss: 0.45\n",
      "train epoch: 4445, loss 0.1056\n",
      "test epoch: 4445, loss: 0.68\n",
      "train epoch: 4446, loss 0.1085\n",
      "test epoch: 4446, loss: 0.44\n",
      "train epoch: 4447, loss 0.1084\n",
      "test epoch: 4447, loss: 0.64\n",
      "train epoch: 4448, loss 0.0964\n",
      "test epoch: 4448, loss: 0.49\n",
      "train epoch: 4449, loss 0.0818\n",
      "test epoch: 4449, loss: 0.56\n",
      "train epoch: 4450, loss 0.0729\n",
      "test epoch: 4450, loss: 0.58\n",
      "train epoch: 4451, loss 0.0738\n",
      "test epoch: 4451, loss: 0.50\n",
      "train epoch: 4452, loss 0.0805\n",
      "test epoch: 4452, loss: 0.62\n",
      "train epoch: 4453, loss 0.0853\n",
      "test epoch: 4453, loss: 0.49\n",
      "train epoch: 4454, loss 0.0841\n",
      "test epoch: 4454, loss: 0.60\n",
      "train epoch: 4455, loss 0.0784\n",
      "test epoch: 4455, loss: 0.52\n",
      "train epoch: 4456, loss 0.0735\n",
      "test epoch: 4456, loss: 0.53\n",
      "train epoch: 4457, loss 0.0722\n",
      "test epoch: 4457, loss: 0.57\n",
      "train epoch: 4458, loss 0.0748\n",
      "test epoch: 4458, loss: 0.50\n",
      "train epoch: 4459, loss 0.0782\n",
      "test epoch: 4459, loss: 0.60\n",
      "train epoch: 4460, loss 0.0788\n",
      "test epoch: 4460, loss: 0.51\n",
      "train epoch: 4461, loss 0.0762\n",
      "test epoch: 4461, loss: 0.57\n",
      "train epoch: 4462, loss 0.0727\n",
      "test epoch: 4462, loss: 0.56\n",
      "train epoch: 4463, loss 0.0723\n",
      "test epoch: 4463, loss: 0.52\n",
      "train epoch: 4464, loss 0.0743\n",
      "test epoch: 4464, loss: 0.59\n",
      "train epoch: 4465, loss 0.0767\n",
      "test epoch: 4465, loss: 0.51\n",
      "train epoch: 4466, loss 0.0765\n",
      "test epoch: 4466, loss: 0.58\n",
      "train epoch: 4467, loss 0.0742\n",
      "test epoch: 4467, loss: 0.54\n",
      "train epoch: 4468, loss 0.0721\n",
      "test epoch: 4468, loss: 0.54\n",
      "train epoch: 4469, loss 0.0723\n",
      "test epoch: 4469, loss: 0.58\n",
      "train epoch: 4470, loss 0.0738\n",
      "test epoch: 4470, loss: 0.52\n",
      "train epoch: 4471, loss 0.0746\n",
      "test epoch: 4471, loss: 0.58\n",
      "train epoch: 4472, loss 0.0742\n",
      "test epoch: 4472, loss: 0.53\n",
      "train epoch: 4473, loss 0.0727\n",
      "test epoch: 4473, loss: 0.55\n",
      "train epoch: 4474, loss 0.0718\n",
      "test epoch: 4474, loss: 0.56\n",
      "train epoch: 4475, loss 0.0722\n",
      "test epoch: 4475, loss: 0.52\n",
      "train epoch: 4476, loss 0.0733\n",
      "test epoch: 4476, loss: 0.58\n",
      "train epoch: 4477, loss 0.0740\n",
      "test epoch: 4477, loss: 0.52\n",
      "train epoch: 4478, loss 0.0736\n",
      "test epoch: 4478, loss: 0.57\n",
      "train epoch: 4479, loss 0.0729\n",
      "test epoch: 4479, loss: 0.54\n",
      "train epoch: 4480, loss 0.0720\n",
      "test epoch: 4480, loss: 0.55\n",
      "train epoch: 4481, loss 0.0718\n",
      "test epoch: 4481, loss: 0.56\n",
      "train epoch: 4482, loss 0.0723\n",
      "test epoch: 4482, loss: 0.53\n",
      "train epoch: 4483, loss 0.0727\n",
      "test epoch: 4483, loss: 0.56\n",
      "train epoch: 4484, loss 0.0724\n",
      "test epoch: 4484, loss: 0.53\n",
      "train epoch: 4485, loss 0.0722\n",
      "test epoch: 4485, loss: 0.55\n",
      "train epoch: 4486, loss 0.0717\n",
      "test epoch: 4486, loss: 0.55\n",
      "train epoch: 4487, loss 0.0717\n",
      "test epoch: 4487, loss: 0.53\n",
      "train epoch: 4488, loss 0.0723\n",
      "test epoch: 4488, loss: 0.57\n",
      "train epoch: 4489, loss 0.0727\n",
      "test epoch: 4489, loss: 0.53\n",
      "train epoch: 4490, loss 0.0725\n",
      "test epoch: 4490, loss: 0.56\n",
      "train epoch: 4491, loss 0.0720\n",
      "test epoch: 4491, loss: 0.54\n",
      "train epoch: 4492, loss 0.0715\n",
      "test epoch: 4492, loss: 0.53\n",
      "train epoch: 4493, loss 0.0717\n",
      "test epoch: 4493, loss: 0.56\n",
      "train epoch: 4494, loss 0.0723\n",
      "test epoch: 4494, loss: 0.52\n",
      "train epoch: 4495, loss 0.0732\n",
      "test epoch: 4495, loss: 0.57\n",
      "train epoch: 4496, loss 0.0735\n",
      "test epoch: 4496, loss: 0.52\n",
      "train epoch: 4497, loss 0.0730\n",
      "test epoch: 4497, loss: 0.56\n",
      "train epoch: 4498, loss 0.0720\n",
      "test epoch: 4498, loss: 0.54\n",
      "train epoch: 4499, loss 0.0715\n",
      "test epoch: 4499, loss: 0.53\n",
      "train epoch: 4500, loss 0.0716\n",
      "test epoch: 4500, loss: 0.56\n",
      "train epoch: 4501, loss 0.0722\n",
      "test epoch: 4501, loss: 0.52\n",
      "train epoch: 4502, loss 0.0729\n",
      "test epoch: 4502, loss: 0.56\n",
      "train epoch: 4503, loss 0.0727\n",
      "test epoch: 4503, loss: 0.52\n",
      "train epoch: 4504, loss 0.0726\n",
      "test epoch: 4504, loss: 0.56\n",
      "train epoch: 4505, loss 0.0721\n",
      "test epoch: 4505, loss: 0.53\n",
      "train epoch: 4506, loss 0.0717\n",
      "test epoch: 4506, loss: 0.55\n",
      "train epoch: 4507, loss 0.0714\n",
      "test epoch: 4507, loss: 0.55\n",
      "train epoch: 4508, loss 0.0714\n",
      "test epoch: 4508, loss: 0.53\n",
      "train epoch: 4509, loss 0.0716\n",
      "test epoch: 4509, loss: 0.55\n",
      "train epoch: 4510, loss 0.0718\n",
      "test epoch: 4510, loss: 0.52\n",
      "train epoch: 4511, loss 0.0721\n",
      "test epoch: 4511, loss: 0.55\n",
      "train epoch: 4512, loss 0.0719\n",
      "test epoch: 4512, loss: 0.53\n",
      "train epoch: 4513, loss 0.0714\n",
      "test epoch: 4513, loss: 0.54\n",
      "train epoch: 4514, loss 0.0713\n",
      "test epoch: 4514, loss: 0.55\n",
      "train epoch: 4515, loss 0.0716\n",
      "test epoch: 4515, loss: 0.53\n",
      "train epoch: 4516, loss 0.0719\n",
      "test epoch: 4516, loss: 0.56\n",
      "train epoch: 4517, loss 0.0720\n",
      "test epoch: 4517, loss: 0.52\n",
      "train epoch: 4518, loss 0.0720\n",
      "test epoch: 4518, loss: 0.56\n",
      "train epoch: 4519, loss 0.0719\n",
      "test epoch: 4519, loss: 0.52\n",
      "train epoch: 4520, loss 0.0720\n",
      "test epoch: 4520, loss: 0.56\n",
      "train epoch: 4521, loss 0.0719\n",
      "test epoch: 4521, loss: 0.53\n",
      "train epoch: 4522, loss 0.0718\n",
      "test epoch: 4522, loss: 0.56\n",
      "train epoch: 4523, loss 0.0719\n",
      "test epoch: 4523, loss: 0.53\n",
      "train epoch: 4524, loss 0.0718\n",
      "test epoch: 4524, loss: 0.55\n",
      "train epoch: 4525, loss 0.0715\n",
      "test epoch: 4525, loss: 0.53\n",
      "train epoch: 4526, loss 0.0712\n",
      "test epoch: 4526, loss: 0.54\n",
      "train epoch: 4527, loss 0.0711\n",
      "test epoch: 4527, loss: 0.55\n",
      "train epoch: 4528, loss 0.0713\n",
      "test epoch: 4528, loss: 0.52\n",
      "train epoch: 4529, loss 0.0723\n",
      "test epoch: 4529, loss: 0.57\n",
      "train epoch: 4530, loss 0.0733\n",
      "test epoch: 4530, loss: 0.51\n",
      "train epoch: 4531, loss 0.0744\n",
      "test epoch: 4531, loss: 0.58\n",
      "train epoch: 4532, loss 0.0750\n",
      "test epoch: 4532, loss: 0.51\n",
      "train epoch: 4533, loss 0.0749\n",
      "test epoch: 4533, loss: 0.58\n",
      "train epoch: 4534, loss 0.0747\n",
      "test epoch: 4534, loss: 0.51\n",
      "train epoch: 4535, loss 0.0737\n",
      "test epoch: 4535, loss: 0.56\n",
      "train epoch: 4536, loss 0.0723\n",
      "test epoch: 4536, loss: 0.53\n",
      "train epoch: 4537, loss 0.0714\n",
      "test epoch: 4537, loss: 0.54\n",
      "train epoch: 4538, loss 0.0710\n",
      "test epoch: 4538, loss: 0.54\n",
      "train epoch: 4539, loss 0.0711\n",
      "test epoch: 4539, loss: 0.52\n",
      "train epoch: 4540, loss 0.0718\n",
      "test epoch: 4540, loss: 0.57\n",
      "train epoch: 4541, loss 0.0736\n",
      "test epoch: 4541, loss: 0.51\n",
      "train epoch: 4542, loss 0.0754\n",
      "test epoch: 4542, loss: 0.59\n",
      "train epoch: 4543, loss 0.0773\n",
      "test epoch: 4543, loss: 0.49\n",
      "train epoch: 4544, loss 0.0790\n",
      "test epoch: 4544, loss: 0.60\n",
      "train epoch: 4545, loss 0.0793\n",
      "test epoch: 4545, loss: 0.49\n",
      "train epoch: 4546, loss 0.0781\n",
      "test epoch: 4546, loss: 0.58\n",
      "train epoch: 4547, loss 0.0756\n",
      "test epoch: 4547, loss: 0.51\n",
      "train epoch: 4548, loss 0.0733\n",
      "test epoch: 4548, loss: 0.55\n",
      "train epoch: 4549, loss 0.0716\n",
      "test epoch: 4549, loss: 0.53\n",
      "train epoch: 4550, loss 0.0708\n",
      "test epoch: 4550, loss: 0.53\n",
      "train epoch: 4551, loss 0.0710\n",
      "test epoch: 4551, loss: 0.57\n",
      "train epoch: 4552, loss 0.0724\n",
      "test epoch: 4552, loss: 0.51\n",
      "train epoch: 4553, loss 0.0744\n",
      "test epoch: 4553, loss: 0.59\n",
      "train epoch: 4554, loss 0.0767\n",
      "test epoch: 4554, loss: 0.50\n",
      "train epoch: 4555, loss 0.0778\n",
      "test epoch: 4555, loss: 0.60\n",
      "train epoch: 4556, loss 0.0777\n",
      "test epoch: 4556, loss: 0.50\n",
      "train epoch: 4557, loss 0.0762\n",
      "test epoch: 4557, loss: 0.58\n",
      "train epoch: 4558, loss 0.0743\n",
      "test epoch: 4558, loss: 0.52\n",
      "train epoch: 4559, loss 0.0725\n",
      "test epoch: 4559, loss: 0.55\n",
      "train epoch: 4560, loss 0.0714\n",
      "test epoch: 4560, loss: 0.53\n",
      "train epoch: 4561, loss 0.0708\n",
      "test epoch: 4561, loss: 0.53\n",
      "train epoch: 4562, loss 0.0709\n",
      "test epoch: 4562, loss: 0.56\n",
      "train epoch: 4563, loss 0.0717\n",
      "test epoch: 4563, loss: 0.51\n",
      "train epoch: 4564, loss 0.0732\n",
      "test epoch: 4564, loss: 0.58\n",
      "train epoch: 4565, loss 0.0750\n",
      "test epoch: 4565, loss: 0.50\n",
      "train epoch: 4566, loss 0.0760\n",
      "test epoch: 4566, loss: 0.59\n",
      "train epoch: 4567, loss 0.0762\n",
      "test epoch: 4567, loss: 0.51\n",
      "train epoch: 4568, loss 0.0752\n",
      "test epoch: 4568, loss: 0.58\n",
      "train epoch: 4569, loss 0.0740\n",
      "test epoch: 4569, loss: 0.52\n",
      "train epoch: 4570, loss 0.0724\n",
      "test epoch: 4570, loss: 0.56\n",
      "train epoch: 4571, loss 0.0715\n",
      "test epoch: 4571, loss: 0.53\n",
      "train epoch: 4572, loss 0.0709\n",
      "test epoch: 4572, loss: 0.54\n",
      "train epoch: 4573, loss 0.0706\n",
      "test epoch: 4573, loss: 0.55\n",
      "train epoch: 4574, loss 0.0711\n",
      "test epoch: 4574, loss: 0.51\n",
      "train epoch: 4575, loss 0.0724\n",
      "test epoch: 4575, loss: 0.58\n",
      "train epoch: 4576, loss 0.0741\n",
      "test epoch: 4576, loss: 0.50\n",
      "train epoch: 4577, loss 0.0755\n",
      "test epoch: 4577, loss: 0.59\n",
      "train epoch: 4578, loss 0.0762\n",
      "test epoch: 4578, loss: 0.50\n",
      "train epoch: 4579, loss 0.0758\n",
      "test epoch: 4579, loss: 0.58\n",
      "train epoch: 4580, loss 0.0749\n",
      "test epoch: 4580, loss: 0.51\n",
      "train epoch: 4581, loss 0.0733\n",
      "test epoch: 4581, loss: 0.57\n",
      "train epoch: 4582, loss 0.0721\n",
      "test epoch: 4582, loss: 0.53\n",
      "train epoch: 4583, loss 0.0711\n",
      "test epoch: 4583, loss: 0.55\n",
      "train epoch: 4584, loss 0.0708\n",
      "test epoch: 4584, loss: 0.54\n",
      "train epoch: 4585, loss 0.0706\n",
      "test epoch: 4585, loss: 0.54\n",
      "train epoch: 4586, loss 0.0705\n",
      "test epoch: 4586, loss: 0.54\n",
      "train epoch: 4587, loss 0.0706\n",
      "test epoch: 4587, loss: 0.54\n",
      "train epoch: 4588, loss 0.0706\n",
      "test epoch: 4588, loss: 0.55\n",
      "train epoch: 4589, loss 0.0706\n",
      "test epoch: 4589, loss: 0.54\n",
      "train epoch: 4590, loss 0.0707\n",
      "test epoch: 4590, loss: 0.55\n",
      "train epoch: 4591, loss 0.0708\n",
      "test epoch: 4591, loss: 0.53\n",
      "train epoch: 4592, loss 0.0707\n",
      "test epoch: 4592, loss: 0.55\n",
      "train epoch: 4593, loss 0.0706\n",
      "test epoch: 4593, loss: 0.53\n",
      "train epoch: 4594, loss 0.0707\n",
      "test epoch: 4594, loss: 0.55\n",
      "train epoch: 4595, loss 0.0707\n",
      "test epoch: 4595, loss: 0.54\n",
      "train epoch: 4596, loss 0.0704\n",
      "test epoch: 4596, loss: 0.56\n",
      "train epoch: 4597, loss 0.0706\n",
      "test epoch: 4597, loss: 0.55\n",
      "train epoch: 4598, loss 0.0708\n",
      "test epoch: 4598, loss: 0.56\n",
      "train epoch: 4599, loss 0.0708\n",
      "test epoch: 4599, loss: 0.54\n",
      "train epoch: 4600, loss 0.0706\n",
      "test epoch: 4600, loss: 0.55\n",
      "train epoch: 4601, loss 0.0704\n",
      "test epoch: 4601, loss: 0.53\n",
      "train epoch: 4602, loss 0.0707\n",
      "test epoch: 4602, loss: 0.54\n",
      "train epoch: 4603, loss 0.0708\n",
      "test epoch: 4603, loss: 0.53\n",
      "train epoch: 4604, loss 0.0706\n",
      "test epoch: 4604, loss: 0.55\n",
      "train epoch: 4605, loss 0.0705\n",
      "test epoch: 4605, loss: 0.54\n",
      "train epoch: 4606, loss 0.0706\n",
      "test epoch: 4606, loss: 0.56\n",
      "train epoch: 4607, loss 0.0707\n",
      "test epoch: 4607, loss: 0.54\n",
      "train epoch: 4608, loss 0.0706\n",
      "test epoch: 4608, loss: 0.55\n",
      "train epoch: 4609, loss 0.0704\n",
      "test epoch: 4609, loss: 0.53\n",
      "train epoch: 4610, loss 0.0706\n",
      "test epoch: 4610, loss: 0.55\n",
      "train epoch: 4611, loss 0.0707\n",
      "test epoch: 4611, loss: 0.53\n",
      "train epoch: 4612, loss 0.0705\n",
      "test epoch: 4612, loss: 0.55\n",
      "train epoch: 4613, loss 0.0706\n",
      "test epoch: 4613, loss: 0.53\n",
      "train epoch: 4614, loss 0.0708\n",
      "test epoch: 4614, loss: 0.56\n",
      "train epoch: 4615, loss 0.0709\n",
      "test epoch: 4615, loss: 0.53\n",
      "train epoch: 4616, loss 0.0708\n",
      "test epoch: 4616, loss: 0.55\n",
      "train epoch: 4617, loss 0.0707\n",
      "test epoch: 4617, loss: 0.52\n",
      "train epoch: 4618, loss 0.0710\n",
      "test epoch: 4618, loss: 0.55\n",
      "train epoch: 4619, loss 0.0712\n",
      "test epoch: 4619, loss: 0.52\n",
      "train epoch: 4620, loss 0.0710\n",
      "test epoch: 4620, loss: 0.56\n",
      "train epoch: 4621, loss 0.0711\n",
      "test epoch: 4621, loss: 0.53\n",
      "train epoch: 4622, loss 0.0715\n",
      "test epoch: 4622, loss: 0.57\n",
      "train epoch: 4623, loss 0.0716\n",
      "test epoch: 4623, loss: 0.52\n",
      "train epoch: 4624, loss 0.0717\n",
      "test epoch: 4624, loss: 0.57\n",
      "train epoch: 4625, loss 0.0717\n",
      "test epoch: 4625, loss: 0.51\n",
      "train epoch: 4626, loss 0.0723\n",
      "test epoch: 4626, loss: 0.57\n",
      "train epoch: 4627, loss 0.0728\n",
      "test epoch: 4627, loss: 0.51\n",
      "train epoch: 4628, loss 0.0727\n",
      "test epoch: 4628, loss: 0.57\n",
      "train epoch: 4629, loss 0.0727\n",
      "test epoch: 4629, loss: 0.51\n",
      "train epoch: 4630, loss 0.0731\n",
      "test epoch: 4630, loss: 0.58\n",
      "train epoch: 4631, loss 0.0735\n",
      "test epoch: 4631, loss: 0.51\n",
      "train epoch: 4632, loss 0.0739\n",
      "test epoch: 4632, loss: 0.59\n",
      "train epoch: 4633, loss 0.0756\n",
      "test epoch: 4633, loss: 0.49\n",
      "train epoch: 4634, loss 0.0776\n",
      "test epoch: 4634, loss: 0.60\n",
      "train epoch: 4635, loss 0.0802\n",
      "test epoch: 4635, loss: 0.47\n",
      "train epoch: 4636, loss 0.0836\n",
      "test epoch: 4636, loss: 0.62\n",
      "train epoch: 4637, loss 0.0862\n",
      "test epoch: 4637, loss: 0.46\n",
      "train epoch: 4638, loss 0.0890\n",
      "test epoch: 4638, loss: 0.63\n",
      "train epoch: 4639, loss 0.0901\n",
      "test epoch: 4639, loss: 0.46\n",
      "train epoch: 4640, loss 0.0897\n",
      "test epoch: 4640, loss: 0.62\n",
      "train epoch: 4641, loss 0.0866\n",
      "test epoch: 4641, loss: 0.47\n",
      "train epoch: 4642, loss 0.0836\n",
      "test epoch: 4642, loss: 0.59\n",
      "train epoch: 4643, loss 0.0784\n",
      "test epoch: 4643, loss: 0.50\n",
      "train epoch: 4644, loss 0.0739\n",
      "test epoch: 4644, loss: 0.55\n",
      "train epoch: 4645, loss 0.0707\n",
      "test epoch: 4645, loss: 0.54\n",
      "train epoch: 4646, loss 0.0699\n",
      "test epoch: 4646, loss: 0.52\n",
      "train epoch: 4647, loss 0.0704\n",
      "test epoch: 4647, loss: 0.57\n",
      "train epoch: 4648, loss 0.0721\n",
      "test epoch: 4648, loss: 0.51\n",
      "train epoch: 4649, loss 0.0737\n",
      "test epoch: 4649, loss: 0.58\n",
      "train epoch: 4650, loss 0.0742\n",
      "test epoch: 4650, loss: 0.50\n",
      "train epoch: 4651, loss 0.0736\n",
      "test epoch: 4651, loss: 0.57\n",
      "train epoch: 4652, loss 0.0727\n",
      "test epoch: 4652, loss: 0.51\n",
      "train epoch: 4653, loss 0.0717\n",
      "test epoch: 4653, loss: 0.55\n",
      "train epoch: 4654, loss 0.0707\n",
      "test epoch: 4654, loss: 0.52\n",
      "train epoch: 4655, loss 0.0703\n",
      "test epoch: 4655, loss: 0.54\n",
      "train epoch: 4656, loss 0.0700\n",
      "test epoch: 4656, loss: 0.53\n",
      "train epoch: 4657, loss 0.0699\n",
      "test epoch: 4657, loss: 0.53\n",
      "train epoch: 4658, loss 0.0698\n",
      "test epoch: 4658, loss: 0.54\n",
      "train epoch: 4659, loss 0.0699\n",
      "test epoch: 4659, loss: 0.52\n",
      "train epoch: 4660, loss 0.0701\n",
      "test epoch: 4660, loss: 0.55\n",
      "train epoch: 4661, loss 0.0704\n",
      "test epoch: 4661, loss: 0.51\n",
      "train epoch: 4662, loss 0.0710\n",
      "test epoch: 4662, loss: 0.56\n",
      "train epoch: 4663, loss 0.0717\n",
      "test epoch: 4663, loss: 0.51\n",
      "train epoch: 4664, loss 0.0724\n",
      "test epoch: 4664, loss: 0.57\n",
      "train epoch: 4665, loss 0.0730\n",
      "test epoch: 4665, loss: 0.50\n",
      "train epoch: 4666, loss 0.0729\n",
      "test epoch: 4666, loss: 0.57\n",
      "train epoch: 4667, loss 0.0729\n",
      "test epoch: 4667, loss: 0.51\n",
      "train epoch: 4668, loss 0.0722\n",
      "test epoch: 4668, loss: 0.56\n",
      "train epoch: 4669, loss 0.0719\n",
      "test epoch: 4669, loss: 0.51\n",
      "train epoch: 4670, loss 0.0711\n",
      "test epoch: 4670, loss: 0.55\n",
      "train epoch: 4671, loss 0.0708\n",
      "test epoch: 4671, loss: 0.52\n",
      "train epoch: 4672, loss 0.0703\n",
      "test epoch: 4672, loss: 0.55\n",
      "train epoch: 4673, loss 0.0703\n",
      "test epoch: 4673, loss: 0.52\n",
      "train epoch: 4674, loss 0.0702\n",
      "test epoch: 4674, loss: 0.55\n",
      "train epoch: 4675, loss 0.0704\n",
      "test epoch: 4675, loss: 0.52\n",
      "train epoch: 4676, loss 0.0705\n",
      "test epoch: 4676, loss: 0.56\n",
      "train epoch: 4677, loss 0.0711\n",
      "test epoch: 4677, loss: 0.51\n",
      "train epoch: 4678, loss 0.0714\n",
      "test epoch: 4678, loss: 0.57\n",
      "train epoch: 4679, loss 0.0721\n",
      "test epoch: 4679, loss: 0.51\n",
      "train epoch: 4680, loss 0.0724\n",
      "test epoch: 4680, loss: 0.57\n",
      "train epoch: 4681, loss 0.0728\n",
      "test epoch: 4681, loss: 0.50\n",
      "train epoch: 4682, loss 0.0726\n",
      "test epoch: 4682, loss: 0.57\n",
      "train epoch: 4683, loss 0.0725\n",
      "test epoch: 4683, loss: 0.51\n",
      "train epoch: 4684, loss 0.0719\n",
      "test epoch: 4684, loss: 0.56\n",
      "train epoch: 4685, loss 0.0716\n",
      "test epoch: 4685, loss: 0.51\n",
      "train epoch: 4686, loss 0.0710\n",
      "test epoch: 4686, loss: 0.55\n",
      "train epoch: 4687, loss 0.0708\n",
      "test epoch: 4687, loss: 0.51\n",
      "train epoch: 4688, loss 0.0705\n",
      "test epoch: 4688, loss: 0.55\n",
      "train epoch: 4689, loss 0.0707\n",
      "test epoch: 4689, loss: 0.52\n",
      "train epoch: 4690, loss 0.0706\n",
      "test epoch: 4690, loss: 0.56\n",
      "train epoch: 4691, loss 0.0710\n",
      "test epoch: 4691, loss: 0.51\n",
      "train epoch: 4692, loss 0.0710\n",
      "test epoch: 4692, loss: 0.56\n",
      "train epoch: 4693, loss 0.0716\n",
      "test epoch: 4693, loss: 0.51\n",
      "train epoch: 4694, loss 0.0718\n",
      "test epoch: 4694, loss: 0.57\n",
      "train epoch: 4695, loss 0.0722\n",
      "test epoch: 4695, loss: 0.50\n",
      "train epoch: 4696, loss 0.0724\n",
      "test epoch: 4696, loss: 0.56\n",
      "train epoch: 4697, loss 0.0724\n",
      "test epoch: 4697, loss: 0.50\n",
      "train epoch: 4698, loss 0.0722\n",
      "test epoch: 4698, loss: 0.56\n",
      "train epoch: 4699, loss 0.0721\n",
      "test epoch: 4699, loss: 0.51\n",
      "train epoch: 4700, loss 0.0718\n",
      "test epoch: 4700, loss: 0.56\n",
      "train epoch: 4701, loss 0.0717\n",
      "test epoch: 4701, loss: 0.51\n",
      "train epoch: 4702, loss 0.0714\n",
      "test epoch: 4702, loss: 0.56\n",
      "train epoch: 4703, loss 0.0715\n",
      "test epoch: 4703, loss: 0.51\n",
      "train epoch: 4704, loss 0.0713\n",
      "test epoch: 4704, loss: 0.56\n",
      "train epoch: 4705, loss 0.0715\n",
      "test epoch: 4705, loss: 0.51\n",
      "train epoch: 4706, loss 0.0714\n",
      "test epoch: 4706, loss: 0.56\n",
      "train epoch: 4707, loss 0.0717\n",
      "test epoch: 4707, loss: 0.51\n",
      "train epoch: 4708, loss 0.0716\n",
      "test epoch: 4708, loss: 0.56\n",
      "train epoch: 4709, loss 0.0718\n",
      "test epoch: 4709, loss: 0.51\n",
      "train epoch: 4710, loss 0.0718\n",
      "test epoch: 4710, loss: 0.57\n",
      "train epoch: 4711, loss 0.0720\n",
      "test epoch: 4711, loss: 0.51\n",
      "train epoch: 4712, loss 0.0720\n",
      "test epoch: 4712, loss: 0.57\n",
      "train epoch: 4713, loss 0.0722\n",
      "test epoch: 4713, loss: 0.51\n",
      "train epoch: 4714, loss 0.0721\n",
      "test epoch: 4714, loss: 0.57\n",
      "train epoch: 4715, loss 0.0722\n",
      "test epoch: 4715, loss: 0.51\n",
      "train epoch: 4716, loss 0.0720\n",
      "test epoch: 4716, loss: 0.57\n",
      "train epoch: 4717, loss 0.0719\n",
      "test epoch: 4717, loss: 0.51\n",
      "train epoch: 4718, loss 0.0717\n",
      "test epoch: 4718, loss: 0.57\n",
      "train epoch: 4719, loss 0.0715\n",
      "test epoch: 4719, loss: 0.51\n",
      "train epoch: 4720, loss 0.0713\n",
      "test epoch: 4720, loss: 0.56\n",
      "train epoch: 4721, loss 0.0713\n",
      "test epoch: 4721, loss: 0.51\n",
      "train epoch: 4722, loss 0.0712\n",
      "test epoch: 4722, loss: 0.56\n",
      "train epoch: 4723, loss 0.0714\n",
      "test epoch: 4723, loss: 0.51\n",
      "train epoch: 4724, loss 0.0714\n",
      "test epoch: 4724, loss: 0.56\n",
      "train epoch: 4725, loss 0.0716\n",
      "test epoch: 4725, loss: 0.51\n",
      "train epoch: 4726, loss 0.0716\n",
      "test epoch: 4726, loss: 0.57\n",
      "train epoch: 4727, loss 0.0718\n",
      "test epoch: 4727, loss: 0.50\n",
      "train epoch: 4728, loss 0.0720\n",
      "test epoch: 4728, loss: 0.57\n",
      "train epoch: 4729, loss 0.0722\n",
      "test epoch: 4729, loss: 0.50\n",
      "train epoch: 4730, loss 0.0722\n",
      "test epoch: 4730, loss: 0.57\n",
      "train epoch: 4731, loss 0.0722\n",
      "test epoch: 4731, loss: 0.50\n",
      "train epoch: 4732, loss 0.0720\n",
      "test epoch: 4732, loss: 0.56\n",
      "train epoch: 4733, loss 0.0719\n",
      "test epoch: 4733, loss: 0.50\n",
      "train epoch: 4734, loss 0.0717\n",
      "test epoch: 4734, loss: 0.56\n",
      "train epoch: 4735, loss 0.0717\n",
      "test epoch: 4735, loss: 0.50\n",
      "train epoch: 4736, loss 0.0716\n",
      "test epoch: 4736, loss: 0.56\n",
      "train epoch: 4737, loss 0.0717\n",
      "test epoch: 4737, loss: 0.51\n",
      "train epoch: 4738, loss 0.0716\n",
      "test epoch: 4738, loss: 0.57\n",
      "train epoch: 4739, loss 0.0717\n",
      "test epoch: 4739, loss: 0.51\n",
      "train epoch: 4740, loss 0.0716\n",
      "test epoch: 4740, loss: 0.57\n",
      "train epoch: 4741, loss 0.0717\n",
      "test epoch: 4741, loss: 0.51\n",
      "train epoch: 4742, loss 0.0717\n",
      "test epoch: 4742, loss: 0.57\n",
      "train epoch: 4743, loss 0.0719\n",
      "test epoch: 4743, loss: 0.51\n",
      "train epoch: 4744, loss 0.0719\n",
      "test epoch: 4744, loss: 0.57\n",
      "train epoch: 4745, loss 0.0720\n",
      "test epoch: 4745, loss: 0.51\n",
      "train epoch: 4746, loss 0.0719\n",
      "test epoch: 4746, loss: 0.57\n",
      "train epoch: 4747, loss 0.0720\n",
      "test epoch: 4747, loss: 0.51\n",
      "train epoch: 4748, loss 0.0719\n",
      "test epoch: 4748, loss: 0.57\n",
      "train epoch: 4749, loss 0.0717\n",
      "test epoch: 4749, loss: 0.51\n",
      "train epoch: 4750, loss 0.0715\n",
      "test epoch: 4750, loss: 0.57\n",
      "train epoch: 4751, loss 0.0717\n",
      "test epoch: 4751, loss: 0.51\n",
      "train epoch: 4752, loss 0.0718\n",
      "test epoch: 4752, loss: 0.57\n",
      "train epoch: 4753, loss 0.0724\n",
      "test epoch: 4753, loss: 0.50\n",
      "train epoch: 4754, loss 0.0727\n",
      "test epoch: 4754, loss: 0.57\n",
      "train epoch: 4755, loss 0.0729\n",
      "test epoch: 4755, loss: 0.49\n",
      "train epoch: 4756, loss 0.0728\n",
      "test epoch: 4756, loss: 0.56\n",
      "train epoch: 4757, loss 0.0726\n",
      "test epoch: 4757, loss: 0.49\n",
      "train epoch: 4758, loss 0.0721\n",
      "test epoch: 4758, loss: 0.56\n",
      "train epoch: 4759, loss 0.0716\n",
      "test epoch: 4759, loss: 0.50\n",
      "train epoch: 4760, loss 0.0711\n",
      "test epoch: 4760, loss: 0.55\n",
      "train epoch: 4761, loss 0.0708\n",
      "test epoch: 4761, loss: 0.50\n",
      "train epoch: 4762, loss 0.0705\n",
      "test epoch: 4762, loss: 0.55\n",
      "train epoch: 4763, loss 0.0705\n",
      "test epoch: 4763, loss: 0.50\n",
      "train epoch: 4764, loss 0.0704\n",
      "test epoch: 4764, loss: 0.55\n",
      "train epoch: 4765, loss 0.0708\n",
      "test epoch: 4765, loss: 0.50\n",
      "train epoch: 4766, loss 0.0709\n",
      "test epoch: 4766, loss: 0.55\n",
      "train epoch: 4767, loss 0.0714\n",
      "test epoch: 4767, loss: 0.49\n",
      "train epoch: 4768, loss 0.0717\n",
      "test epoch: 4768, loss: 0.55\n",
      "train epoch: 4769, loss 0.0719\n",
      "test epoch: 4769, loss: 0.49\n",
      "train epoch: 4770, loss 0.0721\n",
      "test epoch: 4770, loss: 0.55\n",
      "train epoch: 4771, loss 0.0720\n",
      "test epoch: 4771, loss: 0.49\n",
      "train epoch: 4772, loss 0.0718\n",
      "test epoch: 4772, loss: 0.55\n",
      "train epoch: 4773, loss 0.0716\n",
      "test epoch: 4773, loss: 0.49\n",
      "train epoch: 4774, loss 0.0712\n",
      "test epoch: 4774, loss: 0.55\n",
      "train epoch: 4775, loss 0.0711\n",
      "test epoch: 4775, loss: 0.50\n",
      "train epoch: 4776, loss 0.0708\n",
      "test epoch: 4776, loss: 0.55\n",
      "train epoch: 4777, loss 0.0708\n",
      "test epoch: 4777, loss: 0.50\n",
      "train epoch: 4778, loss 0.0707\n",
      "test epoch: 4778, loss: 0.55\n",
      "train epoch: 4779, loss 0.0709\n",
      "test epoch: 4779, loss: 0.49\n",
      "train epoch: 4780, loss 0.0709\n",
      "test epoch: 4780, loss: 0.55\n",
      "train epoch: 4781, loss 0.0710\n",
      "test epoch: 4781, loss: 0.49\n",
      "train epoch: 4782, loss 0.0710\n",
      "test epoch: 4782, loss: 0.55\n",
      "train epoch: 4783, loss 0.0711\n",
      "test epoch: 4783, loss: 0.49\n",
      "train epoch: 4784, loss 0.0710\n",
      "test epoch: 4784, loss: 0.55\n",
      "train epoch: 4785, loss 0.0711\n",
      "test epoch: 4785, loss: 0.49\n",
      "train epoch: 4786, loss 0.0710\n",
      "test epoch: 4786, loss: 0.55\n",
      "train epoch: 4787, loss 0.0711\n",
      "test epoch: 4787, loss: 0.49\n",
      "train epoch: 4788, loss 0.0710\n",
      "test epoch: 4788, loss: 0.55\n",
      "train epoch: 4789, loss 0.0712\n",
      "test epoch: 4789, loss: 0.49\n",
      "train epoch: 4790, loss 0.0711\n",
      "test epoch: 4790, loss: 0.55\n",
      "train epoch: 4791, loss 0.0713\n",
      "test epoch: 4791, loss: 0.49\n",
      "train epoch: 4792, loss 0.0714\n",
      "test epoch: 4792, loss: 0.55\n",
      "train epoch: 4793, loss 0.0716\n",
      "test epoch: 4793, loss: 0.48\n",
      "train epoch: 4794, loss 0.0717\n",
      "test epoch: 4794, loss: 0.55\n",
      "train epoch: 4795, loss 0.0718\n",
      "test epoch: 4795, loss: 0.48\n",
      "train epoch: 4796, loss 0.0718\n",
      "test epoch: 4796, loss: 0.55\n",
      "train epoch: 4797, loss 0.0717\n",
      "test epoch: 4797, loss: 0.49\n",
      "train epoch: 4798, loss 0.0714\n",
      "test epoch: 4798, loss: 0.55\n",
      "train epoch: 4799, loss 0.0712\n",
      "test epoch: 4799, loss: 0.49\n",
      "train epoch: 4800, loss 0.0709\n",
      "test epoch: 4800, loss: 0.55\n",
      "train epoch: 4801, loss 0.0709\n",
      "test epoch: 4801, loss: 0.49\n",
      "train epoch: 4802, loss 0.0706\n",
      "test epoch: 4802, loss: 0.55\n",
      "train epoch: 4803, loss 0.0707\n",
      "test epoch: 4803, loss: 0.49\n",
      "train epoch: 4804, loss 0.0706\n",
      "test epoch: 4804, loss: 0.55\n",
      "train epoch: 4805, loss 0.0708\n",
      "test epoch: 4805, loss: 0.49\n",
      "train epoch: 4806, loss 0.0709\n",
      "test epoch: 4806, loss: 0.55\n",
      "train epoch: 4807, loss 0.0711\n",
      "test epoch: 4807, loss: 0.49\n",
      "train epoch: 4808, loss 0.0712\n",
      "test epoch: 4808, loss: 0.55\n",
      "train epoch: 4809, loss 0.0713\n",
      "test epoch: 4809, loss: 0.49\n",
      "train epoch: 4810, loss 0.0712\n",
      "test epoch: 4810, loss: 0.55\n",
      "train epoch: 4811, loss 0.0713\n",
      "test epoch: 4811, loss: 0.49\n",
      "train epoch: 4812, loss 0.0710\n",
      "test epoch: 4812, loss: 0.55\n",
      "train epoch: 4813, loss 0.0710\n",
      "test epoch: 4813, loss: 0.49\n",
      "train epoch: 4814, loss 0.0708\n",
      "test epoch: 4814, loss: 0.55\n",
      "train epoch: 4815, loss 0.0708\n",
      "test epoch: 4815, loss: 0.49\n",
      "train epoch: 4816, loss 0.0706\n",
      "test epoch: 4816, loss: 0.55\n",
      "train epoch: 4817, loss 0.0708\n",
      "test epoch: 4817, loss: 0.49\n",
      "train epoch: 4818, loss 0.0707\n",
      "test epoch: 4818, loss: 0.55\n",
      "train epoch: 4819, loss 0.0709\n",
      "test epoch: 4819, loss: 0.49\n",
      "train epoch: 4820, loss 0.0709\n",
      "test epoch: 4820, loss: 0.55\n",
      "train epoch: 4821, loss 0.0711\n",
      "test epoch: 4821, loss: 0.49\n",
      "train epoch: 4822, loss 0.0711\n",
      "test epoch: 4822, loss: 0.55\n",
      "train epoch: 4823, loss 0.0712\n",
      "test epoch: 4823, loss: 0.49\n",
      "train epoch: 4824, loss 0.0714\n",
      "test epoch: 4824, loss: 0.55\n",
      "train epoch: 4825, loss 0.0713\n",
      "test epoch: 4825, loss: 0.49\n",
      "train epoch: 4826, loss 0.0713\n",
      "test epoch: 4826, loss: 0.56\n",
      "train epoch: 4827, loss 0.0714\n",
      "test epoch: 4827, loss: 0.49\n",
      "train epoch: 4828, loss 0.0713\n",
      "test epoch: 4828, loss: 0.55\n",
      "train epoch: 4829, loss 0.0711\n",
      "test epoch: 4829, loss: 0.49\n",
      "train epoch: 4830, loss 0.0709\n",
      "test epoch: 4830, loss: 0.55\n",
      "train epoch: 4831, loss 0.0707\n",
      "test epoch: 4831, loss: 0.50\n",
      "train epoch: 4832, loss 0.0705\n",
      "test epoch: 4832, loss: 0.55\n",
      "train epoch: 4833, loss 0.0703\n",
      "test epoch: 4833, loss: 0.50\n",
      "train epoch: 4834, loss 0.0701\n",
      "test epoch: 4834, loss: 0.55\n",
      "train epoch: 4835, loss 0.0702\n",
      "test epoch: 4835, loss: 0.50\n",
      "train epoch: 4836, loss 0.0701\n",
      "test epoch: 4836, loss: 0.55\n",
      "train epoch: 4837, loss 0.0703\n",
      "test epoch: 4837, loss: 0.50\n",
      "train epoch: 4838, loss 0.0704\n",
      "test epoch: 4838, loss: 0.55\n",
      "train epoch: 4839, loss 0.0707\n",
      "test epoch: 4839, loss: 0.49\n",
      "train epoch: 4840, loss 0.0709\n",
      "test epoch: 4840, loss: 0.56\n",
      "train epoch: 4841, loss 0.0712\n",
      "test epoch: 4841, loss: 0.49\n",
      "train epoch: 4842, loss 0.0714\n",
      "test epoch: 4842, loss: 0.56\n",
      "train epoch: 4843, loss 0.0715\n",
      "test epoch: 4843, loss: 0.49\n",
      "train epoch: 4844, loss 0.0713\n",
      "test epoch: 4844, loss: 0.55\n",
      "train epoch: 4845, loss 0.0711\n",
      "test epoch: 4845, loss: 0.49\n",
      "train epoch: 4846, loss 0.0710\n",
      "test epoch: 4846, loss: 0.55\n",
      "train epoch: 4847, loss 0.0708\n",
      "test epoch: 4847, loss: 0.49\n",
      "train epoch: 4848, loss 0.0705\n",
      "test epoch: 4848, loss: 0.55\n",
      "train epoch: 4849, loss 0.0705\n",
      "test epoch: 4849, loss: 0.50\n",
      "train epoch: 4850, loss 0.0705\n",
      "test epoch: 4850, loss: 0.55\n",
      "train epoch: 4851, loss 0.0705\n",
      "test epoch: 4851, loss: 0.49\n",
      "train epoch: 4852, loss 0.0705\n",
      "test epoch: 4852, loss: 0.55\n",
      "train epoch: 4853, loss 0.0705\n",
      "test epoch: 4853, loss: 0.49\n",
      "train epoch: 4854, loss 0.0706\n",
      "test epoch: 4854, loss: 0.55\n",
      "train epoch: 4855, loss 0.0707\n",
      "test epoch: 4855, loss: 0.49\n",
      "train epoch: 4856, loss 0.0708\n",
      "test epoch: 4856, loss: 0.55\n",
      "train epoch: 4857, loss 0.0707\n",
      "test epoch: 4857, loss: 0.49\n",
      "train epoch: 4858, loss 0.0710\n",
      "test epoch: 4858, loss: 0.56\n",
      "train epoch: 4859, loss 0.0708\n",
      "test epoch: 4859, loss: 0.50\n",
      "train epoch: 4860, loss 0.0711\n",
      "test epoch: 4860, loss: 0.56\n",
      "train epoch: 4861, loss 0.0710\n",
      "test epoch: 4861, loss: 0.50\n",
      "train epoch: 4862, loss 0.0710\n",
      "test epoch: 4862, loss: 0.56\n",
      "train epoch: 4863, loss 0.0708\n",
      "test epoch: 4863, loss: 0.50\n",
      "train epoch: 4864, loss 0.0708\n",
      "test epoch: 4864, loss: 0.56\n",
      "train epoch: 4865, loss 0.0705\n",
      "test epoch: 4865, loss: 0.50\n",
      "train epoch: 4866, loss 0.0703\n",
      "test epoch: 4866, loss: 0.55\n",
      "train epoch: 4867, loss 0.0701\n",
      "test epoch: 4867, loss: 0.50\n",
      "train epoch: 4868, loss 0.0700\n",
      "test epoch: 4868, loss: 0.55\n",
      "train epoch: 4869, loss 0.0699\n",
      "test epoch: 4869, loss: 0.50\n",
      "train epoch: 4870, loss 0.0699\n",
      "test epoch: 4870, loss: 0.56\n",
      "train epoch: 4871, loss 0.0700\n",
      "test epoch: 4871, loss: 0.50\n",
      "train epoch: 4872, loss 0.0703\n",
      "test epoch: 4872, loss: 0.56\n",
      "train epoch: 4873, loss 0.0708\n",
      "test epoch: 4873, loss: 0.49\n",
      "train epoch: 4874, loss 0.0712\n",
      "test epoch: 4874, loss: 0.56\n",
      "train epoch: 4875, loss 0.0715\n",
      "test epoch: 4875, loss: 0.49\n",
      "train epoch: 4876, loss 0.0716\n",
      "test epoch: 4876, loss: 0.56\n",
      "train epoch: 4877, loss 0.0715\n",
      "test epoch: 4877, loss: 0.49\n",
      "train epoch: 4878, loss 0.0711\n",
      "test epoch: 4878, loss: 0.55\n",
      "train epoch: 4879, loss 0.0707\n",
      "test epoch: 4879, loss: 0.49\n",
      "train epoch: 4880, loss 0.0703\n",
      "test epoch: 4880, loss: 0.55\n",
      "train epoch: 4881, loss 0.0702\n",
      "test epoch: 4881, loss: 0.49\n",
      "train epoch: 4882, loss 0.0699\n",
      "test epoch: 4882, loss: 0.54\n",
      "train epoch: 4883, loss 0.0699\n",
      "test epoch: 4883, loss: 0.49\n",
      "train epoch: 4884, loss 0.0698\n",
      "test epoch: 4884, loss: 0.54\n",
      "train epoch: 4885, loss 0.0698\n",
      "test epoch: 4885, loss: 0.49\n",
      "train epoch: 4886, loss 0.0698\n",
      "test epoch: 4886, loss: 0.54\n",
      "train epoch: 4887, loss 0.0699\n",
      "test epoch: 4887, loss: 0.49\n",
      "train epoch: 4888, loss 0.0700\n",
      "test epoch: 4888, loss: 0.55\n",
      "train epoch: 4889, loss 0.0701\n",
      "test epoch: 4889, loss: 0.49\n",
      "train epoch: 4890, loss 0.0702\n",
      "test epoch: 4890, loss: 0.55\n",
      "train epoch: 4891, loss 0.0704\n",
      "test epoch: 4891, loss: 0.49\n",
      "train epoch: 4892, loss 0.0705\n",
      "test epoch: 4892, loss: 0.55\n",
      "train epoch: 4893, loss 0.0707\n",
      "test epoch: 4893, loss: 0.49\n",
      "train epoch: 4894, loss 0.0707\n",
      "test epoch: 4894, loss: 0.55\n",
      "train epoch: 4895, loss 0.0707\n",
      "test epoch: 4895, loss: 0.49\n",
      "train epoch: 4896, loss 0.0706\n",
      "test epoch: 4896, loss: 0.55\n",
      "train epoch: 4897, loss 0.0705\n",
      "test epoch: 4897, loss: 0.49\n",
      "train epoch: 4898, loss 0.0703\n",
      "test epoch: 4898, loss: 0.55\n",
      "train epoch: 4899, loss 0.0702\n",
      "test epoch: 4899, loss: 0.49\n",
      "train epoch: 4900, loss 0.0700\n",
      "test epoch: 4900, loss: 0.54\n",
      "train epoch: 4901, loss 0.0699\n",
      "test epoch: 4901, loss: 0.49\n",
      "train epoch: 4902, loss 0.0697\n",
      "test epoch: 4902, loss: 0.54\n",
      "train epoch: 4903, loss 0.0698\n",
      "test epoch: 4903, loss: 0.49\n",
      "train epoch: 4904, loss 0.0698\n",
      "test epoch: 4904, loss: 0.54\n",
      "train epoch: 4905, loss 0.0699\n",
      "test epoch: 4905, loss: 0.49\n",
      "train epoch: 4906, loss 0.0699\n",
      "test epoch: 4906, loss: 0.54\n",
      "train epoch: 4907, loss 0.0702\n",
      "test epoch: 4907, loss: 0.49\n",
      "train epoch: 4908, loss 0.0701\n",
      "test epoch: 4908, loss: 0.55\n",
      "train epoch: 4909, loss 0.0703\n",
      "test epoch: 4909, loss: 0.49\n",
      "train epoch: 4910, loss 0.0704\n",
      "test epoch: 4910, loss: 0.55\n",
      "train epoch: 4911, loss 0.0705\n",
      "test epoch: 4911, loss: 0.49\n",
      "train epoch: 4912, loss 0.0705\n",
      "test epoch: 4912, loss: 0.55\n",
      "train epoch: 4913, loss 0.0705\n",
      "test epoch: 4913, loss: 0.49\n",
      "train epoch: 4914, loss 0.0704\n",
      "test epoch: 4914, loss: 0.55\n",
      "train epoch: 4915, loss 0.0702\n",
      "test epoch: 4915, loss: 0.49\n",
      "train epoch: 4916, loss 0.0700\n",
      "test epoch: 4916, loss: 0.54\n",
      "train epoch: 4917, loss 0.0700\n",
      "test epoch: 4917, loss: 0.49\n",
      "train epoch: 4918, loss 0.0698\n",
      "test epoch: 4918, loss: 0.54\n",
      "train epoch: 4919, loss 0.0698\n",
      "test epoch: 4919, loss: 0.49\n",
      "train epoch: 4920, loss 0.0698\n",
      "test epoch: 4920, loss: 0.54\n",
      "train epoch: 4921, loss 0.0699\n",
      "test epoch: 4921, loss: 0.49\n",
      "train epoch: 4922, loss 0.0699\n",
      "test epoch: 4922, loss: 0.55\n",
      "train epoch: 4923, loss 0.0698\n",
      "test epoch: 4923, loss: 0.49\n",
      "train epoch: 4924, loss 0.0698\n",
      "test epoch: 4924, loss: 0.55\n",
      "train epoch: 4925, loss 0.0698\n",
      "test epoch: 4925, loss: 0.49\n",
      "train epoch: 4926, loss 0.0699\n",
      "test epoch: 4926, loss: 0.55\n",
      "train epoch: 4927, loss 0.0699\n",
      "test epoch: 4927, loss: 0.49\n",
      "train epoch: 4928, loss 0.0699\n",
      "test epoch: 4928, loss: 0.55\n",
      "train epoch: 4929, loss 0.0700\n",
      "test epoch: 4929, loss: 0.49\n",
      "train epoch: 4930, loss 0.0701\n",
      "test epoch: 4930, loss: 0.55\n",
      "train epoch: 4931, loss 0.0701\n",
      "test epoch: 4931, loss: 0.49\n",
      "train epoch: 4932, loss 0.0701\n",
      "test epoch: 4932, loss: 0.55\n",
      "train epoch: 4933, loss 0.0702\n",
      "test epoch: 4933, loss: 0.49\n",
      "train epoch: 4934, loss 0.0702\n",
      "test epoch: 4934, loss: 0.55\n",
      "train epoch: 4935, loss 0.0701\n",
      "test epoch: 4935, loss: 0.49\n",
      "train epoch: 4936, loss 0.0700\n",
      "test epoch: 4936, loss: 0.55\n",
      "train epoch: 4937, loss 0.0700\n",
      "test epoch: 4937, loss: 0.49\n",
      "train epoch: 4938, loss 0.0699\n",
      "test epoch: 4938, loss: 0.55\n",
      "train epoch: 4939, loss 0.0700\n",
      "test epoch: 4939, loss: 0.49\n",
      "train epoch: 4940, loss 0.0699\n",
      "test epoch: 4940, loss: 0.55\n",
      "train epoch: 4941, loss 0.0699\n",
      "test epoch: 4941, loss: 0.49\n",
      "train epoch: 4942, loss 0.0699\n",
      "test epoch: 4942, loss: 0.55\n",
      "train epoch: 4943, loss 0.0700\n",
      "test epoch: 4943, loss: 0.49\n",
      "train epoch: 4944, loss 0.0700\n",
      "test epoch: 4944, loss: 0.55\n",
      "train epoch: 4945, loss 0.0701\n",
      "test epoch: 4945, loss: 0.49\n",
      "train epoch: 4946, loss 0.0701\n",
      "test epoch: 4946, loss: 0.55\n",
      "train epoch: 4947, loss 0.0702\n",
      "test epoch: 4947, loss: 0.49\n",
      "train epoch: 4948, loss 0.0701\n",
      "test epoch: 4948, loss: 0.55\n",
      "train epoch: 4949, loss 0.0699\n",
      "test epoch: 4949, loss: 0.49\n",
      "train epoch: 4950, loss 0.0697\n",
      "test epoch: 4950, loss: 0.55\n",
      "train epoch: 4951, loss 0.0696\n",
      "test epoch: 4951, loss: 0.49\n",
      "train epoch: 4952, loss 0.0695\n",
      "test epoch: 4952, loss: 0.55\n",
      "train epoch: 4953, loss 0.0696\n",
      "test epoch: 4953, loss: 0.49\n",
      "train epoch: 4954, loss 0.0695\n",
      "test epoch: 4954, loss: 0.55\n",
      "train epoch: 4955, loss 0.0696\n",
      "test epoch: 4955, loss: 0.49\n",
      "train epoch: 4956, loss 0.0696\n",
      "test epoch: 4956, loss: 0.55\n",
      "train epoch: 4957, loss 0.0698\n",
      "test epoch: 4957, loss: 0.49\n",
      "train epoch: 4958, loss 0.0699\n",
      "test epoch: 4958, loss: 0.55\n",
      "train epoch: 4959, loss 0.0701\n",
      "test epoch: 4959, loss: 0.49\n",
      "train epoch: 4960, loss 0.0701\n",
      "test epoch: 4960, loss: 0.55\n",
      "train epoch: 4961, loss 0.0702\n",
      "test epoch: 4961, loss: 0.49\n",
      "train epoch: 4962, loss 0.0701\n",
      "test epoch: 4962, loss: 0.55\n",
      "train epoch: 4963, loss 0.0700\n",
      "test epoch: 4963, loss: 0.49\n",
      "train epoch: 4964, loss 0.0698\n",
      "test epoch: 4964, loss: 0.55\n",
      "train epoch: 4965, loss 0.0696\n",
      "test epoch: 4965, loss: 0.49\n",
      "train epoch: 4966, loss 0.0694\n",
      "test epoch: 4966, loss: 0.55\n",
      "train epoch: 4967, loss 0.0694\n",
      "test epoch: 4967, loss: 0.50\n",
      "train epoch: 4968, loss 0.0693\n",
      "test epoch: 4968, loss: 0.55\n",
      "train epoch: 4969, loss 0.0694\n",
      "test epoch: 4969, loss: 0.50\n",
      "train epoch: 4970, loss 0.0693\n",
      "test epoch: 4970, loss: 0.55\n",
      "train epoch: 4971, loss 0.0697\n",
      "test epoch: 4971, loss: 0.49\n",
      "train epoch: 4972, loss 0.0699\n",
      "test epoch: 4972, loss: 0.55\n",
      "train epoch: 4973, loss 0.0702\n",
      "test epoch: 4973, loss: 0.49\n",
      "train epoch: 4974, loss 0.0704\n",
      "test epoch: 4974, loss: 0.55\n",
      "train epoch: 4975, loss 0.0705\n",
      "test epoch: 4975, loss: 0.48\n",
      "train epoch: 4976, loss 0.0704\n",
      "test epoch: 4976, loss: 0.55\n",
      "train epoch: 4977, loss 0.0703\n",
      "test epoch: 4977, loss: 0.49\n",
      "train epoch: 4978, loss 0.0699\n",
      "test epoch: 4978, loss: 0.55\n",
      "train epoch: 4979, loss 0.0696\n",
      "test epoch: 4979, loss: 0.49\n",
      "train epoch: 4980, loss 0.0693\n",
      "test epoch: 4980, loss: 0.54\n",
      "train epoch: 4981, loss 0.0691\n",
      "test epoch: 4981, loss: 0.49\n",
      "train epoch: 4982, loss 0.0689\n",
      "test epoch: 4982, loss: 0.54\n",
      "train epoch: 4983, loss 0.0689\n",
      "test epoch: 4983, loss: 0.49\n",
      "train epoch: 4984, loss 0.0688\n",
      "test epoch: 4984, loss: 0.54\n",
      "train epoch: 4985, loss 0.0690\n",
      "test epoch: 4985, loss: 0.49\n",
      "train epoch: 4986, loss 0.0691\n",
      "test epoch: 4986, loss: 0.55\n",
      "train epoch: 4987, loss 0.0693\n",
      "test epoch: 4987, loss: 0.49\n",
      "train epoch: 4988, loss 0.0696\n",
      "test epoch: 4988, loss: 0.55\n",
      "train epoch: 4989, loss 0.0698\n",
      "test epoch: 4989, loss: 0.49\n",
      "train epoch: 4990, loss 0.0700\n",
      "test epoch: 4990, loss: 0.55\n",
      "train epoch: 4991, loss 0.0701\n",
      "test epoch: 4991, loss: 0.49\n",
      "train epoch: 4992, loss 0.0700\n",
      "test epoch: 4992, loss: 0.55\n",
      "train epoch: 4993, loss 0.0699\n",
      "test epoch: 4993, loss: 0.49\n",
      "train epoch: 4994, loss 0.0696\n",
      "test epoch: 4994, loss: 0.55\n",
      "train epoch: 4995, loss 0.0694\n",
      "test epoch: 4995, loss: 0.49\n",
      "train epoch: 4996, loss 0.0692\n",
      "test epoch: 4996, loss: 0.55\n",
      "train epoch: 4997, loss 0.0691\n",
      "test epoch: 4997, loss: 0.50\n",
      "train epoch: 4998, loss 0.0690\n",
      "test epoch: 4998, loss: 0.55\n",
      "train epoch: 4999, loss 0.0689\n",
      "test epoch: 4999, loss: 0.50\n",
      "train epoch: 5000, loss 0.0689\n",
      "test epoch: 5000, loss: 0.55\n"
     ]
    }
   ],
   "source": [
    "test_loss, final_state = train_network()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now create some functions that help in evaluating the trained model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def evaluate_model_outputs(state, input):\n",
    "    \"\"\"\n",
    "    A single evaluation step of the output logits of the neural network for a batch of inputs, as well as the\n",
    "    cross-entropy loss and the classification accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logits : jnp.array\n",
    "        the outputs of the neural network for a given input\n",
    "    \"\"\"\n",
    "    logit_outputs = state.apply_fn(state.params, input, train=False)\n",
    "\n",
    "    return logit_outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's evaluate the model on the original input range x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "logit_outputs = evaluate_model_outputs(state = final_state, input = jnp.expand_dims(x,axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot the outputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x29bfdf220>]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_training,y_training,c='red')\n",
    "plt.scatter(x_test,y_test,c='blue')\n",
    "plt.plot(x, logit_outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now define an optimization step that differentiates with respect to a diversity term as well. What do we hope to get instead of the output in the above figure? We'd hope that the output is pushed both towards the positive part of the y axis as well as the negative part. The predictions will be made more ``uncertain\" as the predictive values will be far away from the values predicted on the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_diverse(state, dropout_rng):\n",
    "    \"\"\"\n",
    "    Trains the neural network for a single step.\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The initial training state of the experiment.\n",
    "    batch : dict\n",
    "        Dictionary with keys 'image' and 'label' corresponding to a batch of the training set.\n",
    "    dropout_rng : jax.random.PRNGKey\n",
    "        Pseudo-random number generator (PRNG) key for the randomness of the dropout layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "    metrics : dict\n",
    "        A python dictionary with keys \"loss\" and \"accuracy\" corresponding to the cross-entropy loss and the accuracy\n",
    "        for some logits and labels.\n",
    "    new_dropout_rng : jax.random.PRNGKey\n",
    "        New pseudo-random number generator (PRNG) key for the randomness of the dropout layers.\n",
    "\n",
    "    \"\"\"\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, x_training, train=True, rngs={'dropout': dropout_rng})\n",
    "        loss = squared_error(logits=logits, targets=y_training)\n",
    "        return loss, logits\n",
    "\n",
    "    def output_logits_training(params):\n",
    "        logits = state.apply_fn(params, x_training, train=True, rngs={'dropout': dropout_rng})\n",
    "        logits_sum = jnp.sum(logits)\n",
    "        return logits_sum\n",
    "\n",
    "    def output_logits_unlabeled(params):\n",
    "        logits = state.apply_fn(params, x_unlabeled, train=True, rngs={'dropout': dropout_rng})\n",
    "        logits_sum = jnp.sum(logits)\n",
    "        return logits_sum\n",
    "\n",
    "    grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "    output_training_grad_fn = jax.grad(output_logits_training, has_aux=False)\n",
    "    output_unlabeled_grad_fn = jax.grad(output_logits_unlabeled, has_aux=False)\n",
    "\n",
    "    grads, logits = grad_fn(state.params)\n",
    "    grads_diverse_training = output_training_grad_fn(state.params)\n",
    "    grads_diverse_unlabeled = output_unlabeled_grad_fn(state.params)\n",
    "\n",
    "    def diverse_term(grads_diverse_training, grads_diverse_unlabeled, mylambda, prior_var):\n",
    "\n",
    "        term1 = jax.tree_map(lambda x: (mylambda*x**2+1/prior_var)**(-2)*(2*mylambda*x**3), grads_diverse_training)\n",
    "        term2 = jax.tree_map(lambda x: x**2, grads_diverse_unlabeled)\n",
    "        term3 = jax.tree_map(lambda x: (mylambda*x**2+1/prior_var)**(-1), grads_diverse_training)\n",
    "        term4 = jax.tree_map(lambda x: 2*x**3, grads_diverse_unlabeled)\n",
    "\n",
    "        return jax.tree_map(lambda x, y, z, k: x*y+z*k, term1, term2, term3, term4)\n",
    "\n",
    "    mylambda = 0.0001\n",
    "    prior_var = 0.0001\n",
    "    beta = 0.0001\n",
    "\n",
    "    divers_grads = jax.tree_map(lambda x: x*beta,diverse_term(grads_diverse_training, grads_diverse_unlabeled, mylambda, prior_var))\n",
    "\n",
    "    grads_total = jax.tree_map(lambda x, y: x+y, grads, divers_grads)\n",
    "\n",
    "    #print(divers_grads['params']['Dense_1']['kernel'])\n",
    "    #print(grads['params']['Dense_1']['kernel'])\n",
    "\n",
    "    state = state.apply_gradients(grads=grads_total)\n",
    "    metrics = compute_metrics(logits=logits, targets=y_training)\n",
    "    return state, metrics, new_dropout_rng\n",
    "\n",
    "def train_network_diverse():\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : DictConfig\n",
    "        The configuration file for the experiment.\n",
    "    Returns\n",
    "    -------\n",
    "    test_accuracy : float\n",
    "        The final test accuracy of the trained model. This is useful when doing hyperparameter search with optuna.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rng = jax.random.PRNGKey(40)#0\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "    state = create_train_state(init_rng)\n",
    "    del init_rng #Must not be used anymore\n",
    "\n",
    "    num_epochs = 5000\n",
    "\n",
    "    test_log_dir = 'logs/diverse_training/test'\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        # Use a separate PRNG key to permute image data during shuffling\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        # Run an optimization step over a training batch\n",
    "        state = train_epoch_diverse(state, epoch, input_rng)\n",
    "        # Evaluate on the test set after each training epoch\n",
    "        test_loss = eval_model(state)\n",
    "        print('test epoch: %d, loss: %.2f' % (epoch, test_loss))\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss, step=epoch)\n",
    "\n",
    "    return test_loss, state\n",
    "\n",
    "def train_epoch_diverse(state, epoch, rng):\n",
    "    \"\"\"\n",
    "    Train for a single epoch.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : train_state.TrainState\n",
    "        The training state of the experiment.\n",
    "    train_ds: dict\n",
    "        Dictionary with keys 'image' and 'label' corresponding to the training set.\n",
    "    batch_size : int\n",
    "        The size of the batch.\n",
    "    epoch : int\n",
    "        The number of the current epoch.\n",
    "    rng : jax.random.PRNGKey\n",
    "        Pseudo-random number generator (PRNG) key for the random initialization of the neural network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    state : train_state.TrainState\n",
    "        The new training state of the experiment.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    batch_metrics = []\n",
    "    dropout_rng = jax.random.split(rng, jax.local_device_count())[0]\n",
    "\n",
    "    state, metrics, dropout_rng = train_step_diverse(state, dropout_rng)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "    #compute mean of metrics across each batch in epoch.train_state\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0] # jnp.mean does not work on lists\n",
    "    }\n",
    "\n",
    "    print('train epoch: %d, loss %.4f' % (epoch, epoch_metrics_np['loss']))\n",
    "\n",
    "    train_log_dir = 'logs/diverse_training/training'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('l2_error', epoch_metrics_np['loss'], step=epoch)\n",
    "\n",
    "    return state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now train the network with the diverse loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss 1.3165\n",
      "test epoch: 1, loss: 0.73\n",
      "train epoch: 2, loss 0.3693\n",
      "test epoch: 2, loss: 1.20\n",
      "train epoch: 3, loss 0.8990\n",
      "test epoch: 3, loss: 0.93\n",
      "train epoch: 4, loss 0.5679\n",
      "test epoch: 4, loss: 0.53\n",
      "train epoch: 5, loss 0.2438\n",
      "test epoch: 5, loss: 0.30\n",
      "train epoch: 6, loss 0.3523\n",
      "test epoch: 6, loss: 0.25\n",
      "train epoch: 7, loss 0.5533\n",
      "test epoch: 7, loss: 0.25\n",
      "train epoch: 8, loss 0.5233\n",
      "test epoch: 8, loss: 0.31\n",
      "train epoch: 9, loss 0.3397\n",
      "test epoch: 9, loss: 0.44\n",
      "train epoch: 10, loss 0.2312\n",
      "test epoch: 10, loss: 0.62\n",
      "train epoch: 11, loss 0.2801\n",
      "test epoch: 11, loss: 0.75\n",
      "train epoch: 12, loss 0.3731\n",
      "test epoch: 12, loss: 0.75\n",
      "train epoch: 13, loss 0.3772\n",
      "test epoch: 13, loss: 0.65\n",
      "train epoch: 14, loss 0.2972\n",
      "test epoch: 14, loss: 0.51\n",
      "train epoch: 15, loss 0.2315\n",
      "test epoch: 15, loss: 0.39\n",
      "train epoch: 16, loss 0.2440\n",
      "test epoch: 16, loss: 0.33\n",
      "train epoch: 17, loss 0.3029\n",
      "test epoch: 17, loss: 0.31\n",
      "train epoch: 18, loss 0.3314\n",
      "test epoch: 18, loss: 0.33\n",
      "train epoch: 19, loss 0.2994\n",
      "test epoch: 19, loss: 0.39\n",
      "train epoch: 20, loss 0.2458\n",
      "test epoch: 20, loss: 0.47\n",
      "train epoch: 21, loss 0.2235\n",
      "test epoch: 21, loss: 0.56\n",
      "train epoch: 22, loss 0.2404\n",
      "test epoch: 22, loss: 0.61\n",
      "train epoch: 23, loss 0.2640\n",
      "test epoch: 23, loss: 0.61\n",
      "train epoch: 24, loss 0.2625\n",
      "test epoch: 24, loss: 0.56\n",
      "train epoch: 25, loss 0.2388\n",
      "test epoch: 25, loss: 0.49\n",
      "train epoch: 26, loss 0.2209\n",
      "test epoch: 26, loss: 0.42\n",
      "train epoch: 27, loss 0.2266\n",
      "test epoch: 27, loss: 0.38\n",
      "train epoch: 28, loss 0.2452\n",
      "test epoch: 28, loss: 0.37\n",
      "train epoch: 29, loss 0.2529\n",
      "test epoch: 29, loss: 0.39\n",
      "train epoch: 30, loss 0.2411\n",
      "test epoch: 30, loss: 0.43\n",
      "train epoch: 31, loss 0.2231\n",
      "test epoch: 31, loss: 0.48\n",
      "train epoch: 32, loss 0.2163\n",
      "test epoch: 32, loss: 0.53\n",
      "train epoch: 33, loss 0.2224\n",
      "test epoch: 33, loss: 0.56\n",
      "train epoch: 34, loss 0.2286\n",
      "test epoch: 34, loss: 0.55\n",
      "train epoch: 35, loss 0.2251\n",
      "test epoch: 35, loss: 0.51\n",
      "train epoch: 36, loss 0.2162\n",
      "test epoch: 36, loss: 0.47\n",
      "train epoch: 37, loss 0.2130\n",
      "test epoch: 37, loss: 0.43\n",
      "train epoch: 38, loss 0.2182\n",
      "test epoch: 38, loss: 0.41\n",
      "train epoch: 39, loss 0.2240\n",
      "test epoch: 39, loss: 0.42\n",
      "train epoch: 40, loss 0.2224\n",
      "test epoch: 40, loss: 0.44\n",
      "train epoch: 41, loss 0.2149\n",
      "test epoch: 41, loss: 0.47\n",
      "train epoch: 42, loss 0.2094\n",
      "test epoch: 42, loss: 0.51\n",
      "train epoch: 43, loss 0.2097\n",
      "test epoch: 43, loss: 0.53\n",
      "train epoch: 44, loss 0.2119\n",
      "test epoch: 44, loss: 0.53\n",
      "train epoch: 45, loss 0.2108\n",
      "test epoch: 45, loss: 0.51\n",
      "train epoch: 46, loss 0.2073\n",
      "test epoch: 46, loss: 0.48\n",
      "train epoch: 47, loss 0.2058\n",
      "test epoch: 47, loss: 0.45\n",
      "train epoch: 48, loss 0.2078\n",
      "test epoch: 48, loss: 0.44\n",
      "train epoch: 49, loss 0.2098\n",
      "test epoch: 49, loss: 0.45\n",
      "train epoch: 50, loss 0.2082\n",
      "test epoch: 50, loss: 0.46\n",
      "train epoch: 51, loss 0.2045\n",
      "test epoch: 51, loss: 0.49\n",
      "train epoch: 52, loss 0.2021\n",
      "test epoch: 52, loss: 0.51\n",
      "train epoch: 53, loss 0.2020\n",
      "test epoch: 53, loss: 0.52\n",
      "train epoch: 54, loss 0.2020\n",
      "test epoch: 54, loss: 0.51\n",
      "train epoch: 55, loss 0.2007\n",
      "test epoch: 55, loss: 0.50\n",
      "train epoch: 56, loss 0.1992\n",
      "test epoch: 56, loss: 0.48\n",
      "train epoch: 57, loss 0.1993\n",
      "test epoch: 57, loss: 0.47\n",
      "train epoch: 58, loss 0.2001\n",
      "test epoch: 58, loss: 0.47\n",
      "train epoch: 59, loss 0.1997\n",
      "test epoch: 59, loss: 0.48\n",
      "train epoch: 60, loss 0.1977\n",
      "test epoch: 60, loss: 0.49\n",
      "train epoch: 61, loss 0.1958\n",
      "test epoch: 61, loss: 0.51\n",
      "train epoch: 62, loss 0.1951\n",
      "test epoch: 62, loss: 0.52\n",
      "train epoch: 63, loss 0.1947\n",
      "test epoch: 63, loss: 0.51\n",
      "train epoch: 64, loss 0.1938\n",
      "test epoch: 64, loss: 0.50\n",
      "train epoch: 65, loss 0.1929\n",
      "test epoch: 65, loss: 0.49\n",
      "train epoch: 66, loss 0.1927\n",
      "test epoch: 66, loss: 0.48\n",
      "train epoch: 67, loss 0.1927\n",
      "test epoch: 67, loss: 0.48\n",
      "train epoch: 68, loss 0.1920\n",
      "test epoch: 68, loss: 0.49\n",
      "train epoch: 69, loss 0.1907\n",
      "test epoch: 69, loss: 0.50\n",
      "train epoch: 70, loss 0.1894\n",
      "test epoch: 70, loss: 0.51\n",
      "train epoch: 71, loss 0.1888\n",
      "test epoch: 71, loss: 0.52\n",
      "train epoch: 72, loss 0.1881\n",
      "test epoch: 72, loss: 0.51\n",
      "train epoch: 73, loss 0.1874\n",
      "test epoch: 73, loss: 0.51\n",
      "train epoch: 74, loss 0.1869\n",
      "test epoch: 74, loss: 0.50\n",
      "train epoch: 75, loss 0.1867\n",
      "test epoch: 75, loss: 0.50\n",
      "train epoch: 76, loss 0.1863\n",
      "test epoch: 76, loss: 0.50\n",
      "train epoch: 77, loss 0.1854\n",
      "test epoch: 77, loss: 0.51\n",
      "train epoch: 78, loss 0.1845\n",
      "test epoch: 78, loss: 0.52\n",
      "train epoch: 79, loss 0.1837\n",
      "test epoch: 79, loss: 0.52\n",
      "train epoch: 80, loss 0.1831\n",
      "test epoch: 80, loss: 0.52\n",
      "train epoch: 81, loss 0.1825\n",
      "test epoch: 81, loss: 0.52\n",
      "train epoch: 82, loss 0.1820\n",
      "test epoch: 82, loss: 0.51\n",
      "train epoch: 83, loss 0.1817\n",
      "test epoch: 83, loss: 0.51\n",
      "train epoch: 84, loss 0.1813\n",
      "test epoch: 84, loss: 0.51\n",
      "train epoch: 85, loss 0.1806\n",
      "test epoch: 85, loss: 0.52\n",
      "train epoch: 86, loss 0.1799\n",
      "test epoch: 86, loss: 0.52\n",
      "train epoch: 87, loss 0.1793\n",
      "test epoch: 87, loss: 0.53\n",
      "train epoch: 88, loss 0.1787\n",
      "test epoch: 88, loss: 0.53\n",
      "train epoch: 89, loss 0.1783\n",
      "test epoch: 89, loss: 0.52\n",
      "train epoch: 90, loss 0.1779\n",
      "test epoch: 90, loss: 0.52\n",
      "train epoch: 91, loss 0.1776\n",
      "test epoch: 91, loss: 0.52\n",
      "train epoch: 92, loss 0.1772\n",
      "test epoch: 92, loss: 0.52\n",
      "train epoch: 93, loss 0.1767\n",
      "test epoch: 93, loss: 0.53\n",
      "train epoch: 94, loss 0.1762\n",
      "test epoch: 94, loss: 0.53\n",
      "train epoch: 95, loss 0.1758\n",
      "test epoch: 95, loss: 0.53\n",
      "train epoch: 96, loss 0.1754\n",
      "test epoch: 96, loss: 0.53\n",
      "train epoch: 97, loss 0.1751\n",
      "test epoch: 97, loss: 0.53\n",
      "train epoch: 98, loss 0.1749\n",
      "test epoch: 98, loss: 0.53\n",
      "train epoch: 99, loss 0.1745\n",
      "test epoch: 99, loss: 0.53\n",
      "train epoch: 100, loss 0.1741\n",
      "test epoch: 100, loss: 0.54\n",
      "train epoch: 101, loss 0.1737\n",
      "test epoch: 101, loss: 0.54\n",
      "train epoch: 102, loss 0.1733\n",
      "test epoch: 102, loss: 0.54\n",
      "train epoch: 103, loss 0.1729\n",
      "test epoch: 103, loss: 0.54\n",
      "train epoch: 104, loss 0.1726\n",
      "test epoch: 104, loss: 0.54\n",
      "train epoch: 105, loss 0.1724\n",
      "test epoch: 105, loss: 0.54\n",
      "train epoch: 106, loss 0.1720\n",
      "test epoch: 106, loss: 0.54\n",
      "train epoch: 107, loss 0.1715\n",
      "test epoch: 107, loss: 0.55\n",
      "train epoch: 108, loss 0.1708\n",
      "test epoch: 108, loss: 0.56\n",
      "train epoch: 109, loss 0.1704\n",
      "test epoch: 109, loss: 0.55\n",
      "train epoch: 110, loss 0.1700\n",
      "test epoch: 110, loss: 0.54\n",
      "train epoch: 111, loss 0.1699\n",
      "test epoch: 111, loss: 0.55\n",
      "train epoch: 112, loss 0.1695\n",
      "test epoch: 112, loss: 0.55\n",
      "train epoch: 113, loss 0.1689\n",
      "test epoch: 113, loss: 0.56\n",
      "train epoch: 114, loss 0.1684\n",
      "test epoch: 114, loss: 0.56\n",
      "train epoch: 115, loss 0.1674\n",
      "test epoch: 115, loss: 0.56\n",
      "train epoch: 116, loss 0.1663\n",
      "test epoch: 116, loss: 0.60\n",
      "train epoch: 117, loss 0.1661\n",
      "test epoch: 117, loss: 0.54\n",
      "train epoch: 118, loss 0.1672\n",
      "test epoch: 118, loss: 0.55\n",
      "train epoch: 119, loss 0.1656\n",
      "test epoch: 119, loss: 0.59\n",
      "train epoch: 120, loss 0.1640\n",
      "test epoch: 120, loss: 0.60\n",
      "train epoch: 121, loss 0.1640\n",
      "test epoch: 121, loss: 0.56\n",
      "train epoch: 122, loss 0.1636\n",
      "test epoch: 122, loss: 0.55\n",
      "train epoch: 123, loss 0.1649\n",
      "test epoch: 123, loss: 0.57\n",
      "train epoch: 124, loss 0.1626\n",
      "test epoch: 124, loss: 0.60\n",
      "train epoch: 125, loss 0.1629\n",
      "test epoch: 125, loss: 0.58\n",
      "train epoch: 126, loss 0.1618\n",
      "test epoch: 126, loss: 0.56\n",
      "train epoch: 127, loss 0.1625\n",
      "test epoch: 127, loss: 0.58\n",
      "train epoch: 128, loss 0.1608\n",
      "test epoch: 128, loss: 0.66\n",
      "train epoch: 129, loss 0.1626\n",
      "test epoch: 129, loss: 0.56\n",
      "train epoch: 130, loss 0.1618\n",
      "test epoch: 130, loss: 0.55\n",
      "train epoch: 131, loss 0.1635\n",
      "test epoch: 131, loss: 0.63\n",
      "train epoch: 132, loss 0.1589\n",
      "test epoch: 132, loss: 0.64\n",
      "train epoch: 133, loss 0.1602\n",
      "test epoch: 133, loss: 0.58\n",
      "train epoch: 134, loss 0.1596\n",
      "test epoch: 134, loss: 0.56\n",
      "train epoch: 135, loss 0.1623\n",
      "test epoch: 135, loss: 0.60\n",
      "train epoch: 136, loss 0.1586\n",
      "test epoch: 136, loss: 0.67\n",
      "train epoch: 137, loss 0.1623\n",
      "test epoch: 137, loss: 0.57\n",
      "train epoch: 138, loss 0.1611\n",
      "test epoch: 138, loss: 0.56\n",
      "train epoch: 139, loss 0.1632\n",
      "test epoch: 139, loss: 0.61\n",
      "train epoch: 140, loss 0.1587\n",
      "test epoch: 140, loss: 0.65\n",
      "train epoch: 141, loss 0.1605\n",
      "test epoch: 141, loss: 0.61\n",
      "train epoch: 142, loss 0.1587\n",
      "test epoch: 142, loss: 0.57\n",
      "train epoch: 143, loss 0.1623\n",
      "test epoch: 143, loss: 0.58\n",
      "train epoch: 144, loss 0.1605\n",
      "test epoch: 144, loss: 0.63\n",
      "train epoch: 145, loss 0.1591\n",
      "test epoch: 145, loss: 0.62\n",
      "train epoch: 146, loss 0.1593\n",
      "test epoch: 146, loss: 0.58\n",
      "train epoch: 147, loss 0.1599\n",
      "test epoch: 147, loss: 0.56\n",
      "train epoch: 148, loss 0.1624\n",
      "test epoch: 148, loss: 0.58\n",
      "train epoch: 149, loss 0.1606\n",
      "test epoch: 149, loss: 0.60\n",
      "train epoch: 150, loss 0.1600\n",
      "test epoch: 150, loss: 0.60\n",
      "train epoch: 151, loss 0.1593\n",
      "test epoch: 151, loss: 0.57\n",
      "train epoch: 152, loss 0.1608\n",
      "test epoch: 152, loss: 0.57\n",
      "train epoch: 153, loss 0.1611\n",
      "test epoch: 153, loss: 0.59\n",
      "train epoch: 154, loss 0.1604\n",
      "test epoch: 154, loss: 0.61\n",
      "train epoch: 155, loss 0.1600\n",
      "test epoch: 155, loss: 0.60\n",
      "train epoch: 156, loss 0.1600\n",
      "test epoch: 156, loss: 0.60\n",
      "train epoch: 157, loss 0.1601\n",
      "test epoch: 157, loss: 0.62\n",
      "train epoch: 158, loss 0.1593\n",
      "test epoch: 158, loss: 0.61\n",
      "train epoch: 159, loss 0.1598\n",
      "test epoch: 159, loss: 0.59\n",
      "train epoch: 160, loss 0.1611\n",
      "test epoch: 160, loss: 0.59\n",
      "train epoch: 161, loss 0.1605\n",
      "test epoch: 161, loss: 0.61\n",
      "train epoch: 162, loss 0.1601\n",
      "test epoch: 162, loss: 0.60\n",
      "train epoch: 163, loss 0.1599\n",
      "test epoch: 163, loss: 0.59\n",
      "train epoch: 164, loss 0.1612\n",
      "test epoch: 164, loss: 0.58\n",
      "train epoch: 165, loss 0.1620\n",
      "test epoch: 165, loss: 0.59\n",
      "train epoch: 166, loss 0.1622\n",
      "test epoch: 166, loss: 0.59\n",
      "train epoch: 167, loss 0.1624\n",
      "test epoch: 167, loss: 0.58\n",
      "train epoch: 168, loss 0.1630\n",
      "test epoch: 168, loss: 0.57\n",
      "train epoch: 169, loss 0.1641\n",
      "test epoch: 169, loss: 0.56\n",
      "train epoch: 170, loss 0.1643\n",
      "test epoch: 170, loss: 0.56\n",
      "train epoch: 171, loss 0.1649\n",
      "test epoch: 171, loss: 0.55\n",
      "train epoch: 172, loss 0.1652\n",
      "test epoch: 172, loss: 0.54\n",
      "train epoch: 173, loss 0.1666\n",
      "test epoch: 173, loss: 0.54\n",
      "train epoch: 174, loss 0.1674\n",
      "test epoch: 174, loss: 0.55\n",
      "train epoch: 175, loss 0.1670\n",
      "test epoch: 175, loss: 0.56\n",
      "train epoch: 176, loss 0.1676\n",
      "test epoch: 176, loss: 0.56\n",
      "train epoch: 177, loss 0.1689\n",
      "test epoch: 177, loss: 0.56\n",
      "train epoch: 178, loss 0.1689\n",
      "test epoch: 178, loss: 0.58\n",
      "train epoch: 179, loss 0.1698\n",
      "test epoch: 179, loss: 0.55\n",
      "train epoch: 180, loss 0.1725\n",
      "test epoch: 180, loss: 0.55\n",
      "train epoch: 181, loss 0.1738\n",
      "test epoch: 181, loss: 0.56\n",
      "train epoch: 182, loss 0.1752\n",
      "test epoch: 182, loss: 0.55\n",
      "train epoch: 183, loss 0.1768\n",
      "test epoch: 183, loss: 0.52\n",
      "train epoch: 184, loss 0.1817\n",
      "test epoch: 184, loss: 0.51\n",
      "train epoch: 185, loss 0.1828\n",
      "test epoch: 185, loss: 0.49\n",
      "train epoch: 186, loss 0.1875\n",
      "test epoch: 186, loss: 0.47\n",
      "train epoch: 187, loss 0.1959\n",
      "test epoch: 187, loss: 0.46\n",
      "train epoch: 188, loss 0.2011\n",
      "test epoch: 188, loss: 0.46\n",
      "train epoch: 189, loss 0.2010\n",
      "test epoch: 189, loss: 0.48\n",
      "train epoch: 190, loss 0.2015\n",
      "test epoch: 190, loss: 0.48\n",
      "train epoch: 191, loss 0.2029\n",
      "test epoch: 191, loss: 0.45\n",
      "train epoch: 192, loss 0.2095\n",
      "test epoch: 192, loss: 0.45\n",
      "train epoch: 193, loss 0.2105\n",
      "test epoch: 193, loss: 0.47\n",
      "train epoch: 194, loss 0.2071\n",
      "test epoch: 194, loss: 0.49\n",
      "train epoch: 195, loss 0.2067\n",
      "test epoch: 195, loss: 0.48\n",
      "train epoch: 196, loss 0.2075\n",
      "test epoch: 196, loss: 0.47\n",
      "train epoch: 197, loss 0.2102\n",
      "test epoch: 197, loss: 0.48\n",
      "train epoch: 198, loss 0.2079\n",
      "test epoch: 198, loss: 0.50\n",
      "train epoch: 199, loss 0.2056\n",
      "test epoch: 199, loss: 0.49\n",
      "train epoch: 200, loss 0.2061\n",
      "test epoch: 200, loss: 0.48\n",
      "train epoch: 201, loss 0.2070\n",
      "test epoch: 201, loss: 0.47\n",
      "train epoch: 202, loss 0.2081\n",
      "test epoch: 202, loss: 0.46\n",
      "train epoch: 203, loss 0.2107\n",
      "test epoch: 203, loss: 0.46\n",
      "train epoch: 204, loss 0.2095\n",
      "test epoch: 204, loss: 0.48\n",
      "train epoch: 205, loss 0.2058\n",
      "test epoch: 205, loss: 0.54\n",
      "train epoch: 206, loss 0.2069\n",
      "test epoch: 206, loss: 0.46\n",
      "train epoch: 207, loss 0.2086\n",
      "test epoch: 207, loss: 0.44\n",
      "train epoch: 208, loss 0.2170\n",
      "test epoch: 208, loss: 0.49\n",
      "train epoch: 209, loss 0.2064\n",
      "test epoch: 209, loss: 0.52\n",
      "train epoch: 210, loss 0.2104\n",
      "test epoch: 210, loss: 0.47\n",
      "train epoch: 211, loss 0.2086\n",
      "test epoch: 211, loss: 0.45\n",
      "train epoch: 212, loss 0.2143\n",
      "test epoch: 212, loss: 0.49\n",
      "train epoch: 213, loss 0.2049\n",
      "test epoch: 213, loss: 0.53\n",
      "train epoch: 214, loss 0.2037\n",
      "test epoch: 214, loss: 0.51\n",
      "train epoch: 215, loss 0.2010\n",
      "test epoch: 215, loss: 0.48\n",
      "train epoch: 216, loss 0.2040\n",
      "test epoch: 216, loss: 0.51\n",
      "train epoch: 217, loss 0.1993\n",
      "test epoch: 217, loss: 0.54\n",
      "train epoch: 218, loss 0.1986\n",
      "test epoch: 218, loss: 0.51\n",
      "train epoch: 219, loss 0.1974\n",
      "test epoch: 219, loss: 0.49\n",
      "train epoch: 220, loss 0.1993\n",
      "test epoch: 220, loss: 0.52\n",
      "train epoch: 221, loss 0.1933\n",
      "test epoch: 221, loss: 0.54\n",
      "train epoch: 222, loss 0.1897\n",
      "test epoch: 222, loss: 0.52\n",
      "train epoch: 223, loss 0.1862\n",
      "test epoch: 223, loss: 0.52\n",
      "train epoch: 224, loss 0.1822\n",
      "test epoch: 224, loss: 0.56\n",
      "train epoch: 225, loss 0.1811\n",
      "test epoch: 225, loss: 0.49\n",
      "train epoch: 226, loss 0.1854\n",
      "test epoch: 226, loss: 0.50\n",
      "train epoch: 227, loss 0.1820\n",
      "test epoch: 227, loss: 0.55\n",
      "train epoch: 228, loss 0.1795\n",
      "test epoch: 228, loss: 0.55\n",
      "train epoch: 229, loss 0.1770\n",
      "test epoch: 229, loss: 0.51\n",
      "train epoch: 230, loss 0.1807\n",
      "test epoch: 230, loss: 0.53\n",
      "train epoch: 231, loss 0.1784\n",
      "test epoch: 231, loss: 0.57\n",
      "train epoch: 232, loss 0.1773\n",
      "test epoch: 232, loss: 0.56\n",
      "train epoch: 233, loss 0.1766\n",
      "test epoch: 233, loss: 0.51\n",
      "train epoch: 234, loss 0.1810\n",
      "test epoch: 234, loss: 0.53\n",
      "train epoch: 235, loss 0.1785\n",
      "test epoch: 235, loss: 0.56\n",
      "train epoch: 236, loss 0.1773\n",
      "test epoch: 236, loss: 0.55\n",
      "train epoch: 237, loss 0.1766\n",
      "test epoch: 237, loss: 0.52\n",
      "train epoch: 238, loss 0.1792\n",
      "test epoch: 238, loss: 0.53\n",
      "train epoch: 239, loss 0.1772\n",
      "test epoch: 239, loss: 0.56\n",
      "train epoch: 240, loss 0.1765\n",
      "test epoch: 240, loss: 0.54\n",
      "train epoch: 241, loss 0.1765\n",
      "test epoch: 241, loss: 0.51\n",
      "train epoch: 242, loss 0.1788\n",
      "test epoch: 242, loss: 0.53\n",
      "train epoch: 243, loss 0.1766\n",
      "test epoch: 243, loss: 0.56\n",
      "train epoch: 244, loss 0.1762\n",
      "test epoch: 244, loss: 0.54\n",
      "train epoch: 245, loss 0.1766\n",
      "test epoch: 245, loss: 0.53\n",
      "train epoch: 246, loss 0.1777\n",
      "test epoch: 246, loss: 0.55\n",
      "train epoch: 247, loss 0.1751\n",
      "test epoch: 247, loss: 0.56\n",
      "train epoch: 248, loss 0.1744\n",
      "test epoch: 248, loss: 0.56\n",
      "train epoch: 249, loss 0.1740\n",
      "test epoch: 249, loss: 0.55\n",
      "train epoch: 250, loss 0.1734\n",
      "test epoch: 250, loss: 0.54\n",
      "train epoch: 251, loss 0.1725\n",
      "test epoch: 251, loss: 0.54\n",
      "train epoch: 252, loss 0.1727\n",
      "test epoch: 252, loss: 0.53\n",
      "train epoch: 253, loss 0.1725\n",
      "test epoch: 253, loss: 0.55\n",
      "train epoch: 254, loss 0.1707\n",
      "test epoch: 254, loss: 0.59\n",
      "train epoch: 255, loss 0.1696\n",
      "test epoch: 255, loss: 0.53\n",
      "train epoch: 256, loss 0.1763\n",
      "test epoch: 256, loss: 0.53\n",
      "train epoch: 257, loss 0.1748\n",
      "test epoch: 257, loss: 0.56\n",
      "train epoch: 258, loss 0.1755\n",
      "test epoch: 258, loss: 0.52\n",
      "train epoch: 259, loss 0.1758\n",
      "test epoch: 259, loss: 0.49\n",
      "train epoch: 260, loss 0.1833\n",
      "test epoch: 260, loss: 0.49\n",
      "train epoch: 261, loss 0.1811\n",
      "test epoch: 261, loss: 0.51\n",
      "train epoch: 262, loss 0.1807\n",
      "test epoch: 262, loss: 0.52\n",
      "train epoch: 263, loss 0.1798\n",
      "test epoch: 263, loss: 0.51\n",
      "train epoch: 264, loss 0.1816\n",
      "test epoch: 264, loss: 0.50\n",
      "train epoch: 265, loss 0.1841\n",
      "test epoch: 265, loss: 0.46\n",
      "train epoch: 266, loss 0.1926\n",
      "test epoch: 266, loss: 0.45\n",
      "train epoch: 267, loss 0.1941\n",
      "test epoch: 267, loss: 0.44\n",
      "train epoch: 268, loss 0.1971\n",
      "test epoch: 268, loss: 0.43\n",
      "train epoch: 269, loss 0.2042\n",
      "test epoch: 269, loss: 0.42\n",
      "train epoch: 270, loss 0.2065\n",
      "test epoch: 270, loss: 0.43\n",
      "train epoch: 271, loss 0.2040\n",
      "test epoch: 271, loss: 0.43\n",
      "train epoch: 272, loss 0.2057\n",
      "test epoch: 272, loss: 0.43\n",
      "train epoch: 273, loss 0.2031\n",
      "test epoch: 273, loss: 0.45\n",
      "train epoch: 274, loss 0.1989\n",
      "test epoch: 274, loss: 0.46\n",
      "train epoch: 275, loss 0.1959\n",
      "test epoch: 275, loss: 0.46\n",
      "train epoch: 276, loss 0.1951\n",
      "test epoch: 276, loss: 0.44\n",
      "train epoch: 277, loss 0.2004\n",
      "test epoch: 277, loss: 0.42\n",
      "train epoch: 278, loss 0.2086\n",
      "test epoch: 278, loss: 0.38\n",
      "train epoch: 279, loss 0.2375\n",
      "test epoch: 279, loss: 0.34\n",
      "train epoch: 280, loss 0.2731\n",
      "test epoch: 280, loss: 0.31\n",
      "train epoch: 281, loss 0.3229\n",
      "test epoch: 281, loss: 0.30\n",
      "train epoch: 282, loss 0.3494\n",
      "test epoch: 282, loss: 0.28\n",
      "train epoch: 283, loss 0.3785\n",
      "test epoch: 283, loss: 0.28\n",
      "train epoch: 284, loss 0.3950\n",
      "test epoch: 284, loss: 0.28\n",
      "train epoch: 285, loss 0.4027\n",
      "test epoch: 285, loss: 0.28\n",
      "train epoch: 286, loss 0.4148\n",
      "test epoch: 286, loss: 0.28\n",
      "train epoch: 287, loss 0.3969\n",
      "test epoch: 287, loss: 0.28\n",
      "train epoch: 288, loss 0.3817\n",
      "test epoch: 288, loss: 0.29\n",
      "train epoch: 289, loss 0.3691\n",
      "test epoch: 289, loss: 0.29\n",
      "train epoch: 290, loss 0.3564\n",
      "test epoch: 290, loss: 0.28\n",
      "train epoch: 291, loss 0.4028\n",
      "test epoch: 291, loss: 0.25\n",
      "train epoch: 292, loss 0.5136\n",
      "test epoch: 292, loss: 0.23\n",
      "train epoch: 293, loss 0.6412\n",
      "test epoch: 293, loss: 0.23\n",
      "train epoch: 294, loss 0.7863\n",
      "test epoch: 294, loss: 0.22\n",
      "train epoch: 295, loss 0.8679\n",
      "test epoch: 295, loss: 0.23\n",
      "train epoch: 296, loss 1.0046\n",
      "test epoch: 296, loss: 0.23\n",
      "train epoch: 297, loss 0.8665\n",
      "test epoch: 297, loss: 0.24\n",
      "train epoch: 298, loss 0.5556\n",
      "test epoch: 298, loss: 0.31\n",
      "train epoch: 299, loss 0.3151\n",
      "test epoch: 299, loss: 0.45\n",
      "train epoch: 300, loss 0.2066\n",
      "test epoch: 300, loss: 0.58\n",
      "train epoch: 301, loss 0.2169\n",
      "test epoch: 301, loss: 0.60\n",
      "train epoch: 302, loss 0.2199\n",
      "test epoch: 302, loss: 0.53\n",
      "train epoch: 303, loss 0.1999\n",
      "test epoch: 303, loss: 0.43\n",
      "train epoch: 304, loss 0.2081\n",
      "test epoch: 304, loss: 0.35\n",
      "train epoch: 305, loss 0.2644\n",
      "test epoch: 305, loss: 0.32\n",
      "train epoch: 306, loss 0.3148\n",
      "test epoch: 306, loss: 0.31\n",
      "train epoch: 307, loss 0.3223\n",
      "test epoch: 307, loss: 0.33\n",
      "train epoch: 308, loss 0.2971\n",
      "test epoch: 308, loss: 0.36\n",
      "train epoch: 309, loss 0.2576\n",
      "test epoch: 309, loss: 0.39\n",
      "train epoch: 310, loss 0.2309\n",
      "test epoch: 310, loss: 0.42\n",
      "train epoch: 311, loss 0.2121\n",
      "test epoch: 311, loss: 0.45\n",
      "train epoch: 312, loss 0.2025\n",
      "test epoch: 312, loss: 0.46\n",
      "train epoch: 313, loss 0.1984\n",
      "test epoch: 313, loss: 0.46\n",
      "train epoch: 314, loss 0.2003\n",
      "test epoch: 314, loss: 0.43\n",
      "train epoch: 315, loss 0.2090\n",
      "test epoch: 315, loss: 0.40\n",
      "train epoch: 316, loss 0.2218\n",
      "test epoch: 316, loss: 0.38\n",
      "train epoch: 317, loss 0.2387\n",
      "test epoch: 317, loss: 0.35\n",
      "train epoch: 318, loss 0.2700\n",
      "test epoch: 318, loss: 0.31\n",
      "train epoch: 319, loss 0.3315\n",
      "test epoch: 319, loss: 0.28\n",
      "train epoch: 320, loss 0.3995\n",
      "test epoch: 320, loss: 0.26\n",
      "train epoch: 321, loss 0.4498\n",
      "test epoch: 321, loss: 0.26\n",
      "train epoch: 322, loss 0.4608\n",
      "test epoch: 322, loss: 0.26\n",
      "train epoch: 323, loss 0.4758\n",
      "test epoch: 323, loss: 0.26\n",
      "train epoch: 324, loss 0.4696\n",
      "test epoch: 324, loss: 0.27\n",
      "train epoch: 325, loss 0.4247\n",
      "test epoch: 325, loss: 0.28\n",
      "train epoch: 326, loss 0.3844\n",
      "test epoch: 326, loss: 0.29\n",
      "train epoch: 327, loss 0.3599\n",
      "test epoch: 327, loss: 0.30\n",
      "train epoch: 328, loss 0.3415\n",
      "test epoch: 328, loss: 0.30\n",
      "train epoch: 329, loss 0.3396\n",
      "test epoch: 329, loss: 0.30\n",
      "train epoch: 330, loss 0.3475\n",
      "test epoch: 330, loss: 0.29\n",
      "train epoch: 331, loss 0.3591\n",
      "test epoch: 331, loss: 0.29\n",
      "train epoch: 332, loss 0.3712\n",
      "test epoch: 332, loss: 0.28\n",
      "train epoch: 333, loss 0.3792\n",
      "test epoch: 333, loss: 0.29\n",
      "train epoch: 334, loss 0.3705\n",
      "test epoch: 334, loss: 0.30\n",
      "train epoch: 335, loss 0.3495\n",
      "test epoch: 335, loss: 0.31\n",
      "train epoch: 336, loss 0.3255\n",
      "test epoch: 336, loss: 0.32\n",
      "train epoch: 337, loss 0.3127\n",
      "test epoch: 337, loss: 0.33\n",
      "train epoch: 338, loss 0.2902\n",
      "test epoch: 338, loss: 0.29\n",
      "train epoch: 339, loss 0.3670\n",
      "test epoch: 339, loss: 0.24\n",
      "train epoch: 340, loss 0.5559\n",
      "test epoch: 340, loss: 0.22\n",
      "train epoch: 341, loss 0.7603\n",
      "test epoch: 341, loss: 0.22\n",
      "train epoch: 342, loss 1.0164\n",
      "test epoch: 342, loss: 0.23\n",
      "train epoch: 343, loss 1.2832\n",
      "test epoch: 343, loss: 0.25\n",
      "train epoch: 344, loss 1.5170\n",
      "test epoch: 344, loss: 0.27\n",
      "train epoch: 345, loss 1.6991\n",
      "test epoch: 345, loss: 0.27\n",
      "train epoch: 346, loss 1.7249\n",
      "test epoch: 346, loss: 0.26\n",
      "train epoch: 347, loss 1.6304\n",
      "test epoch: 347, loss: 0.25\n",
      "train epoch: 348, loss 1.5691\n",
      "test epoch: 348, loss: 0.24\n",
      "train epoch: 349, loss 1.5381\n",
      "test epoch: 349, loss: 0.23\n",
      "train epoch: 350, loss 1.3349\n",
      "test epoch: 350, loss: 0.22\n",
      "train epoch: 351, loss 1.2034\n",
      "test epoch: 351, loss: 0.22\n",
      "train epoch: 352, loss 1.1476\n",
      "test epoch: 352, loss: 0.21\n",
      "train epoch: 353, loss 0.9653\n",
      "test epoch: 353, loss: 0.22\n",
      "train epoch: 354, loss 0.7383\n",
      "test epoch: 354, loss: 0.25\n",
      "train epoch: 355, loss 0.5258\n",
      "test epoch: 355, loss: 0.30\n",
      "train epoch: 356, loss 0.3537\n",
      "test epoch: 356, loss: 0.37\n",
      "train epoch: 357, loss 0.2453\n",
      "test epoch: 357, loss: 0.44\n",
      "train epoch: 358, loss 0.1942\n",
      "test epoch: 358, loss: 0.51\n",
      "train epoch: 359, loss 0.1805\n",
      "test epoch: 359, loss: 0.55\n",
      "train epoch: 360, loss 0.1815\n",
      "test epoch: 360, loss: 0.55\n",
      "train epoch: 361, loss 0.1815\n",
      "test epoch: 361, loss: 0.52\n",
      "train epoch: 362, loss 0.1794\n",
      "test epoch: 362, loss: 0.46\n",
      "train epoch: 363, loss 0.1880\n",
      "test epoch: 363, loss: 0.40\n",
      "train epoch: 364, loss 0.2162\n",
      "test epoch: 364, loss: 0.36\n",
      "train epoch: 365, loss 0.2619\n",
      "test epoch: 365, loss: 0.32\n",
      "train epoch: 366, loss 0.3185\n",
      "test epoch: 366, loss: 0.28\n",
      "train epoch: 367, loss 0.4024\n",
      "test epoch: 367, loss: 0.24\n",
      "train epoch: 368, loss 0.5394\n",
      "test epoch: 368, loss: 0.22\n",
      "train epoch: 369, loss 0.6832\n",
      "test epoch: 369, loss: 0.21\n",
      "train epoch: 370, loss 0.8207\n",
      "test epoch: 370, loss: 0.21\n",
      "train epoch: 371, loss 0.9477\n",
      "test epoch: 371, loss: 0.21\n",
      "train epoch: 372, loss 1.0551\n",
      "test epoch: 372, loss: 0.21\n",
      "train epoch: 373, loss 0.9895\n",
      "test epoch: 373, loss: 0.21\n",
      "train epoch: 374, loss 0.8304\n",
      "test epoch: 374, loss: 0.23\n",
      "train epoch: 375, loss 0.6618\n",
      "test epoch: 375, loss: 0.23\n",
      "train epoch: 376, loss 0.6037\n",
      "test epoch: 376, loss: 0.24\n",
      "train epoch: 377, loss 0.5362\n",
      "test epoch: 377, loss: 0.27\n",
      "train epoch: 378, loss 0.4497\n",
      "test epoch: 378, loss: 0.29\n",
      "train epoch: 379, loss 0.3697\n",
      "test epoch: 379, loss: 0.33\n",
      "train epoch: 380, loss 0.2994\n",
      "test epoch: 380, loss: 0.38\n",
      "train epoch: 381, loss 0.2359\n",
      "test epoch: 381, loss: 0.39\n",
      "train epoch: 382, loss 0.2237\n",
      "test epoch: 382, loss: 0.37\n",
      "train epoch: 383, loss 0.2438\n",
      "test epoch: 383, loss: 0.34\n",
      "train epoch: 384, loss 0.2796\n",
      "test epoch: 384, loss: 0.31\n",
      "train epoch: 385, loss 0.3236\n",
      "test epoch: 385, loss: 0.29\n",
      "train epoch: 386, loss 0.3718\n",
      "test epoch: 386, loss: 0.27\n",
      "train epoch: 387, loss 0.4279\n",
      "test epoch: 387, loss: 0.26\n",
      "train epoch: 388, loss 0.4849\n",
      "test epoch: 388, loss: 0.24\n",
      "train epoch: 389, loss 0.5563\n",
      "test epoch: 389, loss: 0.23\n",
      "train epoch: 390, loss 0.6211\n",
      "test epoch: 390, loss: 0.22\n",
      "train epoch: 391, loss 0.6770\n",
      "test epoch: 391, loss: 0.22\n",
      "train epoch: 392, loss 0.7206\n",
      "test epoch: 392, loss: 0.22\n",
      "train epoch: 393, loss 0.7469\n",
      "test epoch: 393, loss: 0.22\n",
      "train epoch: 394, loss 0.7522\n",
      "test epoch: 394, loss: 0.22\n",
      "train epoch: 395, loss 0.7523\n",
      "test epoch: 395, loss: 0.22\n",
      "train epoch: 396, loss 0.7056\n",
      "test epoch: 396, loss: 0.23\n",
      "train epoch: 397, loss 0.6652\n",
      "test epoch: 397, loss: 0.23\n",
      "train epoch: 398, loss 0.6280\n",
      "test epoch: 398, loss: 0.23\n",
      "train epoch: 399, loss 0.6118\n",
      "test epoch: 399, loss: 0.23\n",
      "train epoch: 400, loss 0.6482\n",
      "test epoch: 400, loss: 0.22\n",
      "train epoch: 401, loss 0.6851\n",
      "test epoch: 401, loss: 0.22\n",
      "train epoch: 402, loss 0.7290\n",
      "test epoch: 402, loss: 0.22\n",
      "train epoch: 403, loss 0.7579\n",
      "test epoch: 403, loss: 0.22\n",
      "train epoch: 404, loss 0.7656\n",
      "test epoch: 404, loss: 0.22\n",
      "train epoch: 405, loss 0.7468\n",
      "test epoch: 405, loss: 0.22\n",
      "train epoch: 406, loss 0.7095\n",
      "test epoch: 406, loss: 0.22\n",
      "train epoch: 407, loss 0.7064\n",
      "test epoch: 407, loss: 0.22\n",
      "train epoch: 408, loss 0.6737\n",
      "test epoch: 408, loss: 0.23\n",
      "train epoch: 409, loss 0.6476\n",
      "test epoch: 409, loss: 0.22\n",
      "train epoch: 410, loss 0.7763\n",
      "test epoch: 410, loss: 0.22\n",
      "train epoch: 411, loss 0.7648\n",
      "test epoch: 411, loss: 0.21\n",
      "train epoch: 412, loss 0.7991\n",
      "test epoch: 412, loss: 0.21\n",
      "train epoch: 413, loss 0.9286\n",
      "test epoch: 413, loss: 0.21\n",
      "train epoch: 414, loss 1.0619\n",
      "test epoch: 414, loss: 0.21\n",
      "train epoch: 415, loss 1.1710\n",
      "test epoch: 415, loss: 0.21\n",
      "train epoch: 416, loss 1.2508\n",
      "test epoch: 416, loss: 0.21\n",
      "train epoch: 417, loss 1.3047\n",
      "test epoch: 417, loss: 0.21\n",
      "train epoch: 418, loss 1.3414\n",
      "test epoch: 418, loss: 0.22\n",
      "train epoch: 419, loss 1.3736\n",
      "test epoch: 419, loss: 0.22\n",
      "train epoch: 420, loss 1.4159\n",
      "test epoch: 420, loss: 0.22\n",
      "train epoch: 421, loss 1.4755\n",
      "test epoch: 421, loss: 0.23\n",
      "train epoch: 422, loss 1.5479\n",
      "test epoch: 422, loss: 0.23\n",
      "train epoch: 423, loss 1.6291\n",
      "test epoch: 423, loss: 0.24\n",
      "train epoch: 424, loss 1.7021\n",
      "test epoch: 424, loss: 0.25\n",
      "train epoch: 425, loss 1.8232\n",
      "test epoch: 425, loss: 0.28\n",
      "train epoch: 426, loss 2.0781\n",
      "test epoch: 426, loss: 0.31\n",
      "train epoch: 427, loss 2.3527\n",
      "test epoch: 427, loss: 0.36\n",
      "train epoch: 428, loss 2.6405\n",
      "test epoch: 428, loss: 0.39\n",
      "train epoch: 429, loss 2.8630\n",
      "test epoch: 429, loss: 0.44\n",
      "train epoch: 430, loss 3.1212\n",
      "test epoch: 430, loss: 0.48\n",
      "train epoch: 431, loss 3.3663\n",
      "test epoch: 431, loss: 0.52\n",
      "train epoch: 432, loss 3.5786\n",
      "test epoch: 432, loss: 0.55\n",
      "train epoch: 433, loss 3.7673\n",
      "test epoch: 433, loss: 0.58\n",
      "train epoch: 434, loss 3.9289\n",
      "test epoch: 434, loss: 0.56\n",
      "train epoch: 435, loss 3.7952\n",
      "test epoch: 435, loss: 0.52\n",
      "train epoch: 436, loss 3.6190\n",
      "test epoch: 436, loss: 0.46\n",
      "train epoch: 437, loss 3.3113\n",
      "test epoch: 437, loss: 0.40\n",
      "train epoch: 438, loss 2.9679\n",
      "test epoch: 438, loss: 0.37\n",
      "train epoch: 439, loss 2.8174\n",
      "test epoch: 439, loss: 0.35\n",
      "train epoch: 440, loss 2.7330\n",
      "test epoch: 440, loss: 0.31\n",
      "train epoch: 441, loss 2.4708\n",
      "test epoch: 441, loss: 0.27\n",
      "train epoch: 442, loss 2.1318\n",
      "test epoch: 442, loss: 0.23\n",
      "train epoch: 443, loss 1.7551\n",
      "test epoch: 443, loss: 0.21\n",
      "train epoch: 444, loss 1.4927\n",
      "test epoch: 444, loss: 0.20\n",
      "train epoch: 445, loss 1.3479\n",
      "test epoch: 445, loss: 0.20\n",
      "train epoch: 446, loss 1.1833\n",
      "test epoch: 446, loss: 0.20\n",
      "train epoch: 447, loss 0.9994\n",
      "test epoch: 447, loss: 0.21\n",
      "train epoch: 448, loss 0.8071\n",
      "test epoch: 448, loss: 0.23\n",
      "train epoch: 449, loss 0.6297\n",
      "test epoch: 449, loss: 0.27\n",
      "train epoch: 450, loss 0.4913\n",
      "test epoch: 450, loss: 0.31\n",
      "train epoch: 451, loss 0.3595\n",
      "test epoch: 451, loss: 0.37\n",
      "train epoch: 452, loss 0.2597\n",
      "test epoch: 452, loss: 0.43\n",
      "train epoch: 453, loss 0.2046\n",
      "test epoch: 453, loss: 0.49\n",
      "train epoch: 454, loss 0.1750\n",
      "test epoch: 454, loss: 0.55\n",
      "train epoch: 455, loss 0.1634\n",
      "test epoch: 455, loss: 0.59\n",
      "train epoch: 456, loss 0.1616\n",
      "test epoch: 456, loss: 0.61\n",
      "train epoch: 457, loss 0.1610\n",
      "test epoch: 457, loss: 0.60\n",
      "train epoch: 458, loss 0.1594\n",
      "test epoch: 458, loss: 0.56\n",
      "train epoch: 459, loss 0.1606\n",
      "test epoch: 459, loss: 0.53\n",
      "train epoch: 460, loss 0.1658\n",
      "test epoch: 460, loss: 0.51\n",
      "train epoch: 461, loss 0.1701\n",
      "test epoch: 461, loss: 0.50\n",
      "train epoch: 462, loss 0.1745\n",
      "test epoch: 462, loss: 0.51\n",
      "train epoch: 463, loss 0.1696\n",
      "test epoch: 463, loss: 0.54\n",
      "train epoch: 464, loss 0.1627\n",
      "test epoch: 464, loss: 0.57\n",
      "train epoch: 465, loss 0.1581\n",
      "test epoch: 465, loss: 0.60\n",
      "train epoch: 466, loss 0.1570\n",
      "test epoch: 466, loss: 0.61\n",
      "train epoch: 467, loss 0.1570\n",
      "test epoch: 467, loss: 0.59\n",
      "train epoch: 468, loss 0.1567\n",
      "test epoch: 468, loss: 0.56\n",
      "train epoch: 469, loss 0.1591\n",
      "test epoch: 469, loss: 0.53\n",
      "train epoch: 470, loss 0.1641\n",
      "test epoch: 470, loss: 0.52\n",
      "train epoch: 471, loss 0.1684\n",
      "test epoch: 471, loss: 0.52\n",
      "train epoch: 472, loss 0.1691\n",
      "test epoch: 472, loss: 0.52\n",
      "train epoch: 473, loss 0.1668\n",
      "test epoch: 473, loss: 0.53\n",
      "train epoch: 474, loss 0.1638\n",
      "test epoch: 474, loss: 0.54\n",
      "train epoch: 475, loss 0.1630\n",
      "test epoch: 475, loss: 0.53\n",
      "train epoch: 476, loss 0.1640\n",
      "test epoch: 476, loss: 0.53\n",
      "train epoch: 477, loss 0.1653\n",
      "test epoch: 477, loss: 0.52\n",
      "train epoch: 478, loss 0.1667\n",
      "test epoch: 478, loss: 0.52\n",
      "train epoch: 479, loss 0.1682\n",
      "test epoch: 479, loss: 0.51\n",
      "train epoch: 480, loss 0.1725\n",
      "test epoch: 480, loss: 0.50\n",
      "train epoch: 481, loss 0.1760\n",
      "test epoch: 481, loss: 0.48\n",
      "train epoch: 482, loss 0.1822\n",
      "test epoch: 482, loss: 0.47\n",
      "train epoch: 483, loss 0.1865\n",
      "test epoch: 483, loss: 0.47\n",
      "train epoch: 484, loss 0.1893\n",
      "test epoch: 484, loss: 0.47\n",
      "train epoch: 485, loss 0.1897\n",
      "test epoch: 485, loss: 0.47\n",
      "train epoch: 486, loss 0.1878\n",
      "test epoch: 486, loss: 0.47\n",
      "train epoch: 487, loss 0.1897\n",
      "test epoch: 487, loss: 0.46\n",
      "train epoch: 488, loss 0.1908\n",
      "test epoch: 488, loss: 0.43\n",
      "train epoch: 489, loss 0.2071\n",
      "test epoch: 489, loss: 0.41\n",
      "train epoch: 490, loss 0.2262\n",
      "test epoch: 490, loss: 0.38\n",
      "train epoch: 491, loss 0.2500\n",
      "test epoch: 491, loss: 0.35\n",
      "train epoch: 492, loss 0.2962\n",
      "test epoch: 492, loss: 0.30\n",
      "train epoch: 493, loss 0.3778\n",
      "test epoch: 493, loss: 0.27\n",
      "train epoch: 494, loss 0.4797\n",
      "test epoch: 494, loss: 0.24\n",
      "train epoch: 495, loss 0.5765\n",
      "test epoch: 495, loss: 0.23\n",
      "train epoch: 496, loss 0.6778\n",
      "test epoch: 496, loss: 0.20\n",
      "train epoch: 497, loss 0.9393\n",
      "test epoch: 497, loss: 0.20\n",
      "train epoch: 498, loss 1.1723\n",
      "test epoch: 498, loss: 0.20\n",
      "train epoch: 499, loss 1.4130\n",
      "test epoch: 499, loss: 0.21\n",
      "train epoch: 500, loss 1.6192\n",
      "test epoch: 500, loss: 0.22\n",
      "train epoch: 501, loss 1.7763\n",
      "test epoch: 501, loss: 0.23\n",
      "train epoch: 502, loss 1.8772\n",
      "test epoch: 502, loss: 0.24\n",
      "train epoch: 503, loss 1.9220\n",
      "test epoch: 503, loss: 0.24\n",
      "train epoch: 504, loss 1.9161\n",
      "test epoch: 504, loss: 0.23\n",
      "train epoch: 505, loss 1.8677\n",
      "test epoch: 505, loss: 0.22\n",
      "train epoch: 506, loss 1.7868\n",
      "test epoch: 506, loss: 0.22\n",
      "train epoch: 507, loss 1.6829\n",
      "test epoch: 507, loss: 0.21\n",
      "train epoch: 508, loss 1.5646\n",
      "test epoch: 508, loss: 0.20\n",
      "train epoch: 509, loss 1.5028\n",
      "test epoch: 509, loss: 0.20\n",
      "train epoch: 510, loss 1.4384\n",
      "test epoch: 510, loss: 0.20\n",
      "train epoch: 511, loss 1.3641\n",
      "test epoch: 511, loss: 0.20\n",
      "train epoch: 512, loss 1.2893\n",
      "test epoch: 512, loss: 0.20\n",
      "train epoch: 513, loss 1.2768\n",
      "test epoch: 513, loss: 0.20\n",
      "train epoch: 514, loss 1.1910\n",
      "test epoch: 514, loss: 0.19\n",
      "train epoch: 515, loss 1.0968\n",
      "test epoch: 515, loss: 0.20\n",
      "train epoch: 516, loss 1.0462\n",
      "test epoch: 516, loss: 0.20\n",
      "train epoch: 517, loss 0.9926\n",
      "test epoch: 517, loss: 0.20\n",
      "train epoch: 518, loss 0.9370\n",
      "test epoch: 518, loss: 0.20\n",
      "train epoch: 519, loss 0.8817\n",
      "test epoch: 519, loss: 0.21\n",
      "train epoch: 520, loss 0.8278\n",
      "test epoch: 520, loss: 0.21\n",
      "train epoch: 521, loss 0.7746\n",
      "test epoch: 521, loss: 0.22\n",
      "train epoch: 522, loss 0.7271\n",
      "test epoch: 522, loss: 0.23\n",
      "train epoch: 523, loss 0.6851\n",
      "test epoch: 523, loss: 0.23\n",
      "train epoch: 524, loss 0.6484\n",
      "test epoch: 524, loss: 0.23\n",
      "train epoch: 525, loss 0.6311\n",
      "test epoch: 525, loss: 0.24\n",
      "train epoch: 526, loss 0.6053\n",
      "test epoch: 526, loss: 0.25\n",
      "train epoch: 527, loss 0.5753\n",
      "test epoch: 527, loss: 0.25\n",
      "train epoch: 528, loss 0.5503\n",
      "test epoch: 528, loss: 0.26\n",
      "train epoch: 529, loss 0.5292\n",
      "test epoch: 529, loss: 0.26\n",
      "train epoch: 530, loss 0.5076\n",
      "test epoch: 530, loss: 0.27\n",
      "train epoch: 531, loss 0.4857\n",
      "test epoch: 531, loss: 0.28\n",
      "train epoch: 532, loss 0.4638\n",
      "test epoch: 532, loss: 0.28\n",
      "train epoch: 533, loss 0.4423\n",
      "test epoch: 533, loss: 0.29\n",
      "train epoch: 534, loss 0.4213\n",
      "test epoch: 534, loss: 0.30\n",
      "train epoch: 535, loss 0.4051\n",
      "test epoch: 535, loss: 0.31\n",
      "train epoch: 536, loss 0.3869\n",
      "test epoch: 536, loss: 0.31\n",
      "train epoch: 537, loss 0.3704\n",
      "test epoch: 537, loss: 0.32\n",
      "train epoch: 538, loss 0.3564\n",
      "test epoch: 538, loss: 0.33\n",
      "train epoch: 539, loss 0.3429\n",
      "test epoch: 539, loss: 0.33\n",
      "train epoch: 540, loss 0.3302\n",
      "test epoch: 540, loss: 0.34\n",
      "train epoch: 541, loss 0.3181\n",
      "test epoch: 541, loss: 0.35\n",
      "train epoch: 542, loss 0.3063\n",
      "test epoch: 542, loss: 0.36\n",
      "train epoch: 543, loss 0.2951\n",
      "test epoch: 543, loss: 0.36\n",
      "train epoch: 544, loss 0.2862\n",
      "test epoch: 544, loss: 0.37\n",
      "train epoch: 545, loss 0.2781\n",
      "test epoch: 545, loss: 0.38\n",
      "train epoch: 546, loss 0.2694\n",
      "test epoch: 546, loss: 0.38\n",
      "train epoch: 547, loss 0.2619\n",
      "test epoch: 547, loss: 0.39\n",
      "train epoch: 548, loss 0.2545\n",
      "test epoch: 548, loss: 0.40\n",
      "train epoch: 549, loss 0.2478\n",
      "test epoch: 549, loss: 0.40\n",
      "train epoch: 550, loss 0.2416\n",
      "test epoch: 550, loss: 0.41\n",
      "train epoch: 551, loss 0.2363\n",
      "test epoch: 551, loss: 0.41\n",
      "train epoch: 552, loss 0.2317\n",
      "test epoch: 552, loss: 0.42\n",
      "train epoch: 553, loss 0.2272\n",
      "test epoch: 553, loss: 0.42\n",
      "train epoch: 554, loss 0.2229\n",
      "test epoch: 554, loss: 0.43\n",
      "train epoch: 555, loss 0.2186\n",
      "test epoch: 555, loss: 0.43\n",
      "train epoch: 556, loss 0.2144\n",
      "test epoch: 556, loss: 0.44\n",
      "train epoch: 557, loss 0.2105\n",
      "test epoch: 557, loss: 0.44\n",
      "train epoch: 558, loss 0.2072\n",
      "test epoch: 558, loss: 0.45\n",
      "train epoch: 559, loss 0.2045\n",
      "test epoch: 559, loss: 0.45\n",
      "train epoch: 560, loss 0.2043\n",
      "test epoch: 560, loss: 0.45\n",
      "train epoch: 561, loss 0.1991\n",
      "test epoch: 561, loss: 0.46\n",
      "train epoch: 562, loss 0.1969\n",
      "test epoch: 562, loss: 0.46\n",
      "train epoch: 563, loss 0.1948\n",
      "test epoch: 563, loss: 0.46\n",
      "train epoch: 564, loss 0.1929\n",
      "test epoch: 564, loss: 0.47\n",
      "train epoch: 565, loss 0.1911\n",
      "test epoch: 565, loss: 0.47\n",
      "train epoch: 566, loss 0.1892\n",
      "test epoch: 566, loss: 0.47\n",
      "train epoch: 567, loss 0.1872\n",
      "test epoch: 567, loss: 0.48\n",
      "train epoch: 568, loss 0.1855\n",
      "test epoch: 568, loss: 0.48\n",
      "train epoch: 569, loss 0.1842\n",
      "test epoch: 569, loss: 0.48\n",
      "train epoch: 570, loss 0.1827\n",
      "test epoch: 570, loss: 0.49\n",
      "train epoch: 571, loss 0.1813\n",
      "test epoch: 571, loss: 0.49\n",
      "train epoch: 572, loss 0.1802\n",
      "test epoch: 572, loss: 0.49\n",
      "train epoch: 573, loss 0.1790\n",
      "test epoch: 573, loss: 0.49\n",
      "train epoch: 574, loss 0.1777\n",
      "test epoch: 574, loss: 0.50\n",
      "train epoch: 575, loss 0.1767\n",
      "test epoch: 575, loss: 0.50\n",
      "train epoch: 576, loss 0.1759\n",
      "test epoch: 576, loss: 0.50\n",
      "train epoch: 577, loss 0.1749\n",
      "test epoch: 577, loss: 0.50\n",
      "train epoch: 578, loss 0.1739\n",
      "test epoch: 578, loss: 0.50\n",
      "train epoch: 579, loss 0.1731\n",
      "test epoch: 579, loss: 0.51\n",
      "train epoch: 580, loss 0.1724\n",
      "test epoch: 580, loss: 0.51\n",
      "train epoch: 581, loss 0.1716\n",
      "test epoch: 581, loss: 0.51\n",
      "train epoch: 582, loss 0.1708\n",
      "test epoch: 582, loss: 0.51\n",
      "train epoch: 583, loss 0.1703\n",
      "test epoch: 583, loss: 0.51\n",
      "train epoch: 584, loss 0.1696\n",
      "test epoch: 584, loss: 0.52\n",
      "train epoch: 585, loss 0.1689\n",
      "test epoch: 585, loss: 0.52\n",
      "train epoch: 586, loss 0.1683\n",
      "test epoch: 586, loss: 0.52\n",
      "train epoch: 587, loss 0.1678\n",
      "test epoch: 587, loss: 0.52\n",
      "train epoch: 588, loss 0.1676\n",
      "test epoch: 588, loss: 0.52\n",
      "train epoch: 589, loss 0.1670\n",
      "test epoch: 589, loss: 0.52\n",
      "train epoch: 590, loss 0.1661\n",
      "test epoch: 590, loss: 0.53\n",
      "train epoch: 591, loss 0.1657\n",
      "test epoch: 591, loss: 0.53\n",
      "train epoch: 592, loss 0.1653\n",
      "test epoch: 592, loss: 0.53\n",
      "train epoch: 593, loss 0.1649\n",
      "test epoch: 593, loss: 0.53\n",
      "train epoch: 594, loss 0.1645\n",
      "test epoch: 594, loss: 0.53\n",
      "train epoch: 595, loss 0.1641\n",
      "test epoch: 595, loss: 0.53\n",
      "train epoch: 596, loss 0.1636\n",
      "test epoch: 596, loss: 0.53\n",
      "train epoch: 597, loss 0.1631\n",
      "test epoch: 597, loss: 0.53\n",
      "train epoch: 598, loss 0.1627\n",
      "test epoch: 598, loss: 0.54\n",
      "train epoch: 599, loss 0.1624\n",
      "test epoch: 599, loss: 0.54\n",
      "train epoch: 600, loss 0.1620\n",
      "test epoch: 600, loss: 0.54\n",
      "train epoch: 601, loss 0.1616\n",
      "test epoch: 601, loss: 0.54\n",
      "train epoch: 602, loss 0.1613\n",
      "test epoch: 602, loss: 0.54\n",
      "train epoch: 603, loss 0.1610\n",
      "test epoch: 603, loss: 0.54\n",
      "train epoch: 604, loss 0.1607\n",
      "test epoch: 604, loss: 0.54\n",
      "train epoch: 605, loss 0.1604\n",
      "test epoch: 605, loss: 0.54\n",
      "train epoch: 606, loss 0.1601\n",
      "test epoch: 606, loss: 0.54\n",
      "train epoch: 607, loss 0.1598\n",
      "test epoch: 607, loss: 0.55\n",
      "train epoch: 608, loss 0.1595\n",
      "test epoch: 608, loss: 0.55\n",
      "train epoch: 609, loss 0.1592\n",
      "test epoch: 609, loss: 0.55\n",
      "train epoch: 610, loss 0.1589\n",
      "test epoch: 610, loss: 0.55\n",
      "train epoch: 611, loss 0.1587\n",
      "test epoch: 611, loss: 0.55\n",
      "train epoch: 612, loss 0.1584\n",
      "test epoch: 612, loss: 0.55\n",
      "train epoch: 613, loss 0.1582\n",
      "test epoch: 613, loss: 0.55\n",
      "train epoch: 614, loss 0.1579\n",
      "test epoch: 614, loss: 0.55\n",
      "train epoch: 615, loss 0.1577\n",
      "test epoch: 615, loss: 0.56\n",
      "train epoch: 616, loss 0.1574\n",
      "test epoch: 616, loss: 0.56\n",
      "train epoch: 617, loss 0.1572\n",
      "test epoch: 617, loss: 0.56\n",
      "train epoch: 618, loss 0.1569\n",
      "test epoch: 618, loss: 0.56\n",
      "train epoch: 619, loss 0.1567\n",
      "test epoch: 619, loss: 0.56\n",
      "train epoch: 620, loss 0.1565\n",
      "test epoch: 620, loss: 0.56\n",
      "train epoch: 621, loss 0.1563\n",
      "test epoch: 621, loss: 0.56\n",
      "train epoch: 622, loss 0.1561\n",
      "test epoch: 622, loss: 0.56\n",
      "train epoch: 623, loss 0.1558\n",
      "test epoch: 623, loss: 0.56\n",
      "train epoch: 624, loss 0.1556\n",
      "test epoch: 624, loss: 0.57\n",
      "train epoch: 625, loss 0.1554\n",
      "test epoch: 625, loss: 0.57\n",
      "train epoch: 626, loss 0.1553\n",
      "test epoch: 626, loss: 0.57\n",
      "train epoch: 627, loss 0.1551\n",
      "test epoch: 627, loss: 0.57\n",
      "train epoch: 628, loss 0.1549\n",
      "test epoch: 628, loss: 0.57\n",
      "train epoch: 629, loss 0.1547\n",
      "test epoch: 629, loss: 0.57\n",
      "train epoch: 630, loss 0.1545\n",
      "test epoch: 630, loss: 0.57\n",
      "train epoch: 631, loss 0.1544\n",
      "test epoch: 631, loss: 0.57\n",
      "train epoch: 632, loss 0.1542\n",
      "test epoch: 632, loss: 0.57\n",
      "train epoch: 633, loss 0.1540\n",
      "test epoch: 633, loss: 0.58\n",
      "train epoch: 634, loss 0.1539\n",
      "test epoch: 634, loss: 0.58\n",
      "train epoch: 635, loss 0.1537\n",
      "test epoch: 635, loss: 0.58\n",
      "train epoch: 636, loss 0.1536\n",
      "test epoch: 636, loss: 0.58\n",
      "train epoch: 637, loss 0.1534\n",
      "test epoch: 637, loss: 0.58\n",
      "train epoch: 638, loss 0.1533\n",
      "test epoch: 638, loss: 0.58\n",
      "train epoch: 639, loss 0.1532\n",
      "test epoch: 639, loss: 0.58\n",
      "train epoch: 640, loss 0.1531\n",
      "test epoch: 640, loss: 0.58\n",
      "train epoch: 641, loss 0.1529\n",
      "test epoch: 641, loss: 0.59\n",
      "train epoch: 642, loss 0.1528\n",
      "test epoch: 642, loss: 0.59\n",
      "train epoch: 643, loss 0.1527\n",
      "test epoch: 643, loss: 0.59\n",
      "train epoch: 644, loss 0.1526\n",
      "test epoch: 644, loss: 0.59\n",
      "train epoch: 645, loss 0.1525\n",
      "test epoch: 645, loss: 0.59\n",
      "train epoch: 646, loss 0.1524\n",
      "test epoch: 646, loss: 0.59\n",
      "train epoch: 647, loss 0.1523\n",
      "test epoch: 647, loss: 0.59\n",
      "train epoch: 648, loss 0.1522\n",
      "test epoch: 648, loss: 0.59\n",
      "train epoch: 649, loss 0.1521\n",
      "test epoch: 649, loss: 0.59\n",
      "train epoch: 650, loss 0.1520\n",
      "test epoch: 650, loss: 0.59\n",
      "train epoch: 651, loss 0.1520\n",
      "test epoch: 651, loss: 0.60\n",
      "train epoch: 652, loss 0.1519\n",
      "test epoch: 652, loss: 0.60\n",
      "train epoch: 653, loss 0.1518\n",
      "test epoch: 653, loss: 0.60\n",
      "train epoch: 654, loss 0.1517\n",
      "test epoch: 654, loss: 0.60\n",
      "train epoch: 655, loss 0.1516\n",
      "test epoch: 655, loss: 0.60\n",
      "train epoch: 656, loss 0.1516\n",
      "test epoch: 656, loss: 0.60\n",
      "train epoch: 657, loss 0.1515\n",
      "test epoch: 657, loss: 0.60\n",
      "train epoch: 658, loss 0.1515\n",
      "test epoch: 658, loss: 0.60\n",
      "train epoch: 659, loss 0.1513\n",
      "test epoch: 659, loss: 0.60\n",
      "train epoch: 660, loss 0.1513\n",
      "test epoch: 660, loss: 0.60\n",
      "train epoch: 661, loss 0.1513\n",
      "test epoch: 661, loss: 0.60\n",
      "train epoch: 662, loss 0.1512\n",
      "test epoch: 662, loss: 0.61\n",
      "train epoch: 663, loss 0.1513\n",
      "test epoch: 663, loss: 0.61\n",
      "train epoch: 664, loss 0.1510\n",
      "test epoch: 664, loss: 0.61\n",
      "train epoch: 665, loss 0.1507\n",
      "test epoch: 665, loss: 0.61\n",
      "train epoch: 666, loss 0.1508\n",
      "test epoch: 666, loss: 0.61\n",
      "train epoch: 667, loss 0.1507\n",
      "test epoch: 667, loss: 0.61\n",
      "train epoch: 668, loss 0.1507\n",
      "test epoch: 668, loss: 0.61\n",
      "train epoch: 669, loss 0.1506\n",
      "test epoch: 669, loss: 0.61\n",
      "train epoch: 670, loss 0.1506\n",
      "test epoch: 670, loss: 0.61\n",
      "train epoch: 671, loss 0.1504\n",
      "test epoch: 671, loss: 0.61\n",
      "train epoch: 672, loss 0.1502\n",
      "test epoch: 672, loss: 0.61\n",
      "train epoch: 673, loss 0.1501\n",
      "test epoch: 673, loss: 0.61\n",
      "train epoch: 674, loss 0.1501\n",
      "test epoch: 674, loss: 0.61\n",
      "train epoch: 675, loss 0.1500\n",
      "test epoch: 675, loss: 0.62\n",
      "train epoch: 676, loss 0.1499\n",
      "test epoch: 676, loss: 0.62\n",
      "train epoch: 677, loss 0.1500\n",
      "test epoch: 677, loss: 0.62\n",
      "train epoch: 678, loss 0.1498\n",
      "test epoch: 678, loss: 0.62\n",
      "train epoch: 679, loss 0.1497\n",
      "test epoch: 679, loss: 0.62\n",
      "train epoch: 680, loss 0.1493\n",
      "test epoch: 680, loss: 0.62\n",
      "train epoch: 681, loss 0.1493\n",
      "test epoch: 681, loss: 0.62\n",
      "train epoch: 682, loss 0.1493\n",
      "test epoch: 682, loss: 0.62\n",
      "train epoch: 683, loss 0.1492\n",
      "test epoch: 683, loss: 0.62\n",
      "train epoch: 684, loss 0.1492\n",
      "test epoch: 684, loss: 0.62\n",
      "train epoch: 685, loss 0.1492\n",
      "test epoch: 685, loss: 0.62\n",
      "train epoch: 686, loss 0.1491\n",
      "test epoch: 686, loss: 0.62\n",
      "train epoch: 687, loss 0.1495\n",
      "test epoch: 687, loss: 0.62\n",
      "train epoch: 688, loss 0.1492\n",
      "test epoch: 688, loss: 0.62\n",
      "train epoch: 689, loss 0.1493\n",
      "test epoch: 689, loss: 0.63\n",
      "train epoch: 690, loss 0.1493\n",
      "test epoch: 690, loss: 0.63\n",
      "train epoch: 691, loss 0.1489\n",
      "test epoch: 691, loss: 0.63\n",
      "train epoch: 692, loss 0.1491\n",
      "test epoch: 692, loss: 0.63\n",
      "train epoch: 693, loss 0.1489\n",
      "test epoch: 693, loss: 0.63\n",
      "train epoch: 694, loss 0.1487\n",
      "test epoch: 694, loss: 0.63\n",
      "train epoch: 695, loss 0.1490\n",
      "test epoch: 695, loss: 0.63\n",
      "train epoch: 696, loss 0.1489\n",
      "test epoch: 696, loss: 0.63\n",
      "train epoch: 697, loss 0.1488\n",
      "test epoch: 697, loss: 0.63\n",
      "train epoch: 698, loss 0.1488\n",
      "test epoch: 698, loss: 0.63\n",
      "train epoch: 699, loss 0.1487\n",
      "test epoch: 699, loss: 0.63\n",
      "train epoch: 700, loss 0.1487\n",
      "test epoch: 700, loss: 0.63\n",
      "train epoch: 701, loss 0.1486\n",
      "test epoch: 701, loss: 0.63\n",
      "train epoch: 702, loss 0.1486\n",
      "test epoch: 702, loss: 0.63\n",
      "train epoch: 703, loss 0.1486\n",
      "test epoch: 703, loss: 0.63\n",
      "train epoch: 704, loss 0.1485\n",
      "test epoch: 704, loss: 0.63\n",
      "train epoch: 705, loss 0.1485\n",
      "test epoch: 705, loss: 0.63\n",
      "train epoch: 706, loss 0.1485\n",
      "test epoch: 706, loss: 0.63\n",
      "train epoch: 707, loss 0.1484\n",
      "test epoch: 707, loss: 0.63\n",
      "train epoch: 708, loss 0.1484\n",
      "test epoch: 708, loss: 0.63\n",
      "train epoch: 709, loss 0.1484\n",
      "test epoch: 709, loss: 0.64\n",
      "train epoch: 710, loss 0.1481\n",
      "test epoch: 710, loss: 0.64\n",
      "train epoch: 711, loss 0.1480\n",
      "test epoch: 711, loss: 0.64\n",
      "train epoch: 712, loss 0.1482\n",
      "test epoch: 712, loss: 0.64\n",
      "train epoch: 713, loss 0.1479\n",
      "test epoch: 713, loss: 0.64\n",
      "train epoch: 714, loss 0.1478\n",
      "test epoch: 714, loss: 0.64\n",
      "train epoch: 715, loss 0.1478\n",
      "test epoch: 715, loss: 0.64\n",
      "train epoch: 716, loss 0.1477\n",
      "test epoch: 716, loss: 0.64\n",
      "train epoch: 717, loss 0.1477\n",
      "test epoch: 717, loss: 0.64\n",
      "train epoch: 718, loss 0.1475\n",
      "test epoch: 718, loss: 0.64\n",
      "train epoch: 719, loss 0.1475\n",
      "test epoch: 719, loss: 0.64\n",
      "train epoch: 720, loss 0.1474\n",
      "test epoch: 720, loss: 0.64\n",
      "train epoch: 721, loss 0.1473\n",
      "test epoch: 721, loss: 0.64\n",
      "train epoch: 722, loss 0.1472\n",
      "test epoch: 722, loss: 0.64\n",
      "train epoch: 723, loss 0.1472\n",
      "test epoch: 723, loss: 0.64\n",
      "train epoch: 724, loss 0.1472\n",
      "test epoch: 724, loss: 0.64\n",
      "train epoch: 725, loss 0.1471\n",
      "test epoch: 725, loss: 0.64\n",
      "train epoch: 726, loss 0.1471\n",
      "test epoch: 726, loss: 0.65\n",
      "train epoch: 727, loss 0.1469\n",
      "test epoch: 727, loss: 0.65\n",
      "train epoch: 728, loss 0.1468\n",
      "test epoch: 728, loss: 0.65\n",
      "train epoch: 729, loss 0.1467\n",
      "test epoch: 729, loss: 0.65\n",
      "train epoch: 730, loss 0.1467\n",
      "test epoch: 730, loss: 0.65\n",
      "train epoch: 731, loss 0.1466\n",
      "test epoch: 731, loss: 0.65\n",
      "train epoch: 732, loss 0.1467\n",
      "test epoch: 732, loss: 0.65\n",
      "train epoch: 733, loss 0.1463\n",
      "test epoch: 733, loss: 0.65\n",
      "train epoch: 734, loss 0.1466\n",
      "test epoch: 734, loss: 0.65\n",
      "train epoch: 735, loss 0.1462\n",
      "test epoch: 735, loss: 0.65\n",
      "train epoch: 736, loss 0.1464\n",
      "test epoch: 736, loss: 0.65\n",
      "train epoch: 737, loss 0.1461\n",
      "test epoch: 737, loss: 0.65\n",
      "train epoch: 738, loss 0.1463\n",
      "test epoch: 738, loss: 0.65\n",
      "train epoch: 739, loss 0.1460\n",
      "test epoch: 739, loss: 0.65\n",
      "train epoch: 740, loss 0.1462\n",
      "test epoch: 740, loss: 0.65\n",
      "train epoch: 741, loss 0.1460\n",
      "test epoch: 741, loss: 0.65\n",
      "train epoch: 742, loss 0.1460\n",
      "test epoch: 742, loss: 0.65\n",
      "train epoch: 743, loss 0.1458\n",
      "test epoch: 743, loss: 0.65\n",
      "train epoch: 744, loss 0.1457\n",
      "test epoch: 744, loss: 0.65\n",
      "train epoch: 745, loss 0.1455\n",
      "test epoch: 745, loss: 0.65\n",
      "train epoch: 746, loss 0.1455\n",
      "test epoch: 746, loss: 0.65\n",
      "train epoch: 747, loss 0.1454\n",
      "test epoch: 747, loss: 0.65\n",
      "train epoch: 748, loss 0.1455\n",
      "test epoch: 748, loss: 0.65\n",
      "train epoch: 749, loss 0.1454\n",
      "test epoch: 749, loss: 0.66\n",
      "train epoch: 750, loss 0.1454\n",
      "test epoch: 750, loss: 0.66\n",
      "train epoch: 751, loss 0.1452\n",
      "test epoch: 751, loss: 0.65\n",
      "train epoch: 752, loss 0.1453\n",
      "test epoch: 752, loss: 0.66\n",
      "train epoch: 753, loss 0.1452\n",
      "test epoch: 753, loss: 0.66\n",
      "train epoch: 754, loss 0.1452\n",
      "test epoch: 754, loss: 0.66\n",
      "train epoch: 755, loss 0.1450\n",
      "test epoch: 755, loss: 0.66\n",
      "train epoch: 756, loss 0.1450\n",
      "test epoch: 756, loss: 0.66\n",
      "train epoch: 757, loss 0.1449\n",
      "test epoch: 757, loss: 0.66\n",
      "train epoch: 758, loss 0.1449\n",
      "test epoch: 758, loss: 0.66\n",
      "train epoch: 759, loss 0.1448\n",
      "test epoch: 759, loss: 0.66\n",
      "train epoch: 760, loss 0.1448\n",
      "test epoch: 760, loss: 0.66\n",
      "train epoch: 761, loss 0.1447\n",
      "test epoch: 761, loss: 0.66\n",
      "train epoch: 762, loss 0.1447\n",
      "test epoch: 762, loss: 0.66\n",
      "train epoch: 763, loss 0.1446\n",
      "test epoch: 763, loss: 0.66\n",
      "train epoch: 764, loss 0.1446\n",
      "test epoch: 764, loss: 0.66\n",
      "train epoch: 765, loss 0.1445\n",
      "test epoch: 765, loss: 0.66\n",
      "train epoch: 766, loss 0.1445\n",
      "test epoch: 766, loss: 0.66\n",
      "train epoch: 767, loss 0.1444\n",
      "test epoch: 767, loss: 0.66\n",
      "train epoch: 768, loss 0.1443\n",
      "test epoch: 768, loss: 0.66\n",
      "train epoch: 769, loss 0.1443\n",
      "test epoch: 769, loss: 0.66\n",
      "train epoch: 770, loss 0.1442\n",
      "test epoch: 770, loss: 0.66\n",
      "train epoch: 771, loss 0.1442\n",
      "test epoch: 771, loss: 0.66\n",
      "train epoch: 772, loss 0.1441\n",
      "test epoch: 772, loss: 0.66\n",
      "train epoch: 773, loss 0.1440\n",
      "test epoch: 773, loss: 0.66\n",
      "train epoch: 774, loss 0.1441\n",
      "test epoch: 774, loss: 0.66\n",
      "train epoch: 775, loss 0.1439\n",
      "test epoch: 775, loss: 0.66\n",
      "train epoch: 776, loss 0.1439\n",
      "test epoch: 776, loss: 0.66\n",
      "train epoch: 777, loss 0.1439\n",
      "test epoch: 777, loss: 0.66\n",
      "train epoch: 778, loss 0.1437\n",
      "test epoch: 778, loss: 0.66\n",
      "train epoch: 779, loss 0.1439\n",
      "test epoch: 779, loss: 0.66\n",
      "train epoch: 780, loss 0.1440\n",
      "test epoch: 780, loss: 0.66\n",
      "train epoch: 781, loss 0.1437\n",
      "test epoch: 781, loss: 0.66\n",
      "train epoch: 782, loss 0.1440\n",
      "test epoch: 782, loss: 0.66\n",
      "train epoch: 783, loss 0.1441\n",
      "test epoch: 783, loss: 0.66\n",
      "train epoch: 784, loss 0.1440\n",
      "test epoch: 784, loss: 0.66\n",
      "train epoch: 785, loss 0.1437\n",
      "test epoch: 785, loss: 0.66\n",
      "train epoch: 786, loss 0.1433\n",
      "test epoch: 786, loss: 0.66\n",
      "train epoch: 787, loss 0.1437\n",
      "test epoch: 787, loss: 0.66\n",
      "train epoch: 788, loss 0.1433\n",
      "test epoch: 788, loss: 0.66\n",
      "train epoch: 789, loss 0.1437\n",
      "test epoch: 789, loss: 0.66\n",
      "train epoch: 790, loss 0.1433\n",
      "test epoch: 790, loss: 0.66\n",
      "train epoch: 791, loss 0.1433\n",
      "test epoch: 791, loss: 0.66\n",
      "train epoch: 792, loss 0.1431\n",
      "test epoch: 792, loss: 0.66\n",
      "train epoch: 793, loss 0.1429\n",
      "test epoch: 793, loss: 0.66\n",
      "train epoch: 794, loss 0.1431\n",
      "test epoch: 794, loss: 0.66\n",
      "train epoch: 795, loss 0.1427\n",
      "test epoch: 795, loss: 0.66\n",
      "train epoch: 796, loss 0.1430\n",
      "test epoch: 796, loss: 0.66\n",
      "train epoch: 797, loss 0.1428\n",
      "test epoch: 797, loss: 0.66\n",
      "train epoch: 798, loss 0.1428\n",
      "test epoch: 798, loss: 0.66\n",
      "train epoch: 799, loss 0.1425\n",
      "test epoch: 799, loss: 0.66\n",
      "train epoch: 800, loss 0.1424\n",
      "test epoch: 800, loss: 0.66\n",
      "train epoch: 801, loss 0.1425\n",
      "test epoch: 801, loss: 0.66\n",
      "train epoch: 802, loss 0.1425\n",
      "test epoch: 802, loss: 0.66\n",
      "train epoch: 803, loss 0.1422\n",
      "test epoch: 803, loss: 0.66\n",
      "train epoch: 804, loss 0.1422\n",
      "test epoch: 804, loss: 0.66\n",
      "train epoch: 805, loss 0.1421\n",
      "test epoch: 805, loss: 0.66\n",
      "train epoch: 806, loss 0.1421\n",
      "test epoch: 806, loss: 0.66\n",
      "train epoch: 807, loss 0.1420\n",
      "test epoch: 807, loss: 0.66\n",
      "train epoch: 808, loss 0.1420\n",
      "test epoch: 808, loss: 0.66\n",
      "train epoch: 809, loss 0.1419\n",
      "test epoch: 809, loss: 0.66\n",
      "train epoch: 810, loss 0.1419\n",
      "test epoch: 810, loss: 0.66\n",
      "train epoch: 811, loss 0.1418\n",
      "test epoch: 811, loss: 0.66\n",
      "train epoch: 812, loss 0.1417\n",
      "test epoch: 812, loss: 0.66\n",
      "train epoch: 813, loss 0.1417\n",
      "test epoch: 813, loss: 0.66\n",
      "train epoch: 814, loss 0.1417\n",
      "test epoch: 814, loss: 0.66\n",
      "train epoch: 815, loss 0.1415\n",
      "test epoch: 815, loss: 0.66\n",
      "train epoch: 816, loss 0.1415\n",
      "test epoch: 816, loss: 0.66\n",
      "train epoch: 817, loss 0.1414\n",
      "test epoch: 817, loss: 0.66\n",
      "train epoch: 818, loss 0.1413\n",
      "test epoch: 818, loss: 0.66\n",
      "train epoch: 819, loss 0.1412\n",
      "test epoch: 819, loss: 0.66\n",
      "train epoch: 820, loss 0.1412\n",
      "test epoch: 820, loss: 0.66\n",
      "train epoch: 821, loss 0.1410\n",
      "test epoch: 821, loss: 0.66\n",
      "train epoch: 822, loss 0.1410\n",
      "test epoch: 822, loss: 0.66\n",
      "train epoch: 823, loss 0.1410\n",
      "test epoch: 823, loss: 0.67\n",
      "train epoch: 824, loss 0.1409\n",
      "test epoch: 824, loss: 0.66\n",
      "train epoch: 825, loss 0.1409\n",
      "test epoch: 825, loss: 0.66\n",
      "train epoch: 826, loss 0.1407\n",
      "test epoch: 826, loss: 0.67\n",
      "train epoch: 827, loss 0.1407\n",
      "test epoch: 827, loss: 0.66\n",
      "train epoch: 828, loss 0.1407\n",
      "test epoch: 828, loss: 0.67\n",
      "train epoch: 829, loss 0.1405\n",
      "test epoch: 829, loss: 0.67\n",
      "train epoch: 830, loss 0.1408\n",
      "test epoch: 830, loss: 0.66\n",
      "train epoch: 831, loss 0.1416\n",
      "test epoch: 831, loss: 0.66\n",
      "train epoch: 832, loss 0.1406\n",
      "test epoch: 832, loss: 0.67\n",
      "train epoch: 833, loss 0.1416\n",
      "test epoch: 833, loss: 0.66\n",
      "train epoch: 834, loss 0.1408\n",
      "test epoch: 834, loss: 0.66\n",
      "train epoch: 835, loss 0.1410\n",
      "test epoch: 835, loss: 0.67\n",
      "train epoch: 836, loss 0.1404\n",
      "test epoch: 836, loss: 0.67\n",
      "train epoch: 837, loss 0.1402\n",
      "test epoch: 837, loss: 0.67\n",
      "train epoch: 838, loss 0.1408\n",
      "test epoch: 838, loss: 0.67\n",
      "train epoch: 839, loss 0.1401\n",
      "test epoch: 839, loss: 0.67\n",
      "train epoch: 840, loss 0.1404\n",
      "test epoch: 840, loss: 0.67\n",
      "train epoch: 841, loss 0.1397\n",
      "test epoch: 841, loss: 0.66\n",
      "train epoch: 842, loss 0.1410\n",
      "test epoch: 842, loss: 0.67\n",
      "train epoch: 843, loss 0.1431\n",
      "test epoch: 843, loss: 0.67\n",
      "train epoch: 844, loss 0.1394\n",
      "test epoch: 844, loss: 0.67\n",
      "train epoch: 845, loss 0.1418\n",
      "test epoch: 845, loss: 0.67\n",
      "train epoch: 846, loss 0.1409\n",
      "test epoch: 846, loss: 0.67\n",
      "train epoch: 847, loss 0.1394\n",
      "test epoch: 847, loss: 0.67\n",
      "train epoch: 848, loss 0.1406\n",
      "test epoch: 848, loss: 0.67\n",
      "train epoch: 849, loss 0.1393\n",
      "test epoch: 849, loss: 0.67\n",
      "train epoch: 850, loss 0.1407\n",
      "test epoch: 850, loss: 0.67\n",
      "train epoch: 851, loss 0.1397\n",
      "test epoch: 851, loss: 0.67\n",
      "train epoch: 852, loss 0.1393\n",
      "test epoch: 852, loss: 0.67\n",
      "train epoch: 853, loss 0.1395\n",
      "test epoch: 853, loss: 0.67\n",
      "train epoch: 854, loss 0.1390\n",
      "test epoch: 854, loss: 0.67\n",
      "train epoch: 855, loss 0.1398\n",
      "test epoch: 855, loss: 0.67\n",
      "train epoch: 856, loss 0.1389\n",
      "test epoch: 856, loss: 0.67\n",
      "train epoch: 857, loss 0.1389\n",
      "test epoch: 857, loss: 0.67\n",
      "train epoch: 858, loss 0.1386\n",
      "test epoch: 858, loss: 0.67\n",
      "train epoch: 859, loss 0.1408\n",
      "test epoch: 859, loss: 0.68\n",
      "train epoch: 860, loss 0.1426\n",
      "test epoch: 860, loss: 0.68\n",
      "train epoch: 861, loss 0.1395\n",
      "test epoch: 861, loss: 0.67\n",
      "train epoch: 862, loss 0.1396\n",
      "test epoch: 862, loss: 0.67\n",
      "train epoch: 863, loss 0.1419\n",
      "test epoch: 863, loss: 0.68\n",
      "train epoch: 864, loss 0.1398\n",
      "test epoch: 864, loss: 0.68\n",
      "train epoch: 865, loss 0.1381\n",
      "test epoch: 865, loss: 0.68\n",
      "train epoch: 866, loss 0.1396\n",
      "test epoch: 866, loss: 0.68\n",
      "train epoch: 867, loss 0.1376\n",
      "test epoch: 867, loss: 0.67\n",
      "train epoch: 868, loss 0.1389\n",
      "test epoch: 868, loss: 0.67\n",
      "train epoch: 869, loss 0.1384\n",
      "test epoch: 869, loss: 0.67\n",
      "train epoch: 870, loss 0.1374\n",
      "test epoch: 870, loss: 0.67\n",
      "train epoch: 871, loss 0.1382\n",
      "test epoch: 871, loss: 0.67\n",
      "train epoch: 872, loss 0.1373\n",
      "test epoch: 872, loss: 0.67\n",
      "train epoch: 873, loss 0.1382\n",
      "test epoch: 873, loss: 0.67\n",
      "train epoch: 874, loss 0.1375\n",
      "test epoch: 874, loss: 0.67\n",
      "train epoch: 875, loss 0.1372\n",
      "test epoch: 875, loss: 0.67\n",
      "train epoch: 876, loss 0.1372\n",
      "test epoch: 876, loss: 0.67\n",
      "train epoch: 877, loss 0.1370\n",
      "test epoch: 877, loss: 0.67\n",
      "train epoch: 878, loss 0.1373\n",
      "test epoch: 878, loss: 0.67\n",
      "train epoch: 879, loss 0.1367\n",
      "test epoch: 879, loss: 0.67\n",
      "train epoch: 880, loss 0.1367\n",
      "test epoch: 880, loss: 0.67\n",
      "train epoch: 881, loss 0.1372\n",
      "test epoch: 881, loss: 0.67\n",
      "train epoch: 882, loss 0.1376\n",
      "test epoch: 882, loss: 0.67\n",
      "train epoch: 883, loss 0.1362\n",
      "test epoch: 883, loss: 0.67\n",
      "train epoch: 884, loss 0.1389\n",
      "test epoch: 884, loss: 0.68\n",
      "train epoch: 885, loss 0.1378\n",
      "test epoch: 885, loss: 0.68\n",
      "train epoch: 886, loss 0.1385\n",
      "test epoch: 886, loss: 0.68\n",
      "train epoch: 887, loss 0.1366\n",
      "test epoch: 887, loss: 0.68\n",
      "train epoch: 888, loss 0.1394\n",
      "test epoch: 888, loss: 0.68\n",
      "train epoch: 889, loss 0.1377\n",
      "test epoch: 889, loss: 0.68\n",
      "train epoch: 890, loss 0.1364\n",
      "test epoch: 890, loss: 0.68\n",
      "train epoch: 891, loss 0.1379\n",
      "test epoch: 891, loss: 0.68\n",
      "train epoch: 892, loss 0.1357\n",
      "test epoch: 892, loss: 0.68\n",
      "train epoch: 893, loss 0.1375\n",
      "test epoch: 893, loss: 0.68\n",
      "train epoch: 894, loss 0.1371\n",
      "test epoch: 894, loss: 0.68\n",
      "train epoch: 895, loss 0.1354\n",
      "test epoch: 895, loss: 0.68\n",
      "train epoch: 896, loss 0.1367\n",
      "test epoch: 896, loss: 0.68\n",
      "train epoch: 897, loss 0.1355\n",
      "test epoch: 897, loss: 0.68\n",
      "train epoch: 898, loss 0.1360\n",
      "test epoch: 898, loss: 0.68\n",
      "train epoch: 899, loss 0.1365\n",
      "test epoch: 899, loss: 0.68\n",
      "train epoch: 900, loss 0.1351\n",
      "test epoch: 900, loss: 0.68\n",
      "train epoch: 901, loss 0.1355\n",
      "test epoch: 901, loss: 0.68\n",
      "train epoch: 902, loss 0.1350\n",
      "test epoch: 902, loss: 0.68\n",
      "train epoch: 903, loss 0.1351\n",
      "test epoch: 903, loss: 0.68\n",
      "train epoch: 904, loss 0.1355\n",
      "test epoch: 904, loss: 0.68\n",
      "train epoch: 905, loss 0.1347\n",
      "test epoch: 905, loss: 0.68\n",
      "train epoch: 906, loss 0.1348\n",
      "test epoch: 906, loss: 0.68\n",
      "train epoch: 907, loss 0.1346\n",
      "test epoch: 907, loss: 0.68\n",
      "train epoch: 908, loss 0.1356\n",
      "test epoch: 908, loss: 0.68\n",
      "train epoch: 909, loss 0.1376\n",
      "test epoch: 909, loss: 0.68\n",
      "train epoch: 910, loss 0.1342\n",
      "test epoch: 910, loss: 0.68\n",
      "train epoch: 911, loss 0.1362\n",
      "test epoch: 911, loss: 0.68\n",
      "train epoch: 912, loss 0.1355\n",
      "test epoch: 912, loss: 0.68\n",
      "train epoch: 913, loss 0.1338\n",
      "test epoch: 913, loss: 0.68\n",
      "train epoch: 914, loss 0.1351\n",
      "test epoch: 914, loss: 0.68\n",
      "train epoch: 915, loss 0.1336\n",
      "test epoch: 915, loss: 0.68\n",
      "train epoch: 916, loss 0.1348\n",
      "test epoch: 916, loss: 0.68\n",
      "train epoch: 917, loss 0.1345\n",
      "test epoch: 917, loss: 0.68\n",
      "train epoch: 918, loss 0.1334\n",
      "test epoch: 918, loss: 0.68\n",
      "train epoch: 919, loss 0.1343\n",
      "test epoch: 919, loss: 0.68\n",
      "train epoch: 920, loss 0.1332\n",
      "test epoch: 920, loss: 0.68\n",
      "train epoch: 921, loss 0.1346\n",
      "test epoch: 921, loss: 0.68\n",
      "train epoch: 922, loss 0.1330\n",
      "test epoch: 922, loss: 0.68\n",
      "train epoch: 923, loss 0.1335\n",
      "test epoch: 923, loss: 0.68\n",
      "train epoch: 924, loss 0.1332\n",
      "test epoch: 924, loss: 0.68\n",
      "train epoch: 925, loss 0.1329\n",
      "test epoch: 925, loss: 0.68\n",
      "train epoch: 926, loss 0.1326\n",
      "test epoch: 926, loss: 0.68\n",
      "train epoch: 927, loss 0.1324\n",
      "test epoch: 927, loss: 0.68\n",
      "train epoch: 928, loss 0.1332\n",
      "test epoch: 928, loss: 0.68\n",
      "train epoch: 929, loss 0.1328\n",
      "test epoch: 929, loss: 0.68\n",
      "train epoch: 930, loss 0.1324\n",
      "test epoch: 930, loss: 0.68\n",
      "train epoch: 931, loss 0.1326\n",
      "test epoch: 931, loss: 0.68\n",
      "train epoch: 932, loss 0.1337\n",
      "test epoch: 932, loss: 0.68\n",
      "train epoch: 933, loss 0.1320\n",
      "test epoch: 933, loss: 0.68\n",
      "train epoch: 934, loss 0.1328\n",
      "test epoch: 934, loss: 0.68\n",
      "train epoch: 935, loss 0.1317\n",
      "test epoch: 935, loss: 0.68\n",
      "train epoch: 936, loss 0.1338\n",
      "test epoch: 936, loss: 0.68\n",
      "train epoch: 937, loss 0.1315\n",
      "test epoch: 937, loss: 0.68\n",
      "train epoch: 938, loss 0.1325\n",
      "test epoch: 938, loss: 0.68\n",
      "train epoch: 939, loss 0.1313\n",
      "test epoch: 939, loss: 0.68\n",
      "train epoch: 940, loss 0.1333\n",
      "test epoch: 940, loss: 0.68\n",
      "train epoch: 941, loss 0.1310\n",
      "test epoch: 941, loss: 0.68\n",
      "train epoch: 942, loss 0.1321\n",
      "test epoch: 942, loss: 0.68\n",
      "train epoch: 943, loss 0.1308\n",
      "test epoch: 943, loss: 0.68\n",
      "train epoch: 944, loss 0.1320\n",
      "test epoch: 944, loss: 0.68\n",
      "train epoch: 945, loss 0.1317\n",
      "test epoch: 945, loss: 0.68\n",
      "train epoch: 946, loss 0.1306\n",
      "test epoch: 946, loss: 0.68\n",
      "train epoch: 947, loss 0.1310\n",
      "test epoch: 947, loss: 0.68\n",
      "train epoch: 948, loss 0.1305\n",
      "test epoch: 948, loss: 0.68\n",
      "train epoch: 949, loss 0.1310\n",
      "test epoch: 949, loss: 0.68\n",
      "train epoch: 950, loss 0.1315\n",
      "test epoch: 950, loss: 0.68\n",
      "train epoch: 951, loss 0.1304\n",
      "test epoch: 951, loss: 0.68\n",
      "train epoch: 952, loss 0.1317\n",
      "test epoch: 952, loss: 0.68\n",
      "train epoch: 953, loss 0.1298\n",
      "test epoch: 953, loss: 0.68\n",
      "train epoch: 954, loss 0.1305\n",
      "test epoch: 954, loss: 0.68\n",
      "train epoch: 955, loss 0.1296\n",
      "test epoch: 955, loss: 0.68\n",
      "train epoch: 956, loss 0.1305\n",
      "test epoch: 956, loss: 0.68\n",
      "train epoch: 957, loss 0.1301\n",
      "test epoch: 957, loss: 0.68\n",
      "train epoch: 958, loss 0.1294\n",
      "test epoch: 958, loss: 0.68\n",
      "train epoch: 959, loss 0.1296\n",
      "test epoch: 959, loss: 0.68\n",
      "train epoch: 960, loss 0.1297\n",
      "test epoch: 960, loss: 0.68\n",
      "train epoch: 961, loss 0.1298\n",
      "test epoch: 961, loss: 0.68\n",
      "train epoch: 962, loss 0.1308\n",
      "test epoch: 962, loss: 0.68\n",
      "train epoch: 963, loss 0.1290\n",
      "test epoch: 963, loss: 0.68\n",
      "train epoch: 964, loss 0.1335\n",
      "test epoch: 964, loss: 0.68\n",
      "train epoch: 965, loss 0.1292\n",
      "test epoch: 965, loss: 0.68\n",
      "train epoch: 966, loss 0.1303\n",
      "test epoch: 966, loss: 0.68\n",
      "train epoch: 967, loss 0.1289\n",
      "test epoch: 967, loss: 0.68\n",
      "train epoch: 968, loss 0.1310\n",
      "test epoch: 968, loss: 0.68\n",
      "train epoch: 969, loss 0.1289\n",
      "test epoch: 969, loss: 0.68\n",
      "train epoch: 970, loss 0.1289\n",
      "test epoch: 970, loss: 0.68\n",
      "train epoch: 971, loss 0.1287\n",
      "test epoch: 971, loss: 0.68\n",
      "train epoch: 972, loss 0.1293\n",
      "test epoch: 972, loss: 0.68\n",
      "train epoch: 973, loss 0.1288\n",
      "test epoch: 973, loss: 0.68\n",
      "train epoch: 974, loss 0.1290\n",
      "test epoch: 974, loss: 0.68\n",
      "train epoch: 975, loss 0.1281\n",
      "test epoch: 975, loss: 0.68\n",
      "train epoch: 976, loss 0.1297\n",
      "test epoch: 976, loss: 0.68\n",
      "train epoch: 977, loss 0.1276\n",
      "test epoch: 977, loss: 0.68\n",
      "train epoch: 978, loss 0.1278\n",
      "test epoch: 978, loss: 0.68\n",
      "train epoch: 979, loss 0.1275\n",
      "test epoch: 979, loss: 0.68\n",
      "train epoch: 980, loss 0.1283\n",
      "test epoch: 980, loss: 0.68\n",
      "train epoch: 981, loss 0.1271\n",
      "test epoch: 981, loss: 0.68\n",
      "train epoch: 982, loss 0.1272\n",
      "test epoch: 982, loss: 0.68\n",
      "train epoch: 983, loss 0.1268\n",
      "test epoch: 983, loss: 0.68\n",
      "train epoch: 984, loss 0.1283\n",
      "test epoch: 984, loss: 0.68\n",
      "train epoch: 985, loss 0.1271\n",
      "test epoch: 985, loss: 0.68\n",
      "train epoch: 986, loss 0.1274\n",
      "test epoch: 986, loss: 0.68\n",
      "train epoch: 987, loss 0.1268\n",
      "test epoch: 987, loss: 0.68\n",
      "train epoch: 988, loss 0.1273\n",
      "test epoch: 988, loss: 0.68\n",
      "train epoch: 989, loss 0.1266\n",
      "test epoch: 989, loss: 0.68\n",
      "train epoch: 990, loss 0.1266\n",
      "test epoch: 990, loss: 0.68\n",
      "train epoch: 991, loss 0.1264\n",
      "test epoch: 991, loss: 0.68\n",
      "train epoch: 992, loss 0.1275\n",
      "test epoch: 992, loss: 0.68\n",
      "train epoch: 993, loss 0.1268\n",
      "test epoch: 993, loss: 0.68\n",
      "train epoch: 994, loss 0.1264\n",
      "test epoch: 994, loss: 0.68\n",
      "train epoch: 995, loss 0.1264\n",
      "test epoch: 995, loss: 0.68\n",
      "train epoch: 996, loss 0.1272\n",
      "test epoch: 996, loss: 0.68\n",
      "train epoch: 997, loss 0.1267\n",
      "test epoch: 997, loss: 0.68\n",
      "train epoch: 998, loss 0.1261\n",
      "test epoch: 998, loss: 0.68\n",
      "train epoch: 999, loss 0.1265\n",
      "test epoch: 999, loss: 0.68\n",
      "train epoch: 1000, loss 0.1259\n",
      "test epoch: 1000, loss: 0.68\n",
      "train epoch: 1001, loss 0.1252\n",
      "test epoch: 1001, loss: 0.68\n",
      "train epoch: 1002, loss 0.1251\n",
      "test epoch: 1002, loss: 0.68\n",
      "train epoch: 1003, loss 0.1251\n",
      "test epoch: 1003, loss: 0.68\n",
      "train epoch: 1004, loss 0.1248\n",
      "test epoch: 1004, loss: 0.68\n",
      "train epoch: 1005, loss 0.1246\n",
      "test epoch: 1005, loss: 0.69\n",
      "train epoch: 1006, loss 0.1244\n",
      "test epoch: 1006, loss: 0.69\n",
      "train epoch: 1007, loss 0.1250\n",
      "test epoch: 1007, loss: 0.69\n",
      "train epoch: 1008, loss 0.1242\n",
      "test epoch: 1008, loss: 0.69\n",
      "train epoch: 1009, loss 0.1244\n",
      "test epoch: 1009, loss: 0.69\n",
      "train epoch: 1010, loss 0.1238\n",
      "test epoch: 1010, loss: 0.69\n",
      "train epoch: 1011, loss 0.1256\n",
      "test epoch: 1011, loss: 0.69\n",
      "train epoch: 1012, loss 0.1237\n",
      "test epoch: 1012, loss: 0.69\n",
      "train epoch: 1013, loss 0.1239\n",
      "test epoch: 1013, loss: 0.69\n",
      "train epoch: 1014, loss 0.1236\n",
      "test epoch: 1014, loss: 0.69\n",
      "train epoch: 1015, loss 0.1251\n",
      "test epoch: 1015, loss: 0.69\n",
      "train epoch: 1016, loss 0.1237\n",
      "test epoch: 1016, loss: 0.69\n",
      "train epoch: 1017, loss 0.1236\n",
      "test epoch: 1017, loss: 0.69\n",
      "train epoch: 1018, loss 0.1237\n",
      "test epoch: 1018, loss: 0.69\n",
      "train epoch: 1019, loss 0.1236\n",
      "test epoch: 1019, loss: 0.69\n",
      "train epoch: 1020, loss 0.1231\n",
      "test epoch: 1020, loss: 0.69\n",
      "train epoch: 1021, loss 0.1229\n",
      "test epoch: 1021, loss: 0.69\n",
      "train epoch: 1022, loss 0.1232\n",
      "test epoch: 1022, loss: 0.69\n",
      "train epoch: 1023, loss 0.1229\n",
      "test epoch: 1023, loss: 0.69\n",
      "train epoch: 1024, loss 0.1223\n",
      "test epoch: 1024, loss: 0.69\n",
      "train epoch: 1025, loss 0.1226\n",
      "test epoch: 1025, loss: 0.69\n",
      "train epoch: 1026, loss 0.1224\n",
      "test epoch: 1026, loss: 0.69\n",
      "train epoch: 1027, loss 0.1227\n",
      "test epoch: 1027, loss: 0.69\n",
      "train epoch: 1028, loss 0.1219\n",
      "test epoch: 1028, loss: 0.69\n",
      "train epoch: 1029, loss 0.1219\n",
      "test epoch: 1029, loss: 0.69\n",
      "train epoch: 1030, loss 0.1221\n",
      "test epoch: 1030, loss: 0.69\n",
      "train epoch: 1031, loss 0.1229\n",
      "test epoch: 1031, loss: 0.69\n",
      "train epoch: 1032, loss 0.1220\n",
      "test epoch: 1032, loss: 0.69\n",
      "train epoch: 1033, loss 0.1214\n",
      "test epoch: 1033, loss: 0.69\n",
      "train epoch: 1034, loss 0.1227\n",
      "test epoch: 1034, loss: 0.69\n",
      "train epoch: 1035, loss 0.1211\n",
      "test epoch: 1035, loss: 0.69\n",
      "train epoch: 1036, loss 0.1211\n",
      "test epoch: 1036, loss: 0.69\n",
      "train epoch: 1037, loss 0.1212\n",
      "test epoch: 1037, loss: 0.69\n",
      "train epoch: 1038, loss 0.1224\n",
      "test epoch: 1038, loss: 0.69\n",
      "train epoch: 1039, loss 0.1209\n",
      "test epoch: 1039, loss: 0.69\n",
      "train epoch: 1040, loss 0.1206\n",
      "test epoch: 1040, loss: 0.69\n",
      "train epoch: 1041, loss 0.1217\n",
      "test epoch: 1041, loss: 0.69\n",
      "train epoch: 1042, loss 0.1204\n",
      "test epoch: 1042, loss: 0.69\n",
      "train epoch: 1043, loss 0.1209\n",
      "test epoch: 1043, loss: 0.69\n",
      "train epoch: 1044, loss 0.1205\n",
      "test epoch: 1044, loss: 0.69\n",
      "train epoch: 1045, loss 0.1211\n",
      "test epoch: 1045, loss: 0.69\n",
      "train epoch: 1046, loss 0.1199\n",
      "test epoch: 1046, loss: 0.69\n",
      "train epoch: 1047, loss 0.1199\n",
      "test epoch: 1047, loss: 0.69\n",
      "train epoch: 1048, loss 0.1205\n",
      "test epoch: 1048, loss: 0.69\n",
      "train epoch: 1049, loss 0.1203\n",
      "test epoch: 1049, loss: 0.69\n",
      "train epoch: 1050, loss 0.1198\n",
      "test epoch: 1050, loss: 0.69\n",
      "train epoch: 1051, loss 0.1193\n",
      "test epoch: 1051, loss: 0.69\n",
      "train epoch: 1052, loss 0.1195\n",
      "test epoch: 1052, loss: 0.69\n",
      "train epoch: 1053, loss 0.1191\n",
      "test epoch: 1053, loss: 0.69\n",
      "train epoch: 1054, loss 0.1191\n",
      "test epoch: 1054, loss: 0.69\n",
      "train epoch: 1055, loss 0.1191\n",
      "test epoch: 1055, loss: 0.69\n",
      "train epoch: 1056, loss 0.1191\n",
      "test epoch: 1056, loss: 0.69\n",
      "train epoch: 1057, loss 0.1186\n",
      "test epoch: 1057, loss: 0.69\n",
      "train epoch: 1058, loss 0.1189\n",
      "test epoch: 1058, loss: 0.69\n",
      "train epoch: 1059, loss 0.1194\n",
      "test epoch: 1059, loss: 0.69\n",
      "train epoch: 1060, loss 0.1190\n",
      "test epoch: 1060, loss: 0.69\n",
      "train epoch: 1061, loss 0.1183\n",
      "test epoch: 1061, loss: 0.69\n",
      "train epoch: 1062, loss 0.1186\n",
      "test epoch: 1062, loss: 0.69\n",
      "train epoch: 1063, loss 0.1184\n",
      "test epoch: 1063, loss: 0.69\n",
      "train epoch: 1064, loss 0.1179\n",
      "test epoch: 1064, loss: 0.69\n",
      "train epoch: 1065, loss 0.1179\n",
      "test epoch: 1065, loss: 0.69\n",
      "train epoch: 1066, loss 0.1188\n",
      "test epoch: 1066, loss: 0.69\n",
      "train epoch: 1067, loss 0.1177\n",
      "test epoch: 1067, loss: 0.69\n",
      "train epoch: 1068, loss 0.1182\n",
      "test epoch: 1068, loss: 0.69\n",
      "train epoch: 1069, loss 0.1176\n",
      "test epoch: 1069, loss: 0.69\n",
      "train epoch: 1070, loss 0.1191\n",
      "test epoch: 1070, loss: 0.69\n",
      "train epoch: 1071, loss 0.1173\n",
      "test epoch: 1071, loss: 0.69\n",
      "train epoch: 1072, loss 0.1169\n",
      "test epoch: 1072, loss: 0.69\n",
      "train epoch: 1073, loss 0.1174\n",
      "test epoch: 1073, loss: 0.69\n",
      "train epoch: 1074, loss 0.1167\n",
      "test epoch: 1074, loss: 0.69\n",
      "train epoch: 1075, loss 0.1171\n",
      "test epoch: 1075, loss: 0.69\n",
      "train epoch: 1076, loss 0.1170\n",
      "test epoch: 1076, loss: 0.69\n",
      "train epoch: 1077, loss 0.1180\n",
      "test epoch: 1077, loss: 0.69\n",
      "train epoch: 1078, loss 0.1169\n",
      "test epoch: 1078, loss: 0.69\n",
      "train epoch: 1079, loss 0.1163\n",
      "test epoch: 1079, loss: 0.69\n",
      "train epoch: 1080, loss 0.1195\n",
      "test epoch: 1080, loss: 0.69\n",
      "train epoch: 1081, loss 0.1161\n",
      "test epoch: 1081, loss: 0.69\n",
      "train epoch: 1082, loss 0.1171\n",
      "test epoch: 1082, loss: 0.69\n",
      "train epoch: 1083, loss 0.1158\n",
      "test epoch: 1083, loss: 0.69\n",
      "train epoch: 1084, loss 0.1169\n",
      "test epoch: 1084, loss: 0.69\n",
      "train epoch: 1085, loss 0.1158\n",
      "test epoch: 1085, loss: 0.69\n",
      "train epoch: 1086, loss 0.1153\n",
      "test epoch: 1086, loss: 0.69\n",
      "train epoch: 1087, loss 0.1163\n",
      "test epoch: 1087, loss: 0.69\n",
      "train epoch: 1088, loss 0.1152\n",
      "test epoch: 1088, loss: 0.69\n",
      "train epoch: 1089, loss 0.1158\n",
      "test epoch: 1089, loss: 0.69\n",
      "train epoch: 1090, loss 0.1151\n",
      "test epoch: 1090, loss: 0.69\n",
      "train epoch: 1091, loss 0.1170\n",
      "test epoch: 1091, loss: 0.69\n",
      "train epoch: 1092, loss 0.1151\n",
      "test epoch: 1092, loss: 0.69\n",
      "train epoch: 1093, loss 0.1157\n",
      "test epoch: 1093, loss: 0.69\n",
      "train epoch: 1094, loss 0.1157\n",
      "test epoch: 1094, loss: 0.69\n",
      "train epoch: 1095, loss 0.1146\n",
      "test epoch: 1095, loss: 0.69\n",
      "train epoch: 1096, loss 0.1144\n",
      "test epoch: 1096, loss: 0.69\n",
      "train epoch: 1097, loss 0.1142\n",
      "test epoch: 1097, loss: 0.69\n",
      "train epoch: 1098, loss 0.1144\n",
      "test epoch: 1098, loss: 0.69\n",
      "train epoch: 1099, loss 0.1142\n",
      "test epoch: 1099, loss: 0.69\n",
      "train epoch: 1100, loss 0.1141\n",
      "test epoch: 1100, loss: 0.69\n",
      "train epoch: 1101, loss 0.1140\n",
      "test epoch: 1101, loss: 0.69\n",
      "train epoch: 1102, loss 0.1155\n",
      "test epoch: 1102, loss: 0.69\n",
      "train epoch: 1103, loss 0.1140\n",
      "test epoch: 1103, loss: 0.69\n",
      "train epoch: 1104, loss 0.1146\n",
      "test epoch: 1104, loss: 0.69\n",
      "train epoch: 1105, loss 0.1145\n",
      "test epoch: 1105, loss: 0.69\n",
      "train epoch: 1106, loss 0.1134\n",
      "test epoch: 1106, loss: 0.69\n",
      "train epoch: 1107, loss 0.1133\n",
      "test epoch: 1107, loss: 0.69\n",
      "train epoch: 1108, loss 0.1138\n",
      "test epoch: 1108, loss: 0.69\n",
      "train epoch: 1109, loss 0.1134\n",
      "test epoch: 1109, loss: 0.69\n",
      "train epoch: 1110, loss 0.1133\n",
      "test epoch: 1110, loss: 0.69\n",
      "train epoch: 1111, loss 0.1148\n",
      "test epoch: 1111, loss: 0.69\n",
      "train epoch: 1112, loss 0.1128\n",
      "test epoch: 1112, loss: 0.69\n",
      "train epoch: 1113, loss 0.1127\n",
      "test epoch: 1113, loss: 0.70\n",
      "train epoch: 1114, loss 0.1126\n",
      "test epoch: 1114, loss: 0.70\n",
      "train epoch: 1115, loss 0.1125\n",
      "test epoch: 1115, loss: 0.70\n",
      "train epoch: 1116, loss 0.1143\n",
      "test epoch: 1116, loss: 0.70\n",
      "train epoch: 1117, loss 0.1124\n",
      "test epoch: 1117, loss: 0.70\n",
      "train epoch: 1118, loss 0.1132\n",
      "test epoch: 1118, loss: 0.70\n",
      "train epoch: 1119, loss 0.1143\n",
      "test epoch: 1119, loss: 0.70\n",
      "train epoch: 1120, loss 0.1122\n",
      "test epoch: 1120, loss: 0.70\n",
      "train epoch: 1121, loss 0.1127\n",
      "test epoch: 1121, loss: 0.70\n",
      "train epoch: 1122, loss 0.1118\n",
      "test epoch: 1122, loss: 0.70\n",
      "train epoch: 1123, loss 0.1133\n",
      "test epoch: 1123, loss: 0.70\n",
      "train epoch: 1124, loss 0.1115\n",
      "test epoch: 1124, loss: 0.70\n",
      "train epoch: 1125, loss 0.1129\n",
      "test epoch: 1125, loss: 0.70\n",
      "train epoch: 1126, loss 0.1130\n",
      "test epoch: 1126, loss: 0.70\n",
      "train epoch: 1127, loss 0.1116\n",
      "test epoch: 1127, loss: 0.70\n",
      "train epoch: 1128, loss 0.1112\n",
      "test epoch: 1128, loss: 0.70\n",
      "train epoch: 1129, loss 0.1113\n",
      "test epoch: 1129, loss: 0.70\n",
      "train epoch: 1130, loss 0.1123\n",
      "test epoch: 1130, loss: 0.70\n",
      "train epoch: 1131, loss 0.1110\n",
      "test epoch: 1131, loss: 0.70\n",
      "train epoch: 1132, loss 0.1108\n",
      "test epoch: 1132, loss: 0.70\n",
      "train epoch: 1133, loss 0.1108\n",
      "test epoch: 1133, loss: 0.70\n",
      "train epoch: 1134, loss 0.1107\n",
      "test epoch: 1134, loss: 0.70\n",
      "train epoch: 1135, loss 0.1116\n",
      "test epoch: 1135, loss: 0.70\n",
      "train epoch: 1136, loss 0.1106\n",
      "test epoch: 1136, loss: 0.70\n",
      "train epoch: 1137, loss 0.1108\n",
      "test epoch: 1137, loss: 0.70\n",
      "train epoch: 1138, loss 0.1104\n",
      "test epoch: 1138, loss: 0.70\n",
      "train epoch: 1139, loss 0.1134\n",
      "test epoch: 1139, loss: 0.70\n",
      "train epoch: 1140, loss 0.1101\n",
      "test epoch: 1140, loss: 0.70\n",
      "train epoch: 1141, loss 0.1130\n",
      "test epoch: 1141, loss: 0.70\n",
      "train epoch: 1142, loss 0.1116\n",
      "test epoch: 1142, loss: 0.70\n",
      "train epoch: 1143, loss 0.1122\n",
      "test epoch: 1143, loss: 0.70\n",
      "train epoch: 1144, loss 0.1110\n",
      "test epoch: 1144, loss: 0.70\n",
      "train epoch: 1145, loss 0.1099\n",
      "test epoch: 1145, loss: 0.70\n",
      "train epoch: 1146, loss 0.1112\n",
      "test epoch: 1146, loss: 0.70\n",
      "train epoch: 1147, loss 0.1098\n",
      "test epoch: 1147, loss: 0.70\n",
      "train epoch: 1148, loss 0.1102\n",
      "test epoch: 1148, loss: 0.70\n",
      "train epoch: 1149, loss 0.1094\n",
      "test epoch: 1149, loss: 0.70\n",
      "train epoch: 1150, loss 0.1117\n",
      "test epoch: 1150, loss: 0.70\n",
      "train epoch: 1151, loss 0.1091\n",
      "test epoch: 1151, loss: 0.70\n",
      "train epoch: 1152, loss 0.1113\n",
      "test epoch: 1152, loss: 0.70\n",
      "train epoch: 1153, loss 0.1101\n",
      "test epoch: 1153, loss: 0.70\n",
      "train epoch: 1154, loss 0.1123\n",
      "test epoch: 1154, loss: 0.70\n",
      "train epoch: 1155, loss 0.1094\n",
      "test epoch: 1155, loss: 0.70\n",
      "train epoch: 1156, loss 0.1106\n",
      "test epoch: 1156, loss: 0.70\n",
      "train epoch: 1157, loss 0.1111\n",
      "test epoch: 1157, loss: 0.70\n",
      "train epoch: 1158, loss 0.1105\n",
      "test epoch: 1158, loss: 0.70\n",
      "train epoch: 1159, loss 0.1094\n",
      "test epoch: 1159, loss: 0.70\n",
      "train epoch: 1160, loss 0.1084\n",
      "test epoch: 1160, loss: 0.70\n",
      "train epoch: 1161, loss 0.1097\n",
      "test epoch: 1161, loss: 0.70\n",
      "train epoch: 1162, loss 0.1094\n",
      "test epoch: 1162, loss: 0.70\n",
      "train epoch: 1163, loss 0.1085\n",
      "test epoch: 1163, loss: 0.70\n",
      "train epoch: 1164, loss 0.1084\n",
      "test epoch: 1164, loss: 0.70\n",
      "train epoch: 1165, loss 0.1086\n",
      "test epoch: 1165, loss: 0.70\n",
      "train epoch: 1166, loss 0.1092\n",
      "test epoch: 1166, loss: 0.70\n",
      "train epoch: 1167, loss 0.1078\n",
      "test epoch: 1167, loss: 0.70\n",
      "train epoch: 1168, loss 0.1087\n",
      "test epoch: 1168, loss: 0.70\n",
      "train epoch: 1169, loss 0.1092\n",
      "test epoch: 1169, loss: 0.70\n",
      "train epoch: 1170, loss 0.1096\n",
      "test epoch: 1170, loss: 0.70\n",
      "train epoch: 1171, loss 0.1078\n",
      "test epoch: 1171, loss: 0.70\n",
      "train epoch: 1172, loss 0.1085\n",
      "test epoch: 1172, loss: 0.70\n",
      "train epoch: 1173, loss 0.1093\n",
      "test epoch: 1173, loss: 0.70\n",
      "train epoch: 1174, loss 0.1100\n",
      "test epoch: 1174, loss: 0.70\n",
      "train epoch: 1175, loss 0.1077\n",
      "test epoch: 1175, loss: 0.70\n",
      "train epoch: 1176, loss 0.1087\n",
      "test epoch: 1176, loss: 0.70\n",
      "train epoch: 1177, loss 0.1090\n",
      "test epoch: 1177, loss: 0.70\n",
      "train epoch: 1178, loss 0.1107\n",
      "test epoch: 1178, loss: 0.70\n",
      "train epoch: 1179, loss 0.1077\n",
      "test epoch: 1179, loss: 0.70\n",
      "train epoch: 1180, loss 0.1091\n",
      "test epoch: 1180, loss: 0.70\n",
      "train epoch: 1181, loss 0.1089\n",
      "test epoch: 1181, loss: 0.70\n",
      "train epoch: 1182, loss 0.1112\n",
      "test epoch: 1182, loss: 0.70\n",
      "train epoch: 1183, loss 0.1073\n",
      "test epoch: 1183, loss: 0.70\n",
      "train epoch: 1184, loss 0.1094\n",
      "test epoch: 1184, loss: 0.70\n",
      "train epoch: 1185, loss 0.1079\n",
      "test epoch: 1185, loss: 0.70\n",
      "train epoch: 1186, loss 0.1099\n",
      "test epoch: 1186, loss: 0.70\n",
      "train epoch: 1187, loss 0.1062\n",
      "test epoch: 1187, loss: 0.70\n",
      "train epoch: 1188, loss 0.1085\n",
      "test epoch: 1188, loss: 0.70\n",
      "train epoch: 1189, loss 0.1060\n",
      "test epoch: 1189, loss: 0.70\n",
      "train epoch: 1190, loss 0.1086\n",
      "test epoch: 1190, loss: 0.70\n",
      "train epoch: 1191, loss 0.1068\n",
      "test epoch: 1191, loss: 0.70\n",
      "train epoch: 1192, loss 0.1081\n",
      "test epoch: 1192, loss: 0.70\n",
      "train epoch: 1193, loss 0.1067\n",
      "test epoch: 1193, loss: 0.70\n",
      "train epoch: 1194, loss 0.1081\n",
      "test epoch: 1194, loss: 0.71\n",
      "train epoch: 1195, loss 0.1080\n",
      "test epoch: 1195, loss: 0.71\n",
      "train epoch: 1196, loss 0.1061\n",
      "test epoch: 1196, loss: 0.71\n",
      "train epoch: 1197, loss 0.1069\n",
      "test epoch: 1197, loss: 0.71\n",
      "train epoch: 1198, loss 0.1057\n",
      "test epoch: 1198, loss: 0.71\n",
      "train epoch: 1199, loss 0.1073\n",
      "test epoch: 1199, loss: 0.71\n",
      "train epoch: 1200, loss 0.1053\n",
      "test epoch: 1200, loss: 0.71\n",
      "train epoch: 1201, loss 0.1063\n",
      "test epoch: 1201, loss: 0.71\n",
      "train epoch: 1202, loss 0.1053\n",
      "test epoch: 1202, loss: 0.71\n",
      "train epoch: 1203, loss 0.1064\n",
      "test epoch: 1203, loss: 0.71\n",
      "train epoch: 1204, loss 0.1051\n",
      "test epoch: 1204, loss: 0.71\n",
      "train epoch: 1205, loss 0.1053\n",
      "test epoch: 1205, loss: 0.71\n",
      "train epoch: 1206, loss 0.1066\n",
      "test epoch: 1206, loss: 0.71\n",
      "train epoch: 1207, loss 0.1056\n",
      "test epoch: 1207, loss: 0.71\n",
      "train epoch: 1208, loss 0.1062\n",
      "test epoch: 1208, loss: 0.71\n",
      "train epoch: 1209, loss 0.1052\n",
      "test epoch: 1209, loss: 0.71\n",
      "train epoch: 1210, loss 0.1058\n",
      "test epoch: 1210, loss: 0.71\n",
      "train epoch: 1211, loss 0.1047\n",
      "test epoch: 1211, loss: 0.71\n",
      "train epoch: 1212, loss 0.1047\n",
      "test epoch: 1212, loss: 0.71\n",
      "train epoch: 1213, loss 0.1048\n",
      "test epoch: 1213, loss: 0.71\n",
      "train epoch: 1214, loss 0.1044\n",
      "test epoch: 1214, loss: 0.71\n",
      "train epoch: 1215, loss 0.1044\n",
      "test epoch: 1215, loss: 0.71\n",
      "train epoch: 1216, loss 0.1045\n",
      "test epoch: 1216, loss: 0.71\n",
      "train epoch: 1217, loss 0.1044\n",
      "test epoch: 1217, loss: 0.71\n",
      "train epoch: 1218, loss 0.1042\n",
      "test epoch: 1218, loss: 0.71\n",
      "train epoch: 1219, loss 0.1052\n",
      "test epoch: 1219, loss: 0.71\n",
      "train epoch: 1220, loss 0.1042\n",
      "test epoch: 1220, loss: 0.71\n",
      "train epoch: 1221, loss 0.1047\n",
      "test epoch: 1221, loss: 0.71\n",
      "train epoch: 1222, loss 0.1051\n",
      "test epoch: 1222, loss: 0.71\n",
      "train epoch: 1223, loss 0.1048\n",
      "test epoch: 1223, loss: 0.71\n",
      "train epoch: 1224, loss 0.1046\n",
      "test epoch: 1224, loss: 0.71\n",
      "train epoch: 1225, loss 0.1042\n",
      "test epoch: 1225, loss: 0.71\n",
      "train epoch: 1226, loss 0.1046\n",
      "test epoch: 1226, loss: 0.71\n",
      "train epoch: 1227, loss 0.1038\n",
      "test epoch: 1227, loss: 0.71\n",
      "train epoch: 1228, loss 0.1037\n",
      "test epoch: 1228, loss: 0.71\n",
      "train epoch: 1229, loss 0.1041\n",
      "test epoch: 1229, loss: 0.71\n",
      "train epoch: 1230, loss 0.1036\n",
      "test epoch: 1230, loss: 0.71\n",
      "train epoch: 1231, loss 0.1037\n",
      "test epoch: 1231, loss: 0.71\n",
      "train epoch: 1232, loss 0.1036\n",
      "test epoch: 1232, loss: 0.71\n",
      "train epoch: 1233, loss 0.1035\n",
      "test epoch: 1233, loss: 0.71\n",
      "train epoch: 1234, loss 0.1033\n",
      "test epoch: 1234, loss: 0.71\n",
      "train epoch: 1235, loss 0.1032\n",
      "test epoch: 1235, loss: 0.71\n",
      "train epoch: 1236, loss 0.1038\n",
      "test epoch: 1236, loss: 0.71\n",
      "train epoch: 1237, loss 0.1032\n",
      "test epoch: 1237, loss: 0.71\n",
      "train epoch: 1238, loss 0.1033\n",
      "test epoch: 1238, loss: 0.71\n",
      "train epoch: 1239, loss 0.1034\n",
      "test epoch: 1239, loss: 0.71\n",
      "train epoch: 1240, loss 0.1031\n",
      "test epoch: 1240, loss: 0.71\n",
      "train epoch: 1241, loss 0.1030\n",
      "test epoch: 1241, loss: 0.71\n",
      "train epoch: 1242, loss 0.1030\n",
      "test epoch: 1242, loss: 0.71\n",
      "train epoch: 1243, loss 0.1029\n",
      "test epoch: 1243, loss: 0.71\n",
      "train epoch: 1244, loss 0.1028\n",
      "test epoch: 1244, loss: 0.71\n",
      "train epoch: 1245, loss 0.1027\n",
      "test epoch: 1245, loss: 0.71\n",
      "train epoch: 1246, loss 0.1027\n",
      "test epoch: 1246, loss: 0.71\n",
      "train epoch: 1247, loss 0.1029\n",
      "test epoch: 1247, loss: 0.71\n",
      "train epoch: 1248, loss 0.1028\n",
      "test epoch: 1248, loss: 0.71\n",
      "train epoch: 1249, loss 0.1027\n",
      "test epoch: 1249, loss: 0.71\n",
      "train epoch: 1250, loss 0.1029\n",
      "test epoch: 1250, loss: 0.71\n",
      "train epoch: 1251, loss 0.1028\n",
      "test epoch: 1251, loss: 0.71\n",
      "train epoch: 1252, loss 0.1025\n",
      "test epoch: 1252, loss: 0.71\n",
      "train epoch: 1253, loss 0.1024\n",
      "test epoch: 1253, loss: 0.71\n",
      "train epoch: 1254, loss 0.1024\n",
      "test epoch: 1254, loss: 0.71\n",
      "train epoch: 1255, loss 0.1023\n",
      "test epoch: 1255, loss: 0.71\n",
      "train epoch: 1256, loss 0.1023\n",
      "test epoch: 1256, loss: 0.71\n",
      "train epoch: 1257, loss 0.1022\n",
      "test epoch: 1257, loss: 0.71\n",
      "train epoch: 1258, loss 0.1022\n",
      "test epoch: 1258, loss: 0.71\n",
      "train epoch: 1259, loss 0.1024\n",
      "test epoch: 1259, loss: 0.71\n",
      "train epoch: 1260, loss 0.1023\n",
      "test epoch: 1260, loss: 0.71\n",
      "train epoch: 1261, loss 0.1021\n",
      "test epoch: 1261, loss: 0.71\n",
      "train epoch: 1262, loss 0.1021\n",
      "test epoch: 1262, loss: 0.71\n",
      "train epoch: 1263, loss 0.1021\n",
      "test epoch: 1263, loss: 0.71\n",
      "train epoch: 1264, loss 0.1019\n",
      "test epoch: 1264, loss: 0.71\n",
      "train epoch: 1265, loss 0.1018\n",
      "test epoch: 1265, loss: 0.71\n",
      "train epoch: 1266, loss 0.1024\n",
      "test epoch: 1266, loss: 0.72\n",
      "train epoch: 1267, loss 0.1020\n",
      "test epoch: 1267, loss: 0.72\n",
      "train epoch: 1268, loss 0.1022\n",
      "test epoch: 1268, loss: 0.72\n",
      "train epoch: 1269, loss 0.1017\n",
      "test epoch: 1269, loss: 0.72\n",
      "train epoch: 1270, loss 0.1023\n",
      "test epoch: 1270, loss: 0.72\n",
      "train epoch: 1271, loss 0.1020\n",
      "test epoch: 1271, loss: 0.72\n",
      "train epoch: 1272, loss 0.1017\n",
      "test epoch: 1272, loss: 0.72\n",
      "train epoch: 1273, loss 0.1018\n",
      "test epoch: 1273, loss: 0.72\n",
      "train epoch: 1274, loss 0.1016\n",
      "test epoch: 1274, loss: 0.72\n",
      "train epoch: 1275, loss 0.1017\n",
      "test epoch: 1275, loss: 0.72\n",
      "train epoch: 1276, loss 0.1013\n",
      "test epoch: 1276, loss: 0.72\n",
      "train epoch: 1277, loss 0.1015\n",
      "test epoch: 1277, loss: 0.72\n",
      "train epoch: 1278, loss 0.1013\n",
      "test epoch: 1278, loss: 0.72\n",
      "train epoch: 1279, loss 0.1013\n",
      "test epoch: 1279, loss: 0.72\n",
      "train epoch: 1280, loss 0.1016\n",
      "test epoch: 1280, loss: 0.72\n",
      "train epoch: 1281, loss 0.1016\n",
      "test epoch: 1281, loss: 0.72\n",
      "train epoch: 1282, loss 0.1015\n",
      "test epoch: 1282, loss: 0.72\n",
      "train epoch: 1283, loss 0.1015\n",
      "test epoch: 1283, loss: 0.72\n",
      "train epoch: 1284, loss 0.1014\n",
      "test epoch: 1284, loss: 0.72\n",
      "train epoch: 1285, loss 0.1015\n",
      "test epoch: 1285, loss: 0.72\n",
      "train epoch: 1286, loss 0.1012\n",
      "test epoch: 1286, loss: 0.72\n",
      "train epoch: 1287, loss 0.1012\n",
      "test epoch: 1287, loss: 0.72\n",
      "train epoch: 1288, loss 0.1011\n",
      "test epoch: 1288, loss: 0.72\n",
      "train epoch: 1289, loss 0.1011\n",
      "test epoch: 1289, loss: 0.72\n",
      "train epoch: 1290, loss 0.1009\n",
      "test epoch: 1290, loss: 0.72\n",
      "train epoch: 1291, loss 0.1009\n",
      "test epoch: 1291, loss: 0.72\n",
      "train epoch: 1292, loss 0.1012\n",
      "test epoch: 1292, loss: 0.72\n",
      "train epoch: 1293, loss 0.1009\n",
      "test epoch: 1293, loss: 0.72\n",
      "train epoch: 1294, loss 0.1009\n",
      "test epoch: 1294, loss: 0.72\n",
      "train epoch: 1295, loss 0.1008\n",
      "test epoch: 1295, loss: 0.72\n",
      "train epoch: 1296, loss 0.1009\n",
      "test epoch: 1296, loss: 0.72\n",
      "train epoch: 1297, loss 0.1010\n",
      "test epoch: 1297, loss: 0.72\n",
      "train epoch: 1298, loss 0.1008\n",
      "test epoch: 1298, loss: 0.72\n",
      "train epoch: 1299, loss 0.1008\n",
      "test epoch: 1299, loss: 0.72\n",
      "train epoch: 1300, loss 0.1008\n",
      "test epoch: 1300, loss: 0.72\n",
      "train epoch: 1301, loss 0.1009\n",
      "test epoch: 1301, loss: 0.72\n",
      "train epoch: 1302, loss 0.1007\n",
      "test epoch: 1302, loss: 0.72\n",
      "train epoch: 1303, loss 0.1007\n",
      "test epoch: 1303, loss: 0.72\n",
      "train epoch: 1304, loss 0.1006\n",
      "test epoch: 1304, loss: 0.72\n",
      "train epoch: 1305, loss 0.1007\n",
      "test epoch: 1305, loss: 0.72\n",
      "train epoch: 1306, loss 0.1005\n",
      "test epoch: 1306, loss: 0.72\n",
      "train epoch: 1307, loss 0.1005\n",
      "test epoch: 1307, loss: 0.72\n",
      "train epoch: 1308, loss 0.1005\n",
      "test epoch: 1308, loss: 0.72\n",
      "train epoch: 1309, loss 0.1005\n",
      "test epoch: 1309, loss: 0.72\n",
      "train epoch: 1310, loss 0.1004\n",
      "test epoch: 1310, loss: 0.72\n",
      "train epoch: 1311, loss 0.1004\n",
      "test epoch: 1311, loss: 0.72\n",
      "train epoch: 1312, loss 0.1004\n",
      "test epoch: 1312, loss: 0.72\n",
      "train epoch: 1313, loss 0.1004\n",
      "test epoch: 1313, loss: 0.72\n",
      "train epoch: 1314, loss 0.1003\n",
      "test epoch: 1314, loss: 0.72\n",
      "train epoch: 1315, loss 0.1003\n",
      "test epoch: 1315, loss: 0.72\n",
      "train epoch: 1316, loss 0.1002\n",
      "test epoch: 1316, loss: 0.72\n",
      "train epoch: 1317, loss 0.1002\n",
      "test epoch: 1317, loss: 0.72\n",
      "train epoch: 1318, loss 0.1001\n",
      "test epoch: 1318, loss: 0.72\n",
      "train epoch: 1319, loss 0.1001\n",
      "test epoch: 1319, loss: 0.72\n",
      "train epoch: 1320, loss 0.1001\n",
      "test epoch: 1320, loss: 0.72\n",
      "train epoch: 1321, loss 0.1002\n",
      "test epoch: 1321, loss: 0.72\n",
      "train epoch: 1322, loss 0.1000\n",
      "test epoch: 1322, loss: 0.72\n",
      "train epoch: 1323, loss 0.1000\n",
      "test epoch: 1323, loss: 0.72\n",
      "train epoch: 1324, loss 0.1000\n",
      "test epoch: 1324, loss: 0.72\n",
      "train epoch: 1325, loss 0.1001\n",
      "test epoch: 1325, loss: 0.72\n",
      "train epoch: 1326, loss 0.0999\n",
      "test epoch: 1326, loss: 0.72\n",
      "train epoch: 1327, loss 0.0999\n",
      "test epoch: 1327, loss: 0.72\n",
      "train epoch: 1328, loss 0.0998\n",
      "test epoch: 1328, loss: 0.72\n",
      "train epoch: 1329, loss 0.0999\n",
      "test epoch: 1329, loss: 0.72\n",
      "train epoch: 1330, loss 0.0998\n",
      "test epoch: 1330, loss: 0.72\n",
      "train epoch: 1331, loss 0.0997\n",
      "test epoch: 1331, loss: 0.72\n",
      "train epoch: 1332, loss 0.0997\n",
      "test epoch: 1332, loss: 0.72\n",
      "train epoch: 1333, loss 0.0997\n",
      "test epoch: 1333, loss: 0.72\n",
      "train epoch: 1334, loss 0.0997\n",
      "test epoch: 1334, loss: 0.72\n",
      "train epoch: 1335, loss 0.0996\n",
      "test epoch: 1335, loss: 0.73\n",
      "train epoch: 1336, loss 0.0996\n",
      "test epoch: 1336, loss: 0.73\n",
      "train epoch: 1337, loss 0.0995\n",
      "test epoch: 1337, loss: 0.73\n",
      "train epoch: 1338, loss 0.0995\n",
      "test epoch: 1338, loss: 0.73\n",
      "train epoch: 1339, loss 0.0995\n",
      "test epoch: 1339, loss: 0.73\n",
      "train epoch: 1340, loss 0.0995\n",
      "test epoch: 1340, loss: 0.73\n",
      "train epoch: 1341, loss 0.0994\n",
      "test epoch: 1341, loss: 0.73\n",
      "train epoch: 1342, loss 0.0994\n",
      "test epoch: 1342, loss: 0.73\n",
      "train epoch: 1343, loss 0.0994\n",
      "test epoch: 1343, loss: 0.73\n",
      "train epoch: 1344, loss 0.0993\n",
      "test epoch: 1344, loss: 0.73\n",
      "train epoch: 1345, loss 0.0993\n",
      "test epoch: 1345, loss: 0.73\n",
      "train epoch: 1346, loss 0.0993\n",
      "test epoch: 1346, loss: 0.73\n",
      "train epoch: 1347, loss 0.0992\n",
      "test epoch: 1347, loss: 0.73\n",
      "train epoch: 1348, loss 0.0992\n",
      "test epoch: 1348, loss: 0.73\n",
      "train epoch: 1349, loss 0.0992\n",
      "test epoch: 1349, loss: 0.73\n",
      "train epoch: 1350, loss 0.0992\n",
      "test epoch: 1350, loss: 0.73\n",
      "train epoch: 1351, loss 0.0991\n",
      "test epoch: 1351, loss: 0.73\n",
      "train epoch: 1352, loss 0.0991\n",
      "test epoch: 1352, loss: 0.73\n",
      "train epoch: 1353, loss 0.0991\n",
      "test epoch: 1353, loss: 0.73\n",
      "train epoch: 1354, loss 0.0990\n",
      "test epoch: 1354, loss: 0.73\n",
      "train epoch: 1355, loss 0.0990\n",
      "test epoch: 1355, loss: 0.73\n",
      "train epoch: 1356, loss 0.0990\n",
      "test epoch: 1356, loss: 0.73\n",
      "train epoch: 1357, loss 0.0990\n",
      "test epoch: 1357, loss: 0.73\n",
      "train epoch: 1358, loss 0.0989\n",
      "test epoch: 1358, loss: 0.73\n",
      "train epoch: 1359, loss 0.0989\n",
      "test epoch: 1359, loss: 0.73\n",
      "train epoch: 1360, loss 0.0988\n",
      "test epoch: 1360, loss: 0.73\n",
      "train epoch: 1361, loss 0.0988\n",
      "test epoch: 1361, loss: 0.73\n",
      "train epoch: 1362, loss 0.0988\n",
      "test epoch: 1362, loss: 0.73\n",
      "train epoch: 1363, loss 0.0988\n",
      "test epoch: 1363, loss: 0.73\n",
      "train epoch: 1364, loss 0.0987\n",
      "test epoch: 1364, loss: 0.73\n",
      "train epoch: 1365, loss 0.0987\n",
      "test epoch: 1365, loss: 0.73\n",
      "train epoch: 1366, loss 0.0987\n",
      "test epoch: 1366, loss: 0.73\n",
      "train epoch: 1367, loss 0.0987\n",
      "test epoch: 1367, loss: 0.73\n",
      "train epoch: 1368, loss 0.0986\n",
      "test epoch: 1368, loss: 0.73\n",
      "train epoch: 1369, loss 0.0987\n",
      "test epoch: 1369, loss: 0.73\n",
      "train epoch: 1370, loss 0.0986\n",
      "test epoch: 1370, loss: 0.73\n",
      "train epoch: 1371, loss 0.0985\n",
      "test epoch: 1371, loss: 0.73\n",
      "train epoch: 1372, loss 0.0985\n",
      "test epoch: 1372, loss: 0.73\n",
      "train epoch: 1373, loss 0.0985\n",
      "test epoch: 1373, loss: 0.73\n",
      "train epoch: 1374, loss 0.0986\n",
      "test epoch: 1374, loss: 0.73\n",
      "train epoch: 1375, loss 0.0984\n",
      "test epoch: 1375, loss: 0.73\n",
      "train epoch: 1376, loss 0.0984\n",
      "test epoch: 1376, loss: 0.73\n",
      "train epoch: 1377, loss 0.0984\n",
      "test epoch: 1377, loss: 0.73\n",
      "train epoch: 1378, loss 0.0983\n",
      "test epoch: 1378, loss: 0.73\n",
      "train epoch: 1379, loss 0.0983\n",
      "test epoch: 1379, loss: 0.73\n",
      "train epoch: 1380, loss 0.0982\n",
      "test epoch: 1380, loss: 0.73\n",
      "train epoch: 1381, loss 0.0983\n",
      "test epoch: 1381, loss: 0.73\n",
      "train epoch: 1382, loss 0.0982\n",
      "test epoch: 1382, loss: 0.73\n",
      "train epoch: 1383, loss 0.0982\n",
      "test epoch: 1383, loss: 0.73\n",
      "train epoch: 1384, loss 0.0982\n",
      "test epoch: 1384, loss: 0.73\n",
      "train epoch: 1385, loss 0.0981\n",
      "test epoch: 1385, loss: 0.73\n",
      "train epoch: 1386, loss 0.0981\n",
      "test epoch: 1386, loss: 0.73\n",
      "train epoch: 1387, loss 0.0980\n",
      "test epoch: 1387, loss: 0.73\n",
      "train epoch: 1388, loss 0.0980\n",
      "test epoch: 1388, loss: 0.73\n",
      "train epoch: 1389, loss 0.0980\n",
      "test epoch: 1389, loss: 0.73\n",
      "train epoch: 1390, loss 0.0979\n",
      "test epoch: 1390, loss: 0.73\n",
      "train epoch: 1391, loss 0.0979\n",
      "test epoch: 1391, loss: 0.73\n",
      "train epoch: 1392, loss 0.0979\n",
      "test epoch: 1392, loss: 0.73\n",
      "train epoch: 1393, loss 0.0979\n",
      "test epoch: 1393, loss: 0.73\n",
      "train epoch: 1394, loss 0.0979\n",
      "test epoch: 1394, loss: 0.73\n",
      "train epoch: 1395, loss 0.0978\n",
      "test epoch: 1395, loss: 0.73\n",
      "train epoch: 1396, loss 0.0978\n",
      "test epoch: 1396, loss: 0.73\n",
      "train epoch: 1397, loss 0.0977\n",
      "test epoch: 1397, loss: 0.73\n",
      "train epoch: 1398, loss 0.0977\n",
      "test epoch: 1398, loss: 0.73\n",
      "train epoch: 1399, loss 0.0978\n",
      "test epoch: 1399, loss: 0.73\n",
      "train epoch: 1400, loss 0.0977\n",
      "test epoch: 1400, loss: 0.73\n",
      "train epoch: 1401, loss 0.0976\n",
      "test epoch: 1401, loss: 0.73\n",
      "train epoch: 1402, loss 0.0978\n",
      "test epoch: 1402, loss: 0.73\n",
      "train epoch: 1403, loss 0.0976\n",
      "test epoch: 1403, loss: 0.74\n",
      "train epoch: 1404, loss 0.0977\n",
      "test epoch: 1404, loss: 0.74\n",
      "train epoch: 1405, loss 0.0975\n",
      "test epoch: 1405, loss: 0.74\n",
      "train epoch: 1406, loss 0.0979\n",
      "test epoch: 1406, loss: 0.74\n",
      "train epoch: 1407, loss 0.0975\n",
      "test epoch: 1407, loss: 0.74\n",
      "train epoch: 1408, loss 0.0977\n",
      "test epoch: 1408, loss: 0.74\n",
      "train epoch: 1409, loss 0.0974\n",
      "test epoch: 1409, loss: 0.74\n",
      "train epoch: 1410, loss 0.0979\n",
      "test epoch: 1410, loss: 0.74\n",
      "train epoch: 1411, loss 0.0975\n",
      "test epoch: 1411, loss: 0.74\n",
      "train epoch: 1412, loss 0.0977\n",
      "test epoch: 1412, loss: 0.74\n",
      "train epoch: 1413, loss 0.0975\n",
      "test epoch: 1413, loss: 0.74\n",
      "train epoch: 1414, loss 0.0977\n",
      "test epoch: 1414, loss: 0.74\n",
      "train epoch: 1415, loss 0.0976\n",
      "test epoch: 1415, loss: 0.74\n",
      "train epoch: 1416, loss 0.0974\n",
      "test epoch: 1416, loss: 0.74\n",
      "train epoch: 1417, loss 0.0974\n",
      "test epoch: 1417, loss: 0.74\n",
      "train epoch: 1418, loss 0.0974\n",
      "test epoch: 1418, loss: 0.74\n",
      "train epoch: 1419, loss 0.0974\n",
      "test epoch: 1419, loss: 0.74\n",
      "train epoch: 1420, loss 0.0972\n",
      "test epoch: 1420, loss: 0.74\n",
      "train epoch: 1421, loss 0.0972\n",
      "test epoch: 1421, loss: 0.74\n",
      "train epoch: 1422, loss 0.0973\n",
      "test epoch: 1422, loss: 0.74\n",
      "train epoch: 1423, loss 0.0972\n",
      "test epoch: 1423, loss: 0.74\n",
      "train epoch: 1424, loss 0.0971\n",
      "test epoch: 1424, loss: 0.74\n",
      "train epoch: 1425, loss 0.0970\n",
      "test epoch: 1425, loss: 0.74\n",
      "train epoch: 1426, loss 0.0972\n",
      "test epoch: 1426, loss: 0.74\n",
      "train epoch: 1427, loss 0.0971\n",
      "test epoch: 1427, loss: 0.74\n",
      "train epoch: 1428, loss 0.0971\n",
      "test epoch: 1428, loss: 0.74\n",
      "train epoch: 1429, loss 0.0969\n",
      "test epoch: 1429, loss: 0.74\n",
      "train epoch: 1430, loss 0.0972\n",
      "test epoch: 1430, loss: 0.74\n",
      "train epoch: 1431, loss 0.0970\n",
      "test epoch: 1431, loss: 0.74\n",
      "train epoch: 1432, loss 0.0970\n",
      "test epoch: 1432, loss: 0.74\n",
      "train epoch: 1433, loss 0.0968\n",
      "test epoch: 1433, loss: 0.74\n",
      "train epoch: 1434, loss 0.0972\n",
      "test epoch: 1434, loss: 0.74\n",
      "train epoch: 1435, loss 0.0969\n",
      "test epoch: 1435, loss: 0.74\n",
      "train epoch: 1436, loss 0.0969\n",
      "test epoch: 1436, loss: 0.74\n",
      "train epoch: 1437, loss 0.0968\n",
      "test epoch: 1437, loss: 0.74\n",
      "train epoch: 1438, loss 0.0970\n",
      "test epoch: 1438, loss: 0.74\n",
      "train epoch: 1439, loss 0.0969\n",
      "test epoch: 1439, loss: 0.74\n",
      "train epoch: 1440, loss 0.0967\n",
      "test epoch: 1440, loss: 0.74\n",
      "train epoch: 1441, loss 0.0966\n",
      "test epoch: 1441, loss: 0.74\n",
      "train epoch: 1442, loss 0.0969\n",
      "test epoch: 1442, loss: 0.74\n",
      "train epoch: 1443, loss 0.0968\n",
      "test epoch: 1443, loss: 0.74\n",
      "train epoch: 1444, loss 0.0966\n",
      "test epoch: 1444, loss: 0.74\n",
      "train epoch: 1445, loss 0.0965\n",
      "test epoch: 1445, loss: 0.74\n",
      "train epoch: 1446, loss 0.0965\n",
      "test epoch: 1446, loss: 0.74\n",
      "train epoch: 1447, loss 0.0965\n",
      "test epoch: 1447, loss: 0.74\n",
      "train epoch: 1448, loss 0.0964\n",
      "test epoch: 1448, loss: 0.74\n",
      "train epoch: 1449, loss 0.0964\n",
      "test epoch: 1449, loss: 0.74\n",
      "train epoch: 1450, loss 0.0964\n",
      "test epoch: 1450, loss: 0.74\n",
      "train epoch: 1451, loss 0.0964\n",
      "test epoch: 1451, loss: 0.74\n",
      "train epoch: 1452, loss 0.0964\n",
      "test epoch: 1452, loss: 0.74\n",
      "train epoch: 1453, loss 0.0964\n",
      "test epoch: 1453, loss: 0.74\n",
      "train epoch: 1454, loss 0.0963\n",
      "test epoch: 1454, loss: 0.74\n",
      "train epoch: 1455, loss 0.0963\n",
      "test epoch: 1455, loss: 0.74\n",
      "train epoch: 1456, loss 0.0964\n",
      "test epoch: 1456, loss: 0.74\n",
      "train epoch: 1457, loss 0.0964\n",
      "test epoch: 1457, loss: 0.74\n",
      "train epoch: 1458, loss 0.0962\n",
      "test epoch: 1458, loss: 0.74\n",
      "train epoch: 1459, loss 0.0963\n",
      "test epoch: 1459, loss: 0.74\n",
      "train epoch: 1460, loss 0.0962\n",
      "test epoch: 1460, loss: 0.74\n",
      "train epoch: 1461, loss 0.0964\n",
      "test epoch: 1461, loss: 0.74\n",
      "train epoch: 1462, loss 0.0961\n",
      "test epoch: 1462, loss: 0.74\n",
      "train epoch: 1463, loss 0.0962\n",
      "test epoch: 1463, loss: 0.74\n",
      "train epoch: 1464, loss 0.0961\n",
      "test epoch: 1464, loss: 0.74\n",
      "train epoch: 1465, loss 0.0963\n",
      "test epoch: 1465, loss: 0.74\n",
      "train epoch: 1466, loss 0.0961\n",
      "test epoch: 1466, loss: 0.74\n",
      "train epoch: 1467, loss 0.0960\n",
      "test epoch: 1467, loss: 0.74\n",
      "train epoch: 1468, loss 0.0960\n",
      "test epoch: 1468, loss: 0.74\n",
      "train epoch: 1469, loss 0.0961\n",
      "test epoch: 1469, loss: 0.74\n",
      "train epoch: 1470, loss 0.0960\n",
      "test epoch: 1470, loss: 0.74\n",
      "train epoch: 1471, loss 0.0959\n",
      "test epoch: 1471, loss: 0.74\n",
      "train epoch: 1472, loss 0.0959\n",
      "test epoch: 1472, loss: 0.75\n",
      "train epoch: 1473, loss 0.0959\n",
      "test epoch: 1473, loss: 0.75\n",
      "train epoch: 1474, loss 0.0959\n",
      "test epoch: 1474, loss: 0.75\n",
      "train epoch: 1475, loss 0.0958\n",
      "test epoch: 1475, loss: 0.75\n",
      "train epoch: 1476, loss 0.0958\n",
      "test epoch: 1476, loss: 0.75\n",
      "train epoch: 1477, loss 0.0958\n",
      "test epoch: 1477, loss: 0.75\n",
      "train epoch: 1478, loss 0.0958\n",
      "test epoch: 1478, loss: 0.75\n",
      "train epoch: 1479, loss 0.0957\n",
      "test epoch: 1479, loss: 0.75\n",
      "train epoch: 1480, loss 0.0957\n",
      "test epoch: 1480, loss: 0.75\n",
      "train epoch: 1481, loss 0.0957\n",
      "test epoch: 1481, loss: 0.75\n",
      "train epoch: 1482, loss 0.0958\n",
      "test epoch: 1482, loss: 0.75\n",
      "train epoch: 1483, loss 0.0957\n",
      "test epoch: 1483, loss: 0.75\n",
      "train epoch: 1484, loss 0.0956\n",
      "test epoch: 1484, loss: 0.75\n",
      "train epoch: 1485, loss 0.0956\n",
      "test epoch: 1485, loss: 0.75\n",
      "train epoch: 1486, loss 0.0956\n",
      "test epoch: 1486, loss: 0.75\n",
      "train epoch: 1487, loss 0.0956\n",
      "test epoch: 1487, loss: 0.75\n",
      "train epoch: 1488, loss 0.0955\n",
      "test epoch: 1488, loss: 0.75\n",
      "train epoch: 1489, loss 0.0955\n",
      "test epoch: 1489, loss: 0.75\n",
      "train epoch: 1490, loss 0.0955\n",
      "test epoch: 1490, loss: 0.75\n",
      "train epoch: 1491, loss 0.0955\n",
      "test epoch: 1491, loss: 0.75\n",
      "train epoch: 1492, loss 0.0954\n",
      "test epoch: 1492, loss: 0.75\n",
      "train epoch: 1493, loss 0.0954\n",
      "test epoch: 1493, loss: 0.75\n",
      "train epoch: 1494, loss 0.0954\n",
      "test epoch: 1494, loss: 0.75\n",
      "train epoch: 1495, loss 0.0954\n",
      "test epoch: 1495, loss: 0.75\n",
      "train epoch: 1496, loss 0.0954\n",
      "test epoch: 1496, loss: 0.75\n",
      "train epoch: 1497, loss 0.0953\n",
      "test epoch: 1497, loss: 0.75\n",
      "train epoch: 1498, loss 0.0953\n",
      "test epoch: 1498, loss: 0.75\n",
      "train epoch: 1499, loss 0.0953\n",
      "test epoch: 1499, loss: 0.75\n",
      "train epoch: 1500, loss 0.0953\n",
      "test epoch: 1500, loss: 0.75\n",
      "train epoch: 1501, loss 0.0952\n",
      "test epoch: 1501, loss: 0.75\n",
      "train epoch: 1502, loss 0.0952\n",
      "test epoch: 1502, loss: 0.75\n",
      "train epoch: 1503, loss 0.0952\n",
      "test epoch: 1503, loss: 0.75\n",
      "train epoch: 1504, loss 0.0952\n",
      "test epoch: 1504, loss: 0.75\n",
      "train epoch: 1505, loss 0.0952\n",
      "test epoch: 1505, loss: 0.75\n",
      "train epoch: 1506, loss 0.0951\n",
      "test epoch: 1506, loss: 0.75\n",
      "train epoch: 1507, loss 0.0951\n",
      "test epoch: 1507, loss: 0.75\n",
      "train epoch: 1508, loss 0.0951\n",
      "test epoch: 1508, loss: 0.75\n",
      "train epoch: 1509, loss 0.0951\n",
      "test epoch: 1509, loss: 0.75\n",
      "train epoch: 1510, loss 0.0950\n",
      "test epoch: 1510, loss: 0.75\n",
      "train epoch: 1511, loss 0.0950\n",
      "test epoch: 1511, loss: 0.75\n",
      "train epoch: 1512, loss 0.0950\n",
      "test epoch: 1512, loss: 0.75\n",
      "train epoch: 1513, loss 0.0950\n",
      "test epoch: 1513, loss: 0.75\n",
      "train epoch: 1514, loss 0.0950\n",
      "test epoch: 1514, loss: 0.75\n",
      "train epoch: 1515, loss 0.0949\n",
      "test epoch: 1515, loss: 0.75\n",
      "train epoch: 1516, loss 0.0949\n",
      "test epoch: 1516, loss: 0.75\n",
      "train epoch: 1517, loss 0.0949\n",
      "test epoch: 1517, loss: 0.75\n",
      "train epoch: 1518, loss 0.0949\n",
      "test epoch: 1518, loss: 0.75\n",
      "train epoch: 1519, loss 0.0948\n",
      "test epoch: 1519, loss: 0.75\n",
      "train epoch: 1520, loss 0.0948\n",
      "test epoch: 1520, loss: 0.75\n",
      "train epoch: 1521, loss 0.0948\n",
      "test epoch: 1521, loss: 0.75\n",
      "train epoch: 1522, loss 0.0948\n",
      "test epoch: 1522, loss: 0.75\n",
      "train epoch: 1523, loss 0.0947\n",
      "test epoch: 1523, loss: 0.75\n",
      "train epoch: 1524, loss 0.0947\n",
      "test epoch: 1524, loss: 0.75\n",
      "train epoch: 1525, loss 0.0947\n",
      "test epoch: 1525, loss: 0.75\n",
      "train epoch: 1526, loss 0.0947\n",
      "test epoch: 1526, loss: 0.75\n",
      "train epoch: 1527, loss 0.0947\n",
      "test epoch: 1527, loss: 0.75\n",
      "train epoch: 1528, loss 0.0946\n",
      "test epoch: 1528, loss: 0.75\n",
      "train epoch: 1529, loss 0.0946\n",
      "test epoch: 1529, loss: 0.75\n",
      "train epoch: 1530, loss 0.0946\n",
      "test epoch: 1530, loss: 0.75\n",
      "train epoch: 1531, loss 0.0946\n",
      "test epoch: 1531, loss: 0.75\n",
      "train epoch: 1532, loss 0.0945\n",
      "test epoch: 1532, loss: 0.75\n",
      "train epoch: 1533, loss 0.0945\n",
      "test epoch: 1533, loss: 0.75\n",
      "train epoch: 1534, loss 0.0945\n",
      "test epoch: 1534, loss: 0.75\n",
      "train epoch: 1535, loss 0.0945\n",
      "test epoch: 1535, loss: 0.75\n",
      "train epoch: 1536, loss 0.0945\n",
      "test epoch: 1536, loss: 0.75\n",
      "train epoch: 1537, loss 0.0944\n",
      "test epoch: 1537, loss: 0.75\n",
      "train epoch: 1538, loss 0.0944\n",
      "test epoch: 1538, loss: 0.75\n",
      "train epoch: 1539, loss 0.0944\n",
      "test epoch: 1539, loss: 0.75\n",
      "train epoch: 1540, loss 0.0944\n",
      "test epoch: 1540, loss: 0.75\n",
      "train epoch: 1541, loss 0.0944\n",
      "test epoch: 1541, loss: 0.75\n",
      "train epoch: 1542, loss 0.0943\n",
      "test epoch: 1542, loss: 0.76\n",
      "train epoch: 1543, loss 0.0943\n",
      "test epoch: 1543, loss: 0.76\n",
      "train epoch: 1544, loss 0.0943\n",
      "test epoch: 1544, loss: 0.76\n",
      "train epoch: 1545, loss 0.0943\n",
      "test epoch: 1545, loss: 0.76\n",
      "train epoch: 1546, loss 0.0943\n",
      "test epoch: 1546, loss: 0.76\n",
      "train epoch: 1547, loss 0.0942\n",
      "test epoch: 1547, loss: 0.76\n",
      "train epoch: 1548, loss 0.0942\n",
      "test epoch: 1548, loss: 0.76\n",
      "train epoch: 1549, loss 0.0942\n",
      "test epoch: 1549, loss: 0.76\n",
      "train epoch: 1550, loss 0.0942\n",
      "test epoch: 1550, loss: 0.76\n",
      "train epoch: 1551, loss 0.0941\n",
      "test epoch: 1551, loss: 0.76\n",
      "train epoch: 1552, loss 0.0941\n",
      "test epoch: 1552, loss: 0.76\n",
      "train epoch: 1553, loss 0.0941\n",
      "test epoch: 1553, loss: 0.76\n",
      "train epoch: 1554, loss 0.0941\n",
      "test epoch: 1554, loss: 0.76\n",
      "train epoch: 1555, loss 0.0941\n",
      "test epoch: 1555, loss: 0.76\n",
      "train epoch: 1556, loss 0.0940\n",
      "test epoch: 1556, loss: 0.76\n",
      "train epoch: 1557, loss 0.0940\n",
      "test epoch: 1557, loss: 0.76\n",
      "train epoch: 1558, loss 0.0940\n",
      "test epoch: 1558, loss: 0.76\n",
      "train epoch: 1559, loss 0.0940\n",
      "test epoch: 1559, loss: 0.76\n",
      "train epoch: 1560, loss 0.0939\n",
      "test epoch: 1560, loss: 0.76\n",
      "train epoch: 1561, loss 0.0939\n",
      "test epoch: 1561, loss: 0.76\n",
      "train epoch: 1562, loss 0.0939\n",
      "test epoch: 1562, loss: 0.76\n",
      "train epoch: 1563, loss 0.0939\n",
      "test epoch: 1563, loss: 0.76\n",
      "train epoch: 1564, loss 0.0939\n",
      "test epoch: 1564, loss: 0.76\n",
      "train epoch: 1565, loss 0.0938\n",
      "test epoch: 1565, loss: 0.76\n",
      "train epoch: 1566, loss 0.0938\n",
      "test epoch: 1566, loss: 0.76\n",
      "train epoch: 1567, loss 0.0938\n",
      "test epoch: 1567, loss: 0.76\n",
      "train epoch: 1568, loss 0.0938\n",
      "test epoch: 1568, loss: 0.76\n",
      "train epoch: 1569, loss 0.0938\n",
      "test epoch: 1569, loss: 0.76\n",
      "train epoch: 1570, loss 0.0938\n",
      "test epoch: 1570, loss: 0.76\n",
      "train epoch: 1571, loss 0.0938\n",
      "test epoch: 1571, loss: 0.76\n",
      "train epoch: 1572, loss 0.0937\n",
      "test epoch: 1572, loss: 0.76\n",
      "train epoch: 1573, loss 0.0937\n",
      "test epoch: 1573, loss: 0.76\n",
      "train epoch: 1574, loss 0.0937\n",
      "test epoch: 1574, loss: 0.76\n",
      "train epoch: 1575, loss 0.0937\n",
      "test epoch: 1575, loss: 0.76\n",
      "train epoch: 1576, loss 0.0936\n",
      "test epoch: 1576, loss: 0.76\n",
      "train epoch: 1577, loss 0.0936\n",
      "test epoch: 1577, loss: 0.76\n",
      "train epoch: 1578, loss 0.0936\n",
      "test epoch: 1578, loss: 0.76\n",
      "train epoch: 1579, loss 0.0936\n",
      "test epoch: 1579, loss: 0.76\n",
      "train epoch: 1580, loss 0.0936\n",
      "test epoch: 1580, loss: 0.76\n",
      "train epoch: 1581, loss 0.0935\n",
      "test epoch: 1581, loss: 0.76\n",
      "train epoch: 1582, loss 0.0935\n",
      "test epoch: 1582, loss: 0.76\n",
      "train epoch: 1583, loss 0.0935\n",
      "test epoch: 1583, loss: 0.76\n",
      "train epoch: 1584, loss 0.0935\n",
      "test epoch: 1584, loss: 0.76\n",
      "train epoch: 1585, loss 0.0935\n",
      "test epoch: 1585, loss: 0.76\n",
      "train epoch: 1586, loss 0.0934\n",
      "test epoch: 1586, loss: 0.76\n",
      "train epoch: 1587, loss 0.0934\n",
      "test epoch: 1587, loss: 0.76\n",
      "train epoch: 1588, loss 0.0934\n",
      "test epoch: 1588, loss: 0.76\n",
      "train epoch: 1589, loss 0.0934\n",
      "test epoch: 1589, loss: 0.76\n",
      "train epoch: 1590, loss 0.0937\n",
      "test epoch: 1590, loss: 0.76\n",
      "train epoch: 1591, loss 0.0934\n",
      "test epoch: 1591, loss: 0.76\n",
      "train epoch: 1592, loss 0.0941\n",
      "test epoch: 1592, loss: 0.76\n",
      "train epoch: 1593, loss 0.0935\n",
      "test epoch: 1593, loss: 0.76\n",
      "train epoch: 1594, loss 0.0935\n",
      "test epoch: 1594, loss: 0.76\n",
      "train epoch: 1595, loss 0.0940\n",
      "test epoch: 1595, loss: 0.76\n",
      "train epoch: 1596, loss 0.0932\n",
      "test epoch: 1596, loss: 0.76\n",
      "train epoch: 1597, loss 0.0934\n",
      "test epoch: 1597, loss: 0.76\n",
      "train epoch: 1598, loss 0.0932\n",
      "test epoch: 1598, loss: 0.76\n",
      "train epoch: 1599, loss 0.0939\n",
      "test epoch: 1599, loss: 0.76\n",
      "train epoch: 1600, loss 0.0933\n",
      "test epoch: 1600, loss: 0.76\n",
      "train epoch: 1601, loss 0.0934\n",
      "test epoch: 1601, loss: 0.76\n",
      "train epoch: 1602, loss 0.0935\n",
      "test epoch: 1602, loss: 0.76\n",
      "train epoch: 1603, loss 0.0931\n",
      "test epoch: 1603, loss: 0.76\n",
      "train epoch: 1604, loss 0.0932\n",
      "test epoch: 1604, loss: 0.76\n",
      "train epoch: 1605, loss 0.0931\n",
      "test epoch: 1605, loss: 0.76\n",
      "train epoch: 1606, loss 0.0936\n",
      "test epoch: 1606, loss: 0.76\n",
      "train epoch: 1607, loss 0.0934\n",
      "test epoch: 1607, loss: 0.76\n",
      "train epoch: 1608, loss 0.0933\n",
      "test epoch: 1608, loss: 0.76\n",
      "train epoch: 1609, loss 0.0934\n",
      "test epoch: 1609, loss: 0.76\n",
      "train epoch: 1610, loss 0.0930\n",
      "test epoch: 1610, loss: 0.76\n",
      "train epoch: 1611, loss 0.0930\n",
      "test epoch: 1611, loss: 0.76\n",
      "train epoch: 1612, loss 0.0929\n",
      "test epoch: 1612, loss: 0.76\n",
      "train epoch: 1613, loss 0.0930\n",
      "test epoch: 1613, loss: 0.76\n",
      "train epoch: 1614, loss 0.0929\n",
      "test epoch: 1614, loss: 0.77\n",
      "train epoch: 1615, loss 0.0935\n",
      "test epoch: 1615, loss: 0.77\n",
      "train epoch: 1616, loss 0.0928\n",
      "test epoch: 1616, loss: 0.77\n",
      "train epoch: 1617, loss 0.0941\n",
      "test epoch: 1617, loss: 0.77\n",
      "train epoch: 1618, loss 0.0928\n",
      "test epoch: 1618, loss: 0.77\n",
      "train epoch: 1619, loss 0.0932\n",
      "test epoch: 1619, loss: 0.77\n",
      "train epoch: 1620, loss 0.0931\n",
      "test epoch: 1620, loss: 0.77\n",
      "train epoch: 1621, loss 0.0927\n",
      "test epoch: 1621, loss: 0.77\n",
      "train epoch: 1622, loss 0.0927\n",
      "test epoch: 1622, loss: 0.77\n",
      "train epoch: 1623, loss 0.0927\n",
      "test epoch: 1623, loss: 0.77\n",
      "train epoch: 1624, loss 0.0927\n",
      "test epoch: 1624, loss: 0.77\n",
      "train epoch: 1625, loss 0.0928\n",
      "test epoch: 1625, loss: 0.77\n",
      "train epoch: 1626, loss 0.0927\n",
      "test epoch: 1626, loss: 0.77\n",
      "train epoch: 1627, loss 0.0929\n",
      "test epoch: 1627, loss: 0.77\n",
      "train epoch: 1628, loss 0.0927\n",
      "test epoch: 1628, loss: 0.77\n",
      "train epoch: 1629, loss 0.0927\n",
      "test epoch: 1629, loss: 0.77\n",
      "train epoch: 1630, loss 0.0926\n",
      "test epoch: 1630, loss: 0.77\n",
      "train epoch: 1631, loss 0.0931\n",
      "test epoch: 1631, loss: 0.77\n",
      "train epoch: 1632, loss 0.0928\n",
      "test epoch: 1632, loss: 0.77\n",
      "train epoch: 1633, loss 0.0928\n",
      "test epoch: 1633, loss: 0.77\n",
      "train epoch: 1634, loss 0.0929\n",
      "test epoch: 1634, loss: 0.77\n",
      "train epoch: 1635, loss 0.0925\n",
      "test epoch: 1635, loss: 0.77\n",
      "train epoch: 1636, loss 0.0925\n",
      "test epoch: 1636, loss: 0.77\n",
      "train epoch: 1637, loss 0.0924\n",
      "test epoch: 1637, loss: 0.77\n",
      "train epoch: 1638, loss 0.0926\n",
      "test epoch: 1638, loss: 0.77\n",
      "train epoch: 1639, loss 0.0926\n",
      "test epoch: 1639, loss: 0.77\n",
      "train epoch: 1640, loss 0.0925\n",
      "test epoch: 1640, loss: 0.77\n",
      "train epoch: 1641, loss 0.0927\n",
      "test epoch: 1641, loss: 0.77\n",
      "train epoch: 1642, loss 0.0923\n",
      "test epoch: 1642, loss: 0.77\n",
      "train epoch: 1643, loss 0.0924\n",
      "test epoch: 1643, loss: 0.77\n",
      "train epoch: 1644, loss 0.0923\n",
      "test epoch: 1644, loss: 0.77\n",
      "train epoch: 1645, loss 0.0927\n",
      "test epoch: 1645, loss: 0.77\n",
      "train epoch: 1646, loss 0.0925\n",
      "test epoch: 1646, loss: 0.77\n",
      "train epoch: 1647, loss 0.0924\n",
      "test epoch: 1647, loss: 0.77\n",
      "train epoch: 1648, loss 0.0926\n",
      "test epoch: 1648, loss: 0.77\n",
      "train epoch: 1649, loss 0.0922\n",
      "test epoch: 1649, loss: 0.77\n",
      "train epoch: 1650, loss 0.0923\n",
      "test epoch: 1650, loss: 0.77\n",
      "train epoch: 1651, loss 0.0922\n",
      "test epoch: 1651, loss: 0.77\n",
      "train epoch: 1652, loss 0.0926\n",
      "test epoch: 1652, loss: 0.77\n",
      "train epoch: 1653, loss 0.0923\n",
      "test epoch: 1653, loss: 0.77\n",
      "train epoch: 1654, loss 0.0923\n",
      "test epoch: 1654, loss: 0.77\n",
      "train epoch: 1655, loss 0.0926\n",
      "test epoch: 1655, loss: 0.77\n",
      "train epoch: 1656, loss 0.0921\n",
      "test epoch: 1656, loss: 0.77\n",
      "train epoch: 1657, loss 0.0922\n",
      "test epoch: 1657, loss: 0.77\n",
      "train epoch: 1658, loss 0.0920\n",
      "test epoch: 1658, loss: 0.77\n",
      "train epoch: 1659, loss 0.0925\n",
      "test epoch: 1659, loss: 0.77\n",
      "train epoch: 1660, loss 0.0922\n",
      "test epoch: 1660, loss: 0.77\n",
      "train epoch: 1661, loss 0.0921\n",
      "test epoch: 1661, loss: 0.77\n",
      "train epoch: 1662, loss 0.0924\n",
      "test epoch: 1662, loss: 0.77\n",
      "train epoch: 1663, loss 0.0920\n",
      "test epoch: 1663, loss: 0.77\n",
      "train epoch: 1664, loss 0.0920\n",
      "test epoch: 1664, loss: 0.77\n",
      "train epoch: 1665, loss 0.0919\n",
      "test epoch: 1665, loss: 0.77\n",
      "train epoch: 1666, loss 0.0924\n",
      "test epoch: 1666, loss: 0.77\n",
      "train epoch: 1667, loss 0.0921\n",
      "test epoch: 1667, loss: 0.77\n",
      "train epoch: 1668, loss 0.0920\n",
      "test epoch: 1668, loss: 0.77\n",
      "train epoch: 1669, loss 0.0925\n",
      "test epoch: 1669, loss: 0.77\n",
      "train epoch: 1670, loss 0.0918\n",
      "test epoch: 1670, loss: 0.77\n",
      "train epoch: 1671, loss 0.0919\n",
      "test epoch: 1671, loss: 0.77\n",
      "train epoch: 1672, loss 0.0918\n",
      "test epoch: 1672, loss: 0.77\n",
      "train epoch: 1673, loss 0.0923\n",
      "test epoch: 1673, loss: 0.77\n",
      "train epoch: 1674, loss 0.0920\n",
      "test epoch: 1674, loss: 0.77\n",
      "train epoch: 1675, loss 0.0919\n",
      "test epoch: 1675, loss: 0.77\n",
      "train epoch: 1676, loss 0.0923\n",
      "test epoch: 1676, loss: 0.77\n",
      "train epoch: 1677, loss 0.0917\n",
      "test epoch: 1677, loss: 0.77\n",
      "train epoch: 1678, loss 0.0918\n",
      "test epoch: 1678, loss: 0.77\n",
      "train epoch: 1679, loss 0.0917\n",
      "test epoch: 1679, loss: 0.77\n",
      "train epoch: 1680, loss 0.0918\n",
      "test epoch: 1680, loss: 0.77\n",
      "train epoch: 1681, loss 0.0917\n",
      "test epoch: 1681, loss: 0.77\n",
      "train epoch: 1682, loss 0.0921\n",
      "test epoch: 1682, loss: 0.77\n",
      "train epoch: 1683, loss 0.0918\n",
      "test epoch: 1683, loss: 0.77\n",
      "train epoch: 1684, loss 0.0918\n",
      "test epoch: 1684, loss: 0.77\n",
      "train epoch: 1685, loss 0.0920\n",
      "test epoch: 1685, loss: 0.77\n",
      "train epoch: 1686, loss 0.0916\n",
      "test epoch: 1686, loss: 0.77\n",
      "train epoch: 1687, loss 0.0916\n",
      "test epoch: 1687, loss: 0.77\n",
      "train epoch: 1688, loss 0.0918\n",
      "test epoch: 1688, loss: 0.77\n",
      "train epoch: 1689, loss 0.0916\n",
      "test epoch: 1689, loss: 0.78\n",
      "train epoch: 1690, loss 0.0915\n",
      "test epoch: 1690, loss: 0.78\n",
      "train epoch: 1691, loss 0.0920\n",
      "test epoch: 1691, loss: 0.78\n",
      "train epoch: 1692, loss 0.0916\n",
      "test epoch: 1692, loss: 0.78\n",
      "train epoch: 1693, loss 0.0916\n",
      "test epoch: 1693, loss: 0.78\n",
      "train epoch: 1694, loss 0.0918\n",
      "test epoch: 1694, loss: 0.78\n",
      "train epoch: 1695, loss 0.0914\n",
      "test epoch: 1695, loss: 0.78\n",
      "train epoch: 1696, loss 0.0914\n",
      "test epoch: 1696, loss: 0.78\n",
      "train epoch: 1697, loss 0.0917\n",
      "test epoch: 1697, loss: 0.78\n",
      "train epoch: 1698, loss 0.0915\n",
      "test epoch: 1698, loss: 0.78\n",
      "train epoch: 1699, loss 0.0913\n",
      "test epoch: 1699, loss: 0.78\n",
      "train epoch: 1700, loss 0.0919\n",
      "test epoch: 1700, loss: 0.78\n",
      "train epoch: 1701, loss 0.0914\n",
      "test epoch: 1701, loss: 0.78\n",
      "train epoch: 1702, loss 0.0913\n",
      "test epoch: 1702, loss: 0.78\n",
      "train epoch: 1703, loss 0.0917\n",
      "test epoch: 1703, loss: 0.78\n",
      "train epoch: 1704, loss 0.0913\n",
      "test epoch: 1704, loss: 0.78\n",
      "train epoch: 1705, loss 0.0913\n",
      "test epoch: 1705, loss: 0.78\n",
      "train epoch: 1706, loss 0.0916\n",
      "test epoch: 1706, loss: 0.78\n",
      "train epoch: 1707, loss 0.0913\n",
      "test epoch: 1707, loss: 0.78\n",
      "train epoch: 1708, loss 0.0912\n",
      "test epoch: 1708, loss: 0.78\n",
      "train epoch: 1709, loss 0.0917\n",
      "test epoch: 1709, loss: 0.78\n",
      "train epoch: 1710, loss 0.0913\n",
      "test epoch: 1710, loss: 0.78\n",
      "train epoch: 1711, loss 0.0912\n",
      "test epoch: 1711, loss: 0.78\n",
      "train epoch: 1712, loss 0.0917\n",
      "test epoch: 1712, loss: 0.78\n",
      "train epoch: 1713, loss 0.0912\n",
      "test epoch: 1713, loss: 0.78\n",
      "train epoch: 1714, loss 0.0911\n",
      "test epoch: 1714, loss: 0.78\n",
      "train epoch: 1715, loss 0.0916\n",
      "test epoch: 1715, loss: 0.78\n",
      "train epoch: 1716, loss 0.0912\n",
      "test epoch: 1716, loss: 0.78\n",
      "train epoch: 1717, loss 0.0911\n",
      "test epoch: 1717, loss: 0.78\n",
      "train epoch: 1718, loss 0.0915\n",
      "test epoch: 1718, loss: 0.78\n",
      "train epoch: 1719, loss 0.0911\n",
      "test epoch: 1719, loss: 0.78\n",
      "train epoch: 1720, loss 0.0911\n",
      "test epoch: 1720, loss: 0.78\n",
      "train epoch: 1721, loss 0.0915\n",
      "test epoch: 1721, loss: 0.78\n",
      "train epoch: 1722, loss 0.0910\n",
      "test epoch: 1722, loss: 0.78\n",
      "train epoch: 1723, loss 0.0910\n",
      "test epoch: 1723, loss: 0.78\n",
      "train epoch: 1724, loss 0.0915\n",
      "test epoch: 1724, loss: 0.78\n",
      "train epoch: 1725, loss 0.0910\n",
      "test epoch: 1725, loss: 0.78\n",
      "train epoch: 1726, loss 0.0910\n",
      "test epoch: 1726, loss: 0.78\n",
      "train epoch: 1727, loss 0.0914\n",
      "test epoch: 1727, loss: 0.78\n",
      "train epoch: 1728, loss 0.0910\n",
      "test epoch: 1728, loss: 0.78\n",
      "train epoch: 1729, loss 0.0909\n",
      "test epoch: 1729, loss: 0.78\n",
      "train epoch: 1730, loss 0.0914\n",
      "test epoch: 1730, loss: 0.78\n",
      "train epoch: 1731, loss 0.0909\n",
      "test epoch: 1731, loss: 0.78\n",
      "train epoch: 1732, loss 0.0908\n",
      "test epoch: 1732, loss: 0.78\n",
      "train epoch: 1733, loss 0.0914\n",
      "test epoch: 1733, loss: 0.78\n",
      "train epoch: 1734, loss 0.0908\n",
      "test epoch: 1734, loss: 0.78\n",
      "train epoch: 1735, loss 0.0908\n",
      "test epoch: 1735, loss: 0.78\n",
      "train epoch: 1736, loss 0.0913\n",
      "test epoch: 1736, loss: 0.78\n",
      "train epoch: 1737, loss 0.0909\n",
      "test epoch: 1737, loss: 0.78\n",
      "train epoch: 1738, loss 0.0908\n",
      "test epoch: 1738, loss: 0.78\n",
      "train epoch: 1739, loss 0.0913\n",
      "test epoch: 1739, loss: 0.78\n",
      "train epoch: 1740, loss 0.0908\n",
      "test epoch: 1740, loss: 0.78\n",
      "train epoch: 1741, loss 0.0907\n",
      "test epoch: 1741, loss: 0.78\n",
      "train epoch: 1742, loss 0.0913\n",
      "test epoch: 1742, loss: 0.78\n",
      "train epoch: 1743, loss 0.0907\n",
      "test epoch: 1743, loss: 0.78\n",
      "train epoch: 1744, loss 0.0906\n",
      "test epoch: 1744, loss: 0.78\n",
      "train epoch: 1745, loss 0.0912\n",
      "test epoch: 1745, loss: 0.78\n",
      "train epoch: 1746, loss 0.0906\n",
      "test epoch: 1746, loss: 0.78\n",
      "train epoch: 1747, loss 0.0906\n",
      "test epoch: 1747, loss: 0.78\n",
      "train epoch: 1748, loss 0.0912\n",
      "test epoch: 1748, loss: 0.78\n",
      "train epoch: 1749, loss 0.0906\n",
      "test epoch: 1749, loss: 0.78\n",
      "train epoch: 1750, loss 0.0906\n",
      "test epoch: 1750, loss: 0.78\n",
      "train epoch: 1751, loss 0.0912\n",
      "test epoch: 1751, loss: 0.78\n",
      "train epoch: 1752, loss 0.0906\n",
      "test epoch: 1752, loss: 0.78\n",
      "train epoch: 1753, loss 0.0905\n",
      "test epoch: 1753, loss: 0.78\n",
      "train epoch: 1754, loss 0.0911\n",
      "test epoch: 1754, loss: 0.78\n",
      "train epoch: 1755, loss 0.0905\n",
      "test epoch: 1755, loss: 0.78\n",
      "train epoch: 1756, loss 0.0904\n",
      "test epoch: 1756, loss: 0.78\n",
      "train epoch: 1757, loss 0.0907\n",
      "test epoch: 1757, loss: 0.78\n",
      "train epoch: 1758, loss 0.0907\n",
      "test epoch: 1758, loss: 0.78\n",
      "train epoch: 1759, loss 0.0906\n",
      "test epoch: 1759, loss: 0.78\n",
      "train epoch: 1760, loss 0.0905\n",
      "test epoch: 1760, loss: 0.78\n",
      "train epoch: 1761, loss 0.0908\n",
      "test epoch: 1761, loss: 0.78\n",
      "train epoch: 1762, loss 0.0905\n",
      "test epoch: 1762, loss: 0.78\n",
      "train epoch: 1763, loss 0.0905\n",
      "test epoch: 1763, loss: 0.78\n",
      "train epoch: 1764, loss 0.0904\n",
      "test epoch: 1764, loss: 0.78\n",
      "train epoch: 1765, loss 0.0907\n",
      "test epoch: 1765, loss: 0.78\n",
      "train epoch: 1766, loss 0.0904\n",
      "test epoch: 1766, loss: 0.78\n",
      "train epoch: 1767, loss 0.0903\n",
      "test epoch: 1767, loss: 0.78\n",
      "train epoch: 1768, loss 0.0909\n",
      "test epoch: 1768, loss: 0.79\n",
      "train epoch: 1769, loss 0.0904\n",
      "test epoch: 1769, loss: 0.79\n",
      "train epoch: 1770, loss 0.0903\n",
      "test epoch: 1770, loss: 0.79\n",
      "train epoch: 1771, loss 0.0903\n",
      "test epoch: 1771, loss: 0.79\n",
      "train epoch: 1772, loss 0.0908\n",
      "test epoch: 1772, loss: 0.79\n",
      "train epoch: 1773, loss 0.0903\n",
      "test epoch: 1773, loss: 0.79\n",
      "train epoch: 1774, loss 0.0902\n",
      "test epoch: 1774, loss: 0.79\n",
      "train epoch: 1775, loss 0.0907\n",
      "test epoch: 1775, loss: 0.79\n",
      "train epoch: 1776, loss 0.0902\n",
      "test epoch: 1776, loss: 0.79\n",
      "train epoch: 1777, loss 0.0903\n",
      "test epoch: 1777, loss: 0.79\n",
      "train epoch: 1778, loss 0.0903\n",
      "test epoch: 1778, loss: 0.79\n",
      "train epoch: 1779, loss 0.0906\n",
      "test epoch: 1779, loss: 0.79\n",
      "train epoch: 1780, loss 0.0902\n",
      "test epoch: 1780, loss: 0.79\n",
      "train epoch: 1781, loss 0.0901\n",
      "test epoch: 1781, loss: 0.79\n",
      "train epoch: 1782, loss 0.0906\n",
      "test epoch: 1782, loss: 0.79\n",
      "train epoch: 1783, loss 0.0902\n",
      "test epoch: 1783, loss: 0.79\n",
      "train epoch: 1784, loss 0.0902\n",
      "test epoch: 1784, loss: 0.79\n",
      "train epoch: 1785, loss 0.0901\n",
      "test epoch: 1785, loss: 0.79\n",
      "train epoch: 1786, loss 0.0905\n",
      "test epoch: 1786, loss: 0.79\n",
      "train epoch: 1787, loss 0.0901\n",
      "test epoch: 1787, loss: 0.79\n",
      "train epoch: 1788, loss 0.0900\n",
      "test epoch: 1788, loss: 0.79\n",
      "train epoch: 1789, loss 0.0906\n",
      "test epoch: 1789, loss: 0.79\n",
      "train epoch: 1790, loss 0.0901\n",
      "test epoch: 1790, loss: 0.79\n",
      "train epoch: 1791, loss 0.0901\n",
      "test epoch: 1791, loss: 0.79\n",
      "train epoch: 1792, loss 0.0900\n",
      "test epoch: 1792, loss: 0.79\n",
      "train epoch: 1793, loss 0.0905\n",
      "test epoch: 1793, loss: 0.79\n",
      "train epoch: 1794, loss 0.0900\n",
      "test epoch: 1794, loss: 0.79\n",
      "train epoch: 1795, loss 0.0899\n",
      "test epoch: 1795, loss: 0.79\n",
      "train epoch: 1796, loss 0.0904\n",
      "test epoch: 1796, loss: 0.79\n",
      "train epoch: 1797, loss 0.0899\n",
      "test epoch: 1797, loss: 0.79\n",
      "train epoch: 1798, loss 0.0900\n",
      "test epoch: 1798, loss: 0.79\n",
      "train epoch: 1799, loss 0.0899\n",
      "test epoch: 1799, loss: 0.79\n",
      "train epoch: 1800, loss 0.0903\n",
      "test epoch: 1800, loss: 0.79\n",
      "train epoch: 1801, loss 0.0899\n",
      "test epoch: 1801, loss: 0.79\n",
      "train epoch: 1802, loss 0.0898\n",
      "test epoch: 1802, loss: 0.79\n",
      "train epoch: 1803, loss 0.0900\n",
      "test epoch: 1803, loss: 0.79\n",
      "train epoch: 1804, loss 0.0901\n",
      "test epoch: 1804, loss: 0.79\n",
      "train epoch: 1805, loss 0.0898\n",
      "test epoch: 1805, loss: 0.79\n",
      "train epoch: 1806, loss 0.0899\n",
      "test epoch: 1806, loss: 0.79\n",
      "train epoch: 1807, loss 0.0899\n",
      "test epoch: 1807, loss: 0.79\n",
      "train epoch: 1808, loss 0.0901\n",
      "test epoch: 1808, loss: 0.79\n",
      "train epoch: 1809, loss 0.0898\n",
      "test epoch: 1809, loss: 0.79\n",
      "train epoch: 1810, loss 0.0903\n",
      "test epoch: 1810, loss: 0.79\n",
      "train epoch: 1811, loss 0.0898\n",
      "test epoch: 1811, loss: 0.79\n",
      "train epoch: 1812, loss 0.0898\n",
      "test epoch: 1812, loss: 0.79\n",
      "train epoch: 1813, loss 0.0898\n",
      "test epoch: 1813, loss: 0.79\n",
      "train epoch: 1814, loss 0.0901\n",
      "test epoch: 1814, loss: 0.79\n",
      "train epoch: 1815, loss 0.0899\n",
      "test epoch: 1815, loss: 0.79\n",
      "train epoch: 1816, loss 0.0899\n",
      "test epoch: 1816, loss: 0.79\n",
      "train epoch: 1817, loss 0.0900\n",
      "test epoch: 1817, loss: 0.79\n",
      "train epoch: 1818, loss 0.0898\n",
      "test epoch: 1818, loss: 0.79\n",
      "train epoch: 1819, loss 0.0897\n",
      "test epoch: 1819, loss: 0.79\n",
      "train epoch: 1820, loss 0.0898\n",
      "test epoch: 1820, loss: 0.79\n",
      "train epoch: 1821, loss 0.0899\n",
      "test epoch: 1821, loss: 0.79\n",
      "train epoch: 1822, loss 0.0897\n",
      "test epoch: 1822, loss: 0.79\n",
      "train epoch: 1823, loss 0.0897\n",
      "test epoch: 1823, loss: 0.79\n",
      "train epoch: 1824, loss 0.0899\n",
      "test epoch: 1824, loss: 0.79\n",
      "train epoch: 1825, loss 0.0897\n",
      "test epoch: 1825, loss: 0.79\n",
      "train epoch: 1826, loss 0.0896\n",
      "test epoch: 1826, loss: 0.79\n",
      "train epoch: 1827, loss 0.0897\n",
      "test epoch: 1827, loss: 0.79\n",
      "train epoch: 1828, loss 0.0897\n",
      "test epoch: 1828, loss: 0.79\n",
      "train epoch: 1829, loss 0.0898\n",
      "test epoch: 1829, loss: 0.79\n",
      "train epoch: 1830, loss 0.0896\n",
      "test epoch: 1830, loss: 0.79\n",
      "train epoch: 1831, loss 0.0899\n",
      "test epoch: 1831, loss: 0.79\n",
      "train epoch: 1832, loss 0.0897\n",
      "test epoch: 1832, loss: 0.79\n",
      "train epoch: 1833, loss 0.0895\n",
      "test epoch: 1833, loss: 0.79\n",
      "train epoch: 1834, loss 0.0902\n",
      "test epoch: 1834, loss: 0.79\n",
      "train epoch: 1835, loss 0.0895\n",
      "test epoch: 1835, loss: 0.79\n",
      "train epoch: 1836, loss 0.0912\n",
      "test epoch: 1836, loss: 0.79\n",
      "train epoch: 1837, loss 0.0899\n",
      "test epoch: 1837, loss: 0.79\n",
      "train epoch: 1838, loss 0.0899\n",
      "test epoch: 1838, loss: 0.79\n",
      "train epoch: 1839, loss 0.0911\n",
      "test epoch: 1839, loss: 0.79\n",
      "train epoch: 1840, loss 0.0895\n",
      "test epoch: 1840, loss: 0.79\n",
      "train epoch: 1841, loss 0.0911\n",
      "test epoch: 1841, loss: 0.79\n",
      "train epoch: 1842, loss 0.0896\n",
      "test epoch: 1842, loss: 0.79\n",
      "train epoch: 1843, loss 0.0923\n",
      "test epoch: 1843, loss: 0.79\n",
      "train epoch: 1844, loss 0.0902\n",
      "test epoch: 1844, loss: 0.79\n",
      "train epoch: 1845, loss 0.0908\n",
      "test epoch: 1845, loss: 0.79\n",
      "train epoch: 1846, loss 0.0905\n",
      "test epoch: 1846, loss: 0.79\n",
      "train epoch: 1847, loss 0.0898\n",
      "test epoch: 1847, loss: 0.79\n",
      "train epoch: 1848, loss 0.0902\n",
      "test epoch: 1848, loss: 0.79\n",
      "train epoch: 1849, loss 0.0896\n",
      "test epoch: 1849, loss: 0.79\n",
      "train epoch: 1850, loss 0.0908\n",
      "test epoch: 1850, loss: 0.79\n",
      "train epoch: 1851, loss 0.0894\n",
      "test epoch: 1851, loss: 0.79\n",
      "train epoch: 1852, loss 0.0909\n",
      "test epoch: 1852, loss: 0.80\n",
      "train epoch: 1853, loss 0.0895\n",
      "test epoch: 1853, loss: 0.80\n",
      "train epoch: 1854, loss 0.0905\n",
      "test epoch: 1854, loss: 0.80\n",
      "train epoch: 1855, loss 0.0898\n",
      "test epoch: 1855, loss: 0.80\n",
      "train epoch: 1856, loss 0.0896\n",
      "test epoch: 1856, loss: 0.80\n",
      "train epoch: 1857, loss 0.0903\n",
      "test epoch: 1857, loss: 0.80\n",
      "train epoch: 1858, loss 0.0896\n",
      "test epoch: 1858, loss: 0.80\n",
      "train epoch: 1859, loss 0.0904\n",
      "test epoch: 1859, loss: 0.80\n",
      "train epoch: 1860, loss 0.0893\n",
      "test epoch: 1860, loss: 0.80\n",
      "train epoch: 1861, loss 0.0916\n",
      "test epoch: 1861, loss: 0.80\n",
      "train epoch: 1862, loss 0.0894\n",
      "test epoch: 1862, loss: 0.80\n",
      "train epoch: 1863, loss 0.0916\n",
      "test epoch: 1863, loss: 0.80\n",
      "train epoch: 1864, loss 0.0893\n",
      "test epoch: 1864, loss: 0.80\n",
      "train epoch: 1865, loss 0.0908\n",
      "test epoch: 1865, loss: 0.80\n",
      "train epoch: 1866, loss 0.0899\n",
      "test epoch: 1866, loss: 0.80\n",
      "train epoch: 1867, loss 0.0900\n",
      "test epoch: 1867, loss: 0.80\n",
      "train epoch: 1868, loss 0.0897\n",
      "test epoch: 1868, loss: 0.80\n",
      "train epoch: 1869, loss 0.0898\n",
      "test epoch: 1869, loss: 0.80\n",
      "train epoch: 1870, loss 0.0896\n",
      "test epoch: 1870, loss: 0.80\n",
      "train epoch: 1871, loss 0.0892\n",
      "test epoch: 1871, loss: 0.80\n",
      "train epoch: 1872, loss 0.0892\n",
      "test epoch: 1872, loss: 0.80\n",
      "train epoch: 1873, loss 0.0890\n",
      "test epoch: 1873, loss: 0.80\n",
      "train epoch: 1874, loss 0.0896\n",
      "test epoch: 1874, loss: 0.80\n",
      "train epoch: 1875, loss 0.0892\n",
      "test epoch: 1875, loss: 0.80\n",
      "train epoch: 1876, loss 0.0901\n",
      "test epoch: 1876, loss: 0.80\n",
      "train epoch: 1877, loss 0.0892\n",
      "test epoch: 1877, loss: 0.80\n",
      "train epoch: 1878, loss 0.0904\n",
      "test epoch: 1878, loss: 0.80\n",
      "train epoch: 1879, loss 0.0893\n",
      "test epoch: 1879, loss: 0.80\n",
      "train epoch: 1880, loss 0.0900\n",
      "test epoch: 1880, loss: 0.80\n",
      "train epoch: 1881, loss 0.0891\n",
      "test epoch: 1881, loss: 0.80\n",
      "train epoch: 1882, loss 0.0892\n",
      "test epoch: 1882, loss: 0.80\n",
      "train epoch: 1883, loss 0.0895\n",
      "test epoch: 1883, loss: 0.80\n",
      "train epoch: 1884, loss 0.0892\n",
      "test epoch: 1884, loss: 0.80\n",
      "train epoch: 1885, loss 0.0895\n",
      "test epoch: 1885, loss: 0.80\n",
      "train epoch: 1886, loss 0.0890\n",
      "test epoch: 1886, loss: 0.80\n",
      "train epoch: 1887, loss 0.0891\n",
      "test epoch: 1887, loss: 0.80\n",
      "train epoch: 1888, loss 0.0890\n",
      "test epoch: 1888, loss: 0.80\n",
      "train epoch: 1889, loss 0.0889\n",
      "test epoch: 1889, loss: 0.80\n",
      "train epoch: 1890, loss 0.0889\n",
      "test epoch: 1890, loss: 0.80\n",
      "train epoch: 1891, loss 0.0891\n",
      "test epoch: 1891, loss: 0.80\n",
      "train epoch: 1892, loss 0.0890\n",
      "test epoch: 1892, loss: 0.80\n",
      "train epoch: 1893, loss 0.0892\n",
      "test epoch: 1893, loss: 0.80\n",
      "train epoch: 1894, loss 0.0891\n",
      "test epoch: 1894, loss: 0.80\n",
      "train epoch: 1895, loss 0.0890\n",
      "test epoch: 1895, loss: 0.80\n",
      "train epoch: 1896, loss 0.0891\n",
      "test epoch: 1896, loss: 0.80\n",
      "train epoch: 1897, loss 0.0887\n",
      "test epoch: 1897, loss: 0.80\n",
      "train epoch: 1898, loss 0.0887\n",
      "test epoch: 1898, loss: 0.80\n",
      "train epoch: 1899, loss 0.0889\n",
      "test epoch: 1899, loss: 0.80\n",
      "train epoch: 1900, loss 0.0889\n",
      "test epoch: 1900, loss: 0.80\n",
      "train epoch: 1901, loss 0.0888\n",
      "test epoch: 1901, loss: 0.80\n",
      "train epoch: 1902, loss 0.0890\n",
      "test epoch: 1902, loss: 0.80\n",
      "train epoch: 1903, loss 0.0889\n",
      "test epoch: 1903, loss: 0.80\n",
      "train epoch: 1904, loss 0.0889\n",
      "test epoch: 1904, loss: 0.80\n",
      "train epoch: 1905, loss 0.0888\n",
      "test epoch: 1905, loss: 0.80\n",
      "train epoch: 1906, loss 0.0887\n",
      "test epoch: 1906, loss: 0.80\n",
      "train epoch: 1907, loss 0.0887\n",
      "test epoch: 1907, loss: 0.80\n",
      "train epoch: 1908, loss 0.0889\n",
      "test epoch: 1908, loss: 0.80\n",
      "train epoch: 1909, loss 0.0888\n",
      "test epoch: 1909, loss: 0.80\n",
      "train epoch: 1910, loss 0.0887\n",
      "test epoch: 1910, loss: 0.80\n",
      "train epoch: 1911, loss 0.0889\n",
      "test epoch: 1911, loss: 0.80\n",
      "train epoch: 1912, loss 0.0887\n",
      "test epoch: 1912, loss: 0.80\n",
      "train epoch: 1913, loss 0.0888\n",
      "test epoch: 1913, loss: 0.80\n",
      "train epoch: 1914, loss 0.0887\n",
      "test epoch: 1914, loss: 0.80\n",
      "train epoch: 1915, loss 0.0887\n",
      "test epoch: 1915, loss: 0.80\n",
      "train epoch: 1916, loss 0.0886\n",
      "test epoch: 1916, loss: 0.80\n",
      "train epoch: 1917, loss 0.0888\n",
      "test epoch: 1917, loss: 0.80\n",
      "train epoch: 1918, loss 0.0888\n",
      "test epoch: 1918, loss: 0.80\n",
      "train epoch: 1919, loss 0.0886\n",
      "test epoch: 1919, loss: 0.80\n",
      "train epoch: 1920, loss 0.0889\n",
      "test epoch: 1920, loss: 0.80\n",
      "train epoch: 1921, loss 0.0887\n",
      "test epoch: 1921, loss: 0.80\n",
      "train epoch: 1922, loss 0.0888\n",
      "test epoch: 1922, loss: 0.80\n",
      "train epoch: 1923, loss 0.0886\n",
      "test epoch: 1923, loss: 0.80\n",
      "train epoch: 1924, loss 0.0886\n",
      "test epoch: 1924, loss: 0.80\n",
      "train epoch: 1925, loss 0.0885\n",
      "test epoch: 1925, loss: 0.80\n",
      "train epoch: 1926, loss 0.0887\n",
      "test epoch: 1926, loss: 0.80\n",
      "train epoch: 1927, loss 0.0887\n",
      "test epoch: 1927, loss: 0.80\n",
      "train epoch: 1928, loss 0.0885\n",
      "test epoch: 1928, loss: 0.80\n",
      "train epoch: 1929, loss 0.0887\n",
      "test epoch: 1929, loss: 0.80\n",
      "train epoch: 1930, loss 0.0886\n",
      "test epoch: 1930, loss: 0.80\n",
      "train epoch: 1931, loss 0.0886\n",
      "test epoch: 1931, loss: 0.80\n",
      "train epoch: 1932, loss 0.0885\n",
      "test epoch: 1932, loss: 0.80\n",
      "train epoch: 1933, loss 0.0885\n",
      "test epoch: 1933, loss: 0.80\n",
      "train epoch: 1934, loss 0.0884\n",
      "test epoch: 1934, loss: 0.80\n",
      "train epoch: 1935, loss 0.0885\n",
      "test epoch: 1935, loss: 0.80\n",
      "train epoch: 1936, loss 0.0885\n",
      "test epoch: 1936, loss: 0.80\n",
      "train epoch: 1937, loss 0.0884\n",
      "test epoch: 1937, loss: 0.80\n",
      "train epoch: 1938, loss 0.0886\n",
      "test epoch: 1938, loss: 0.80\n",
      "train epoch: 1939, loss 0.0886\n",
      "test epoch: 1939, loss: 0.80\n",
      "train epoch: 1940, loss 0.0885\n",
      "test epoch: 1940, loss: 0.80\n",
      "train epoch: 1941, loss 0.0884\n",
      "test epoch: 1941, loss: 0.80\n",
      "train epoch: 1942, loss 0.0885\n",
      "test epoch: 1942, loss: 0.80\n",
      "train epoch: 1943, loss 0.0885\n",
      "test epoch: 1943, loss: 0.80\n",
      "train epoch: 1944, loss 0.0885\n",
      "test epoch: 1944, loss: 0.80\n",
      "train epoch: 1945, loss 0.0884\n",
      "test epoch: 1945, loss: 0.81\n",
      "train epoch: 1946, loss 0.0883\n",
      "test epoch: 1946, loss: 0.81\n",
      "train epoch: 1947, loss 0.0884\n",
      "test epoch: 1947, loss: 0.81\n",
      "train epoch: 1948, loss 0.0884\n",
      "test epoch: 1948, loss: 0.81\n",
      "train epoch: 1949, loss 0.0884\n",
      "test epoch: 1949, loss: 0.81\n",
      "train epoch: 1950, loss 0.0883\n",
      "test epoch: 1950, loss: 0.81\n",
      "train epoch: 1951, loss 0.0884\n",
      "test epoch: 1951, loss: 0.81\n",
      "train epoch: 1952, loss 0.0884\n",
      "test epoch: 1952, loss: 0.81\n",
      "train epoch: 1953, loss 0.0884\n",
      "test epoch: 1953, loss: 0.81\n",
      "train epoch: 1954, loss 0.0884\n",
      "test epoch: 1954, loss: 0.81\n",
      "train epoch: 1955, loss 0.0883\n",
      "test epoch: 1955, loss: 0.81\n",
      "train epoch: 1956, loss 0.0883\n",
      "test epoch: 1956, loss: 0.81\n",
      "train epoch: 1957, loss 0.0883\n",
      "test epoch: 1957, loss: 0.81\n",
      "train epoch: 1958, loss 0.0883\n",
      "test epoch: 1958, loss: 0.81\n",
      "train epoch: 1959, loss 0.0883\n",
      "test epoch: 1959, loss: 0.81\n",
      "train epoch: 1960, loss 0.0883\n",
      "test epoch: 1960, loss: 0.81\n",
      "train epoch: 1961, loss 0.0882\n",
      "test epoch: 1961, loss: 0.81\n",
      "train epoch: 1962, loss 0.0883\n",
      "test epoch: 1962, loss: 0.81\n",
      "train epoch: 1963, loss 0.0882\n",
      "test epoch: 1963, loss: 0.81\n",
      "train epoch: 1964, loss 0.0882\n",
      "test epoch: 1964, loss: 0.81\n",
      "train epoch: 1965, loss 0.0882\n",
      "test epoch: 1965, loss: 0.81\n",
      "train epoch: 1966, loss 0.0883\n",
      "test epoch: 1966, loss: 0.81\n",
      "train epoch: 1967, loss 0.0882\n",
      "test epoch: 1967, loss: 0.81\n",
      "train epoch: 1968, loss 0.0881\n",
      "test epoch: 1968, loss: 0.81\n",
      "train epoch: 1969, loss 0.0882\n",
      "test epoch: 1969, loss: 0.81\n",
      "train epoch: 1970, loss 0.0882\n",
      "test epoch: 1970, loss: 0.81\n",
      "train epoch: 1971, loss 0.0881\n",
      "test epoch: 1971, loss: 0.81\n",
      "train epoch: 1972, loss 0.0882\n",
      "test epoch: 1972, loss: 0.81\n",
      "train epoch: 1973, loss 0.0881\n",
      "test epoch: 1973, loss: 0.81\n",
      "train epoch: 1974, loss 0.0881\n",
      "test epoch: 1974, loss: 0.81\n",
      "train epoch: 1975, loss 0.0882\n",
      "test epoch: 1975, loss: 0.81\n",
      "train epoch: 1976, loss 0.0881\n",
      "test epoch: 1976, loss: 0.81\n",
      "train epoch: 1977, loss 0.0880\n",
      "test epoch: 1977, loss: 0.81\n",
      "train epoch: 1978, loss 0.0881\n",
      "test epoch: 1978, loss: 0.81\n",
      "train epoch: 1979, loss 0.0881\n",
      "test epoch: 1979, loss: 0.81\n",
      "train epoch: 1980, loss 0.0880\n",
      "test epoch: 1980, loss: 0.81\n",
      "train epoch: 1981, loss 0.0880\n",
      "test epoch: 1981, loss: 0.81\n",
      "train epoch: 1982, loss 0.0880\n",
      "test epoch: 1982, loss: 0.81\n",
      "train epoch: 1983, loss 0.0880\n",
      "test epoch: 1983, loss: 0.81\n",
      "train epoch: 1984, loss 0.0881\n",
      "test epoch: 1984, loss: 0.81\n",
      "train epoch: 1985, loss 0.0880\n",
      "test epoch: 1985, loss: 0.81\n",
      "train epoch: 1986, loss 0.0879\n",
      "test epoch: 1986, loss: 0.81\n",
      "train epoch: 1987, loss 0.0883\n",
      "test epoch: 1987, loss: 0.81\n",
      "train epoch: 1988, loss 0.0887\n",
      "test epoch: 1988, loss: 0.81\n",
      "train epoch: 1989, loss 0.0883\n",
      "test epoch: 1989, loss: 0.81\n",
      "train epoch: 1990, loss 0.0887\n",
      "test epoch: 1990, loss: 0.81\n",
      "train epoch: 1991, loss 0.0879\n",
      "test epoch: 1991, loss: 0.81\n",
      "train epoch: 1992, loss 0.0884\n",
      "test epoch: 1992, loss: 0.81\n",
      "train epoch: 1993, loss 0.0879\n",
      "test epoch: 1993, loss: 0.81\n",
      "train epoch: 1994, loss 0.0883\n",
      "test epoch: 1994, loss: 0.81\n",
      "train epoch: 1995, loss 0.0880\n",
      "test epoch: 1995, loss: 0.81\n",
      "train epoch: 1996, loss 0.0882\n",
      "test epoch: 1996, loss: 0.81\n",
      "train epoch: 1997, loss 0.0880\n",
      "test epoch: 1997, loss: 0.81\n",
      "train epoch: 1998, loss 0.0879\n",
      "test epoch: 1998, loss: 0.81\n",
      "train epoch: 1999, loss 0.0881\n",
      "test epoch: 1999, loss: 0.81\n",
      "train epoch: 2000, loss 0.0879\n",
      "test epoch: 2000, loss: 0.81\n",
      "train epoch: 2001, loss 0.0881\n",
      "test epoch: 2001, loss: 0.81\n",
      "train epoch: 2002, loss 0.0882\n",
      "test epoch: 2002, loss: 0.81\n",
      "train epoch: 2003, loss 0.0880\n",
      "test epoch: 2003, loss: 0.81\n",
      "train epoch: 2004, loss 0.0881\n",
      "test epoch: 2004, loss: 0.81\n",
      "train epoch: 2005, loss 0.0879\n",
      "test epoch: 2005, loss: 0.81\n",
      "train epoch: 2006, loss 0.0882\n",
      "test epoch: 2006, loss: 0.81\n",
      "train epoch: 2007, loss 0.0878\n",
      "test epoch: 2007, loss: 0.81\n",
      "train epoch: 2008, loss 0.0881\n",
      "test epoch: 2008, loss: 0.81\n",
      "train epoch: 2009, loss 0.0881\n",
      "test epoch: 2009, loss: 0.81\n",
      "train epoch: 2010, loss 0.0880\n",
      "test epoch: 2010, loss: 0.81\n",
      "train epoch: 2011, loss 0.0881\n",
      "test epoch: 2011, loss: 0.81\n",
      "train epoch: 2012, loss 0.0877\n",
      "test epoch: 2012, loss: 0.81\n",
      "train epoch: 2013, loss 0.0880\n",
      "test epoch: 2013, loss: 0.81\n",
      "train epoch: 2014, loss 0.0877\n",
      "test epoch: 2014, loss: 0.81\n",
      "train epoch: 2015, loss 0.0879\n",
      "test epoch: 2015, loss: 0.81\n",
      "train epoch: 2016, loss 0.0880\n",
      "test epoch: 2016, loss: 0.81\n",
      "train epoch: 2017, loss 0.0879\n",
      "test epoch: 2017, loss: 0.81\n",
      "train epoch: 2018, loss 0.0879\n",
      "test epoch: 2018, loss: 0.81\n",
      "train epoch: 2019, loss 0.0876\n",
      "test epoch: 2019, loss: 0.81\n",
      "train epoch: 2020, loss 0.0879\n",
      "test epoch: 2020, loss: 0.81\n",
      "train epoch: 2021, loss 0.0877\n",
      "test epoch: 2021, loss: 0.81\n",
      "train epoch: 2022, loss 0.0878\n",
      "test epoch: 2022, loss: 0.81\n",
      "train epoch: 2023, loss 0.0880\n",
      "test epoch: 2023, loss: 0.81\n",
      "train epoch: 2024, loss 0.0879\n",
      "test epoch: 2024, loss: 0.81\n",
      "train epoch: 2025, loss 0.0878\n",
      "test epoch: 2025, loss: 0.81\n",
      "train epoch: 2026, loss 0.0877\n",
      "test epoch: 2026, loss: 0.81\n",
      "train epoch: 2027, loss 0.0879\n",
      "test epoch: 2027, loss: 0.81\n",
      "train epoch: 2028, loss 0.0876\n",
      "test epoch: 2028, loss: 0.81\n",
      "train epoch: 2029, loss 0.0878\n",
      "test epoch: 2029, loss: 0.81\n",
      "train epoch: 2030, loss 0.0882\n",
      "test epoch: 2030, loss: 0.81\n",
      "train epoch: 2031, loss 0.0879\n",
      "test epoch: 2031, loss: 0.81\n",
      "train epoch: 2032, loss 0.0880\n",
      "test epoch: 2032, loss: 0.81\n",
      "train epoch: 2033, loss 0.0876\n",
      "test epoch: 2033, loss: 0.81\n",
      "train epoch: 2034, loss 0.0879\n",
      "test epoch: 2034, loss: 0.81\n",
      "train epoch: 2035, loss 0.0875\n",
      "test epoch: 2035, loss: 0.81\n",
      "train epoch: 2036, loss 0.0878\n",
      "test epoch: 2036, loss: 0.81\n",
      "train epoch: 2037, loss 0.0878\n",
      "test epoch: 2037, loss: 0.81\n",
      "train epoch: 2038, loss 0.0878\n",
      "test epoch: 2038, loss: 0.81\n",
      "train epoch: 2039, loss 0.0875\n",
      "test epoch: 2039, loss: 0.81\n",
      "train epoch: 2040, loss 0.0875\n",
      "test epoch: 2040, loss: 0.81\n",
      "train epoch: 2041, loss 0.0876\n",
      "test epoch: 2041, loss: 0.81\n",
      "train epoch: 2042, loss 0.0874\n",
      "test epoch: 2042, loss: 0.81\n",
      "train epoch: 2043, loss 0.0874\n",
      "test epoch: 2043, loss: 0.81\n",
      "train epoch: 2044, loss 0.0877\n",
      "test epoch: 2044, loss: 0.81\n",
      "train epoch: 2045, loss 0.0875\n",
      "test epoch: 2045, loss: 0.81\n",
      "train epoch: 2046, loss 0.0875\n",
      "test epoch: 2046, loss: 0.81\n",
      "train epoch: 2047, loss 0.0874\n",
      "test epoch: 2047, loss: 0.81\n",
      "train epoch: 2048, loss 0.0876\n",
      "test epoch: 2048, loss: 0.82\n",
      "train epoch: 2049, loss 0.0873\n",
      "test epoch: 2049, loss: 0.82\n",
      "train epoch: 2050, loss 0.0876\n",
      "test epoch: 2050, loss: 0.82\n",
      "train epoch: 2051, loss 0.0878\n",
      "test epoch: 2051, loss: 0.82\n",
      "train epoch: 2052, loss 0.0877\n",
      "test epoch: 2052, loss: 0.82\n",
      "train epoch: 2053, loss 0.0875\n",
      "test epoch: 2053, loss: 0.82\n",
      "train epoch: 2054, loss 0.0875\n",
      "test epoch: 2054, loss: 0.82\n",
      "train epoch: 2055, loss 0.0876\n",
      "test epoch: 2055, loss: 0.82\n",
      "train epoch: 2056, loss 0.0873\n",
      "test epoch: 2056, loss: 0.82\n",
      "train epoch: 2057, loss 0.0879\n",
      "test epoch: 2057, loss: 0.82\n",
      "train epoch: 2058, loss 0.0879\n",
      "test epoch: 2058, loss: 0.82\n",
      "train epoch: 2059, loss 0.0877\n",
      "test epoch: 2059, loss: 0.82\n",
      "train epoch: 2060, loss 0.0875\n",
      "test epoch: 2060, loss: 0.82\n",
      "train epoch: 2061, loss 0.0874\n",
      "test epoch: 2061, loss: 0.82\n",
      "train epoch: 2062, loss 0.0877\n",
      "test epoch: 2062, loss: 0.82\n",
      "train epoch: 2063, loss 0.0872\n",
      "test epoch: 2063, loss: 0.82\n",
      "train epoch: 2064, loss 0.0872\n",
      "test epoch: 2064, loss: 0.82\n",
      "train epoch: 2065, loss 0.0872\n",
      "test epoch: 2065, loss: 0.82\n",
      "train epoch: 2066, loss 0.0873\n",
      "test epoch: 2066, loss: 0.82\n",
      "train epoch: 2067, loss 0.0873\n",
      "test epoch: 2067, loss: 0.82\n",
      "train epoch: 2068, loss 0.0874\n",
      "test epoch: 2068, loss: 0.82\n",
      "train epoch: 2069, loss 0.0874\n",
      "test epoch: 2069, loss: 0.82\n",
      "train epoch: 2070, loss 0.0872\n",
      "test epoch: 2070, loss: 0.82\n",
      "train epoch: 2071, loss 0.0871\n",
      "test epoch: 2071, loss: 0.82\n",
      "train epoch: 2072, loss 0.0872\n",
      "test epoch: 2072, loss: 0.82\n",
      "train epoch: 2073, loss 0.0875\n",
      "test epoch: 2073, loss: 0.82\n",
      "train epoch: 2074, loss 0.0871\n",
      "test epoch: 2074, loss: 0.82\n",
      "train epoch: 2075, loss 0.0876\n",
      "test epoch: 2075, loss: 0.82\n",
      "train epoch: 2076, loss 0.0883\n",
      "test epoch: 2076, loss: 0.82\n",
      "train epoch: 2077, loss 0.0874\n",
      "test epoch: 2077, loss: 0.82\n",
      "train epoch: 2078, loss 0.0876\n",
      "test epoch: 2078, loss: 0.82\n",
      "train epoch: 2079, loss 0.0872\n",
      "test epoch: 2079, loss: 0.82\n",
      "train epoch: 2080, loss 0.0874\n",
      "test epoch: 2080, loss: 0.82\n",
      "train epoch: 2081, loss 0.0870\n",
      "test epoch: 2081, loss: 0.82\n",
      "train epoch: 2082, loss 0.0880\n",
      "test epoch: 2082, loss: 0.82\n",
      "train epoch: 2083, loss 0.0878\n",
      "test epoch: 2083, loss: 0.82\n",
      "train epoch: 2084, loss 0.0880\n",
      "test epoch: 2084, loss: 0.82\n",
      "train epoch: 2085, loss 0.0883\n",
      "test epoch: 2085, loss: 0.82\n",
      "train epoch: 2086, loss 0.0871\n",
      "test epoch: 2086, loss: 0.82\n",
      "train epoch: 2087, loss 0.0877\n",
      "test epoch: 2087, loss: 0.82\n",
      "train epoch: 2088, loss 0.0871\n",
      "test epoch: 2088, loss: 0.82\n",
      "train epoch: 2089, loss 0.0876\n",
      "test epoch: 2089, loss: 0.82\n",
      "train epoch: 2090, loss 0.0871\n",
      "test epoch: 2090, loss: 0.82\n",
      "train epoch: 2091, loss 0.0875\n",
      "test epoch: 2091, loss: 0.82\n",
      "train epoch: 2092, loss 0.0870\n",
      "test epoch: 2092, loss: 0.82\n",
      "train epoch: 2093, loss 0.0873\n",
      "test epoch: 2093, loss: 0.82\n",
      "train epoch: 2094, loss 0.0874\n",
      "test epoch: 2094, loss: 0.82\n",
      "train epoch: 2095, loss 0.0877\n",
      "test epoch: 2095, loss: 0.82\n",
      "train epoch: 2096, loss 0.0876\n",
      "test epoch: 2096, loss: 0.82\n",
      "train epoch: 2097, loss 0.0869\n",
      "test epoch: 2097, loss: 0.82\n",
      "train epoch: 2098, loss 0.0872\n",
      "test epoch: 2098, loss: 0.82\n",
      "train epoch: 2099, loss 0.0871\n",
      "test epoch: 2099, loss: 0.82\n",
      "train epoch: 2100, loss 0.0871\n",
      "test epoch: 2100, loss: 0.82\n",
      "train epoch: 2101, loss 0.0871\n",
      "test epoch: 2101, loss: 0.82\n",
      "train epoch: 2102, loss 0.0873\n",
      "test epoch: 2102, loss: 0.82\n",
      "train epoch: 2103, loss 0.0870\n",
      "test epoch: 2103, loss: 0.82\n",
      "train epoch: 2104, loss 0.0870\n",
      "test epoch: 2104, loss: 0.82\n",
      "train epoch: 2105, loss 0.0873\n",
      "test epoch: 2105, loss: 0.82\n",
      "train epoch: 2106, loss 0.0872\n",
      "test epoch: 2106, loss: 0.82\n",
      "train epoch: 2107, loss 0.0869\n",
      "test epoch: 2107, loss: 0.82\n",
      "train epoch: 2108, loss 0.0876\n",
      "test epoch: 2108, loss: 0.82\n",
      "train epoch: 2109, loss 0.0879\n",
      "test epoch: 2109, loss: 0.82\n",
      "train epoch: 2110, loss 0.0872\n",
      "test epoch: 2110, loss: 0.82\n",
      "train epoch: 2111, loss 0.0875\n",
      "test epoch: 2111, loss: 0.82\n",
      "train epoch: 2112, loss 0.0869\n",
      "test epoch: 2112, loss: 0.82\n",
      "train epoch: 2113, loss 0.0872\n",
      "test epoch: 2113, loss: 0.82\n",
      "train epoch: 2114, loss 0.0869\n",
      "test epoch: 2114, loss: 0.82\n",
      "train epoch: 2115, loss 0.0872\n",
      "test epoch: 2115, loss: 0.82\n",
      "train epoch: 2116, loss 0.0870\n",
      "test epoch: 2116, loss: 0.82\n",
      "train epoch: 2117, loss 0.0873\n",
      "test epoch: 2117, loss: 0.82\n",
      "train epoch: 2118, loss 0.0871\n",
      "test epoch: 2118, loss: 0.82\n",
      "train epoch: 2119, loss 0.0868\n",
      "test epoch: 2119, loss: 0.82\n",
      "train epoch: 2120, loss 0.0870\n",
      "test epoch: 2120, loss: 0.82\n",
      "train epoch: 2121, loss 0.0868\n",
      "test epoch: 2121, loss: 0.82\n",
      "train epoch: 2122, loss 0.0870\n",
      "test epoch: 2122, loss: 0.82\n",
      "train epoch: 2123, loss 0.0871\n",
      "test epoch: 2123, loss: 0.82\n",
      "train epoch: 2124, loss 0.0872\n",
      "test epoch: 2124, loss: 0.82\n",
      "train epoch: 2125, loss 0.0868\n",
      "test epoch: 2125, loss: 0.82\n",
      "train epoch: 2126, loss 0.0870\n",
      "test epoch: 2126, loss: 0.82\n",
      "train epoch: 2127, loss 0.0875\n",
      "test epoch: 2127, loss: 0.82\n",
      "train epoch: 2128, loss 0.0868\n",
      "test epoch: 2128, loss: 0.82\n",
      "train epoch: 2129, loss 0.0876\n",
      "test epoch: 2129, loss: 0.82\n",
      "train epoch: 2130, loss 0.0870\n",
      "test epoch: 2130, loss: 0.82\n",
      "train epoch: 2131, loss 0.0873\n",
      "test epoch: 2131, loss: 0.82\n",
      "train epoch: 2132, loss 0.0868\n",
      "test epoch: 2132, loss: 0.82\n",
      "train epoch: 2133, loss 0.0872\n",
      "test epoch: 2133, loss: 0.82\n",
      "train epoch: 2134, loss 0.0877\n",
      "test epoch: 2134, loss: 0.82\n",
      "train epoch: 2135, loss 0.0870\n",
      "test epoch: 2135, loss: 0.82\n",
      "train epoch: 2136, loss 0.0871\n",
      "test epoch: 2136, loss: 0.82\n",
      "train epoch: 2137, loss 0.0867\n",
      "test epoch: 2137, loss: 0.82\n",
      "train epoch: 2138, loss 0.0869\n",
      "test epoch: 2138, loss: 0.82\n",
      "train epoch: 2139, loss 0.0866\n",
      "test epoch: 2139, loss: 0.82\n",
      "train epoch: 2140, loss 0.0869\n",
      "test epoch: 2140, loss: 0.82\n",
      "train epoch: 2141, loss 0.0870\n",
      "test epoch: 2141, loss: 0.82\n",
      "train epoch: 2142, loss 0.0873\n",
      "test epoch: 2142, loss: 0.82\n",
      "train epoch: 2143, loss 0.0870\n",
      "test epoch: 2143, loss: 0.82\n",
      "train epoch: 2144, loss 0.0866\n",
      "test epoch: 2144, loss: 0.82\n",
      "train epoch: 2145, loss 0.0869\n",
      "test epoch: 2145, loss: 0.82\n",
      "train epoch: 2146, loss 0.0867\n",
      "test epoch: 2146, loss: 0.82\n",
      "train epoch: 2147, loss 0.0868\n",
      "test epoch: 2147, loss: 0.82\n",
      "train epoch: 2148, loss 0.0867\n",
      "test epoch: 2148, loss: 0.82\n",
      "train epoch: 2149, loss 0.0869\n",
      "test epoch: 2149, loss: 0.82\n",
      "train epoch: 2150, loss 0.0866\n",
      "test epoch: 2150, loss: 0.82\n",
      "train epoch: 2151, loss 0.0867\n",
      "test epoch: 2151, loss: 0.82\n",
      "train epoch: 2152, loss 0.0865\n",
      "test epoch: 2152, loss: 0.82\n",
      "train epoch: 2153, loss 0.0866\n",
      "test epoch: 2153, loss: 0.82\n",
      "train epoch: 2154, loss 0.0866\n",
      "test epoch: 2154, loss: 0.82\n",
      "train epoch: 2155, loss 0.0869\n",
      "test epoch: 2155, loss: 0.82\n",
      "train epoch: 2156, loss 0.0868\n",
      "test epoch: 2156, loss: 0.82\n",
      "train epoch: 2157, loss 0.0870\n",
      "test epoch: 2157, loss: 0.82\n",
      "train epoch: 2158, loss 0.0871\n",
      "test epoch: 2158, loss: 0.82\n",
      "train epoch: 2159, loss 0.0868\n",
      "test epoch: 2159, loss: 0.82\n",
      "train epoch: 2160, loss 0.0886\n",
      "test epoch: 2160, loss: 0.82\n",
      "train epoch: 2161, loss 0.0867\n",
      "test epoch: 2161, loss: 0.82\n",
      "train epoch: 2162, loss 0.0886\n",
      "test epoch: 2162, loss: 0.82\n",
      "train epoch: 2163, loss 0.0866\n",
      "test epoch: 2163, loss: 0.82\n",
      "train epoch: 2164, loss 0.0882\n",
      "test epoch: 2164, loss: 0.83\n",
      "train epoch: 2165, loss 0.0867\n",
      "test epoch: 2165, loss: 0.83\n",
      "train epoch: 2166, loss 0.0889\n",
      "test epoch: 2166, loss: 0.83\n",
      "train epoch: 2167, loss 0.0873\n",
      "test epoch: 2167, loss: 0.83\n",
      "train epoch: 2168, loss 0.0889\n",
      "test epoch: 2168, loss: 0.83\n",
      "train epoch: 2169, loss 0.0870\n",
      "test epoch: 2169, loss: 0.83\n",
      "train epoch: 2170, loss 0.0869\n",
      "test epoch: 2170, loss: 0.83\n",
      "train epoch: 2171, loss 0.0867\n",
      "test epoch: 2171, loss: 0.83\n",
      "train epoch: 2172, loss 0.0872\n",
      "test epoch: 2172, loss: 0.83\n",
      "train epoch: 2173, loss 0.0867\n",
      "test epoch: 2173, loss: 0.83\n",
      "train epoch: 2174, loss 0.0863\n",
      "test epoch: 2174, loss: 0.83\n",
      "train epoch: 2175, loss 0.0865\n",
      "test epoch: 2175, loss: 0.83\n",
      "train epoch: 2176, loss 0.0865\n",
      "test epoch: 2176, loss: 0.83\n",
      "train epoch: 2177, loss 0.0864\n",
      "test epoch: 2177, loss: 0.83\n",
      "train epoch: 2178, loss 0.0864\n",
      "test epoch: 2178, loss: 0.83\n",
      "train epoch: 2179, loss 0.0863\n",
      "test epoch: 2179, loss: 0.83\n",
      "train epoch: 2180, loss 0.0863\n",
      "test epoch: 2180, loss: 0.83\n",
      "train epoch: 2181, loss 0.0864\n",
      "test epoch: 2181, loss: 0.83\n",
      "train epoch: 2182, loss 0.0863\n",
      "test epoch: 2182, loss: 0.83\n",
      "train epoch: 2183, loss 0.0862\n",
      "test epoch: 2183, loss: 0.83\n",
      "train epoch: 2184, loss 0.0869\n",
      "test epoch: 2184, loss: 0.83\n",
      "train epoch: 2185, loss 0.0864\n",
      "test epoch: 2185, loss: 0.83\n",
      "train epoch: 2186, loss 0.0865\n",
      "test epoch: 2186, loss: 0.83\n",
      "train epoch: 2187, loss 0.0863\n",
      "test epoch: 2187, loss: 0.83\n",
      "train epoch: 2188, loss 0.0874\n",
      "test epoch: 2188, loss: 0.83\n",
      "train epoch: 2189, loss 0.0862\n",
      "test epoch: 2189, loss: 0.83\n",
      "train epoch: 2190, loss 0.0876\n",
      "test epoch: 2190, loss: 0.83\n",
      "train epoch: 2191, loss 0.0865\n",
      "test epoch: 2191, loss: 0.83\n",
      "train epoch: 2192, loss 0.0877\n",
      "test epoch: 2192, loss: 0.83\n",
      "train epoch: 2193, loss 0.0863\n",
      "test epoch: 2193, loss: 0.83\n",
      "train epoch: 2194, loss 0.0877\n",
      "test epoch: 2194, loss: 0.83\n",
      "train epoch: 2195, loss 0.0875\n",
      "test epoch: 2195, loss: 0.83\n",
      "train epoch: 2196, loss 0.0867\n",
      "test epoch: 2196, loss: 0.83\n",
      "train epoch: 2197, loss 0.0869\n",
      "test epoch: 2197, loss: 0.83\n",
      "train epoch: 2198, loss 0.0862\n",
      "test epoch: 2198, loss: 0.83\n",
      "train epoch: 2199, loss 0.0870\n",
      "test epoch: 2199, loss: 0.83\n",
      "train epoch: 2200, loss 0.0862\n",
      "test epoch: 2200, loss: 0.83\n",
      "train epoch: 2201, loss 0.0867\n",
      "test epoch: 2201, loss: 0.83\n",
      "train epoch: 2202, loss 0.0864\n",
      "test epoch: 2202, loss: 0.83\n",
      "train epoch: 2203, loss 0.0868\n",
      "test epoch: 2203, loss: 0.83\n",
      "train epoch: 2204, loss 0.0861\n",
      "test epoch: 2204, loss: 0.83\n",
      "train epoch: 2205, loss 0.0870\n",
      "test epoch: 2205, loss: 0.83\n",
      "train epoch: 2206, loss 0.0867\n",
      "test epoch: 2206, loss: 0.83\n",
      "train epoch: 2207, loss 0.0866\n",
      "test epoch: 2207, loss: 0.83\n",
      "train epoch: 2208, loss 0.0867\n",
      "test epoch: 2208, loss: 0.83\n",
      "train epoch: 2209, loss 0.0861\n",
      "test epoch: 2209, loss: 0.83\n",
      "train epoch: 2210, loss 0.0864\n",
      "test epoch: 2210, loss: 0.83\n",
      "train epoch: 2211, loss 0.0862\n",
      "test epoch: 2211, loss: 0.83\n",
      "train epoch: 2212, loss 0.0861\n",
      "test epoch: 2212, loss: 0.83\n",
      "train epoch: 2213, loss 0.0863\n",
      "test epoch: 2213, loss: 0.83\n",
      "train epoch: 2214, loss 0.0868\n",
      "test epoch: 2214, loss: 0.83\n",
      "train epoch: 2215, loss 0.0863\n",
      "test epoch: 2215, loss: 0.83\n",
      "train epoch: 2216, loss 0.0861\n",
      "test epoch: 2216, loss: 0.83\n",
      "train epoch: 2217, loss 0.0864\n",
      "test epoch: 2217, loss: 0.83\n",
      "train epoch: 2218, loss 0.0868\n",
      "test epoch: 2218, loss: 0.83\n",
      "train epoch: 2219, loss 0.0866\n",
      "test epoch: 2219, loss: 0.83\n",
      "train epoch: 2220, loss 0.0861\n",
      "test epoch: 2220, loss: 0.83\n",
      "train epoch: 2221, loss 0.0873\n",
      "test epoch: 2221, loss: 0.83\n",
      "train epoch: 2222, loss 0.0876\n",
      "test epoch: 2222, loss: 0.83\n",
      "train epoch: 2223, loss 0.0869\n",
      "test epoch: 2223, loss: 0.83\n",
      "train epoch: 2224, loss 0.0874\n",
      "test epoch: 2224, loss: 0.83\n",
      "train epoch: 2225, loss 0.0860\n",
      "test epoch: 2225, loss: 0.83\n",
      "train epoch: 2226, loss 0.0867\n",
      "test epoch: 2226, loss: 0.83\n",
      "train epoch: 2227, loss 0.0861\n",
      "test epoch: 2227, loss: 0.83\n",
      "train epoch: 2228, loss 0.0863\n",
      "test epoch: 2228, loss: 0.83\n",
      "train epoch: 2229, loss 0.0861\n",
      "test epoch: 2229, loss: 0.83\n",
      "train epoch: 2230, loss 0.0868\n",
      "test epoch: 2230, loss: 0.83\n",
      "train epoch: 2231, loss 0.0860\n",
      "test epoch: 2231, loss: 0.83\n",
      "train epoch: 2232, loss 0.0866\n",
      "test epoch: 2232, loss: 0.83\n",
      "train epoch: 2233, loss 0.0867\n",
      "test epoch: 2233, loss: 0.83\n",
      "train epoch: 2234, loss 0.0869\n",
      "test epoch: 2234, loss: 0.83\n",
      "train epoch: 2235, loss 0.0867\n",
      "test epoch: 2235, loss: 0.83\n",
      "train epoch: 2236, loss 0.0860\n",
      "test epoch: 2236, loss: 0.83\n",
      "train epoch: 2237, loss 0.0864\n",
      "test epoch: 2237, loss: 0.83\n",
      "train epoch: 2238, loss 0.0859\n",
      "test epoch: 2238, loss: 0.83\n",
      "train epoch: 2239, loss 0.0861\n",
      "test epoch: 2239, loss: 0.83\n",
      "train epoch: 2240, loss 0.0860\n",
      "test epoch: 2240, loss: 0.83\n",
      "train epoch: 2241, loss 0.0862\n",
      "test epoch: 2241, loss: 0.83\n",
      "train epoch: 2242, loss 0.0859\n",
      "test epoch: 2242, loss: 0.83\n",
      "train epoch: 2243, loss 0.0861\n",
      "test epoch: 2243, loss: 0.83\n",
      "train epoch: 2244, loss 0.0858\n",
      "test epoch: 2244, loss: 0.83\n",
      "train epoch: 2245, loss 0.0860\n",
      "test epoch: 2245, loss: 0.83\n",
      "train epoch: 2246, loss 0.0858\n",
      "test epoch: 2246, loss: 0.83\n",
      "train epoch: 2247, loss 0.0860\n",
      "test epoch: 2247, loss: 0.83\n",
      "train epoch: 2248, loss 0.0860\n",
      "test epoch: 2248, loss: 0.83\n",
      "train epoch: 2249, loss 0.0864\n",
      "test epoch: 2249, loss: 0.83\n",
      "train epoch: 2250, loss 0.0860\n",
      "test epoch: 2250, loss: 0.83\n",
      "train epoch: 2251, loss 0.0862\n",
      "test epoch: 2251, loss: 0.83\n",
      "train epoch: 2252, loss 0.0871\n",
      "test epoch: 2252, loss: 0.83\n",
      "train epoch: 2253, loss 0.0860\n",
      "test epoch: 2253, loss: 0.83\n",
      "train epoch: 2254, loss 0.0868\n",
      "test epoch: 2254, loss: 0.83\n",
      "train epoch: 2255, loss 0.0859\n",
      "test epoch: 2255, loss: 0.83\n",
      "train epoch: 2256, loss 0.0868\n",
      "test epoch: 2256, loss: 0.83\n",
      "train epoch: 2257, loss 0.0858\n",
      "test epoch: 2257, loss: 0.83\n",
      "train epoch: 2258, loss 0.0868\n",
      "test epoch: 2258, loss: 0.83\n",
      "train epoch: 2259, loss 0.0867\n",
      "test epoch: 2259, loss: 0.83\n",
      "train epoch: 2260, loss 0.0864\n",
      "test epoch: 2260, loss: 0.83\n",
      "train epoch: 2261, loss 0.0860\n",
      "test epoch: 2261, loss: 0.83\n",
      "train epoch: 2262, loss 0.0857\n",
      "test epoch: 2262, loss: 0.83\n",
      "train epoch: 2263, loss 0.0860\n",
      "test epoch: 2263, loss: 0.83\n",
      "train epoch: 2264, loss 0.0857\n",
      "test epoch: 2264, loss: 0.83\n",
      "train epoch: 2265, loss 0.0859\n",
      "test epoch: 2265, loss: 0.83\n",
      "train epoch: 2266, loss 0.0861\n",
      "test epoch: 2266, loss: 0.83\n",
      "train epoch: 2267, loss 0.0862\n",
      "test epoch: 2267, loss: 0.83\n",
      "train epoch: 2268, loss 0.0860\n",
      "test epoch: 2268, loss: 0.83\n",
      "train epoch: 2269, loss 0.0862\n",
      "test epoch: 2269, loss: 0.83\n",
      "train epoch: 2270, loss 0.0873\n",
      "test epoch: 2270, loss: 0.83\n",
      "train epoch: 2271, loss 0.0861\n",
      "test epoch: 2271, loss: 0.83\n",
      "train epoch: 2272, loss 0.0863\n",
      "test epoch: 2272, loss: 0.83\n",
      "train epoch: 2273, loss 0.0860\n",
      "test epoch: 2273, loss: 0.83\n",
      "train epoch: 2274, loss 0.0862\n",
      "test epoch: 2274, loss: 0.83\n",
      "train epoch: 2275, loss 0.0864\n",
      "test epoch: 2275, loss: 0.83\n",
      "train epoch: 2276, loss 0.0857\n",
      "test epoch: 2276, loss: 0.83\n",
      "train epoch: 2277, loss 0.0869\n",
      "test epoch: 2277, loss: 0.83\n",
      "train epoch: 2278, loss 0.0870\n",
      "test epoch: 2278, loss: 0.83\n",
      "train epoch: 2279, loss 0.0870\n",
      "test epoch: 2279, loss: 0.83\n",
      "train epoch: 2280, loss 0.0862\n",
      "test epoch: 2280, loss: 0.83\n",
      "train epoch: 2281, loss 0.0871\n",
      "test epoch: 2281, loss: 0.83\n",
      "train epoch: 2282, loss 0.0876\n",
      "test epoch: 2282, loss: 0.83\n",
      "train epoch: 2283, loss 0.0867\n",
      "test epoch: 2283, loss: 0.83\n",
      "train epoch: 2284, loss 0.0878\n",
      "test epoch: 2284, loss: 0.83\n",
      "train epoch: 2285, loss 0.0858\n",
      "test epoch: 2285, loss: 0.83\n",
      "train epoch: 2286, loss 0.0868\n",
      "test epoch: 2286, loss: 0.83\n",
      "train epoch: 2287, loss 0.0859\n",
      "test epoch: 2287, loss: 0.83\n",
      "train epoch: 2288, loss 0.0859\n",
      "test epoch: 2288, loss: 0.83\n",
      "train epoch: 2289, loss 0.0857\n",
      "test epoch: 2289, loss: 0.83\n",
      "train epoch: 2290, loss 0.0863\n",
      "test epoch: 2290, loss: 0.83\n",
      "train epoch: 2291, loss 0.0855\n",
      "test epoch: 2291, loss: 0.83\n",
      "train epoch: 2292, loss 0.0865\n",
      "test epoch: 2292, loss: 0.83\n",
      "train epoch: 2293, loss 0.0863\n",
      "test epoch: 2293, loss: 0.83\n",
      "train epoch: 2294, loss 0.0867\n",
      "test epoch: 2294, loss: 0.83\n",
      "train epoch: 2295, loss 0.0860\n",
      "test epoch: 2295, loss: 0.83\n",
      "train epoch: 2296, loss 0.0869\n",
      "test epoch: 2296, loss: 0.83\n",
      "train epoch: 2297, loss 0.0871\n",
      "test epoch: 2297, loss: 0.83\n",
      "train epoch: 2298, loss 0.0870\n",
      "test epoch: 2298, loss: 0.84\n",
      "train epoch: 2299, loss 0.0867\n",
      "test epoch: 2299, loss: 0.84\n",
      "train epoch: 2300, loss 0.0856\n",
      "test epoch: 2300, loss: 0.84\n",
      "train epoch: 2301, loss 0.0859\n",
      "test epoch: 2301, loss: 0.84\n",
      "train epoch: 2302, loss 0.0860\n",
      "test epoch: 2302, loss: 0.84\n",
      "train epoch: 2303, loss 0.0857\n",
      "test epoch: 2303, loss: 0.84\n",
      "train epoch: 2304, loss 0.0865\n",
      "test epoch: 2304, loss: 0.84\n",
      "train epoch: 2305, loss 0.0869\n",
      "test epoch: 2305, loss: 0.84\n",
      "train epoch: 2306, loss 0.0864\n",
      "test epoch: 2306, loss: 0.84\n",
      "train epoch: 2307, loss 0.0867\n",
      "test epoch: 2307, loss: 0.84\n",
      "train epoch: 2308, loss 0.0855\n",
      "test epoch: 2308, loss: 0.84\n",
      "train epoch: 2309, loss 0.0862\n",
      "test epoch: 2309, loss: 0.84\n",
      "train epoch: 2310, loss 0.0855\n",
      "test epoch: 2310, loss: 0.84\n",
      "train epoch: 2311, loss 0.0856\n",
      "test epoch: 2311, loss: 0.84\n",
      "train epoch: 2312, loss 0.0858\n",
      "test epoch: 2312, loss: 0.84\n",
      "train epoch: 2313, loss 0.0864\n",
      "test epoch: 2313, loss: 0.84\n",
      "train epoch: 2314, loss 0.0857\n",
      "test epoch: 2314, loss: 0.84\n",
      "train epoch: 2315, loss 0.0858\n",
      "test epoch: 2315, loss: 0.84\n",
      "train epoch: 2316, loss 0.0858\n",
      "test epoch: 2316, loss: 0.84\n",
      "train epoch: 2317, loss 0.0857\n",
      "test epoch: 2317, loss: 0.84\n",
      "train epoch: 2318, loss 0.0860\n",
      "test epoch: 2318, loss: 0.84\n",
      "train epoch: 2319, loss 0.0854\n",
      "test epoch: 2319, loss: 0.84\n",
      "train epoch: 2320, loss 0.0861\n",
      "test epoch: 2320, loss: 0.84\n",
      "train epoch: 2321, loss 0.0865\n",
      "test epoch: 2321, loss: 0.84\n",
      "train epoch: 2322, loss 0.0864\n",
      "test epoch: 2322, loss: 0.84\n",
      "train epoch: 2323, loss 0.0858\n",
      "test epoch: 2323, loss: 0.84\n",
      "train epoch: 2324, loss 0.0860\n",
      "test epoch: 2324, loss: 0.84\n",
      "train epoch: 2325, loss 0.0874\n",
      "test epoch: 2325, loss: 0.84\n",
      "train epoch: 2326, loss 0.0859\n",
      "test epoch: 2326, loss: 0.84\n",
      "train epoch: 2327, loss 0.0865\n",
      "test epoch: 2327, loss: 0.84\n",
      "train epoch: 2328, loss 0.0856\n",
      "test epoch: 2328, loss: 0.84\n",
      "train epoch: 2329, loss 0.0870\n",
      "test epoch: 2329, loss: 0.84\n",
      "train epoch: 2330, loss 0.0855\n",
      "test epoch: 2330, loss: 0.84\n",
      "train epoch: 2331, loss 0.0858\n",
      "test epoch: 2331, loss: 0.84\n",
      "train epoch: 2332, loss 0.0855\n",
      "test epoch: 2332, loss: 0.84\n",
      "train epoch: 2333, loss 0.0860\n",
      "test epoch: 2333, loss: 0.84\n",
      "train epoch: 2334, loss 0.0853\n",
      "test epoch: 2334, loss: 0.84\n",
      "train epoch: 2335, loss 0.0858\n",
      "test epoch: 2335, loss: 0.84\n",
      "train epoch: 2336, loss 0.0853\n",
      "test epoch: 2336, loss: 0.84\n",
      "train epoch: 2337, loss 0.0858\n",
      "test epoch: 2337, loss: 0.84\n",
      "train epoch: 2338, loss 0.0854\n",
      "test epoch: 2338, loss: 0.84\n",
      "train epoch: 2339, loss 0.0855\n",
      "test epoch: 2339, loss: 0.84\n",
      "train epoch: 2340, loss 0.0854\n",
      "test epoch: 2340, loss: 0.84\n",
      "train epoch: 2341, loss 0.0854\n",
      "test epoch: 2341, loss: 0.84\n",
      "train epoch: 2342, loss 0.0855\n",
      "test epoch: 2342, loss: 0.84\n",
      "train epoch: 2343, loss 0.0852\n",
      "test epoch: 2343, loss: 0.84\n",
      "train epoch: 2344, loss 0.0855\n",
      "test epoch: 2344, loss: 0.84\n",
      "train epoch: 2345, loss 0.0853\n",
      "test epoch: 2345, loss: 0.84\n",
      "train epoch: 2346, loss 0.0854\n",
      "test epoch: 2346, loss: 0.84\n",
      "train epoch: 2347, loss 0.0853\n",
      "test epoch: 2347, loss: 0.84\n",
      "train epoch: 2348, loss 0.0853\n",
      "test epoch: 2348, loss: 0.84\n",
      "train epoch: 2349, loss 0.0852\n",
      "test epoch: 2349, loss: 0.84\n",
      "train epoch: 2350, loss 0.0854\n",
      "test epoch: 2350, loss: 0.84\n",
      "train epoch: 2351, loss 0.0853\n",
      "test epoch: 2351, loss: 0.84\n",
      "train epoch: 2352, loss 0.0853\n",
      "test epoch: 2352, loss: 0.84\n",
      "train epoch: 2353, loss 0.0853\n",
      "test epoch: 2353, loss: 0.84\n",
      "train epoch: 2354, loss 0.0853\n",
      "test epoch: 2354, loss: 0.84\n",
      "train epoch: 2355, loss 0.0853\n",
      "test epoch: 2355, loss: 0.84\n",
      "train epoch: 2356, loss 0.0853\n",
      "test epoch: 2356, loss: 0.84\n",
      "train epoch: 2357, loss 0.0853\n",
      "test epoch: 2357, loss: 0.84\n",
      "train epoch: 2358, loss 0.0852\n",
      "test epoch: 2358, loss: 0.84\n",
      "train epoch: 2359, loss 0.0852\n",
      "test epoch: 2359, loss: 0.84\n",
      "train epoch: 2360, loss 0.0852\n",
      "test epoch: 2360, loss: 0.84\n",
      "train epoch: 2361, loss 0.0853\n",
      "test epoch: 2361, loss: 0.84\n",
      "train epoch: 2362, loss 0.0853\n",
      "test epoch: 2362, loss: 0.84\n",
      "train epoch: 2363, loss 0.0853\n",
      "test epoch: 2363, loss: 0.84\n",
      "train epoch: 2364, loss 0.0852\n",
      "test epoch: 2364, loss: 0.84\n",
      "train epoch: 2365, loss 0.0853\n",
      "test epoch: 2365, loss: 0.84\n",
      "train epoch: 2366, loss 0.0852\n",
      "test epoch: 2366, loss: 0.84\n",
      "train epoch: 2367, loss 0.0852\n",
      "test epoch: 2367, loss: 0.84\n",
      "train epoch: 2368, loss 0.0852\n",
      "test epoch: 2368, loss: 0.84\n",
      "train epoch: 2369, loss 0.0851\n",
      "test epoch: 2369, loss: 0.84\n",
      "train epoch: 2370, loss 0.0852\n",
      "test epoch: 2370, loss: 0.84\n",
      "train epoch: 2371, loss 0.0852\n",
      "test epoch: 2371, loss: 0.84\n",
      "train epoch: 2372, loss 0.0852\n",
      "test epoch: 2372, loss: 0.84\n",
      "train epoch: 2373, loss 0.0852\n",
      "test epoch: 2373, loss: 0.84\n",
      "train epoch: 2374, loss 0.0852\n",
      "test epoch: 2374, loss: 0.84\n",
      "train epoch: 2375, loss 0.0852\n",
      "test epoch: 2375, loss: 0.84\n",
      "train epoch: 2376, loss 0.0851\n",
      "test epoch: 2376, loss: 0.84\n",
      "train epoch: 2377, loss 0.0851\n",
      "test epoch: 2377, loss: 0.84\n",
      "train epoch: 2378, loss 0.0851\n",
      "test epoch: 2378, loss: 0.84\n",
      "train epoch: 2379, loss 0.0851\n",
      "test epoch: 2379, loss: 0.84\n",
      "train epoch: 2380, loss 0.0851\n",
      "test epoch: 2380, loss: 0.84\n",
      "train epoch: 2381, loss 0.0851\n",
      "test epoch: 2381, loss: 0.84\n",
      "train epoch: 2382, loss 0.0851\n",
      "test epoch: 2382, loss: 0.84\n",
      "train epoch: 2383, loss 0.0851\n",
      "test epoch: 2383, loss: 0.84\n",
      "train epoch: 2384, loss 0.0850\n",
      "test epoch: 2384, loss: 0.84\n",
      "train epoch: 2385, loss 0.0850\n",
      "test epoch: 2385, loss: 0.84\n",
      "train epoch: 2386, loss 0.0850\n",
      "test epoch: 2386, loss: 0.84\n",
      "train epoch: 2387, loss 0.0850\n",
      "test epoch: 2387, loss: 0.84\n",
      "train epoch: 2388, loss 0.0850\n",
      "test epoch: 2388, loss: 0.84\n",
      "train epoch: 2389, loss 0.0851\n",
      "test epoch: 2389, loss: 0.84\n",
      "train epoch: 2390, loss 0.0850\n",
      "test epoch: 2390, loss: 0.84\n",
      "train epoch: 2391, loss 0.0850\n",
      "test epoch: 2391, loss: 0.84\n",
      "train epoch: 2392, loss 0.0850\n",
      "test epoch: 2392, loss: 0.84\n",
      "train epoch: 2393, loss 0.0850\n",
      "test epoch: 2393, loss: 0.84\n",
      "train epoch: 2394, loss 0.0850\n",
      "test epoch: 2394, loss: 0.84\n",
      "train epoch: 2395, loss 0.0850\n",
      "test epoch: 2395, loss: 0.84\n",
      "train epoch: 2396, loss 0.0850\n",
      "test epoch: 2396, loss: 0.84\n",
      "train epoch: 2397, loss 0.0850\n",
      "test epoch: 2397, loss: 0.84\n",
      "train epoch: 2398, loss 0.0850\n",
      "test epoch: 2398, loss: 0.84\n",
      "train epoch: 2399, loss 0.0850\n",
      "test epoch: 2399, loss: 0.84\n",
      "train epoch: 2400, loss 0.0850\n",
      "test epoch: 2400, loss: 0.84\n",
      "train epoch: 2401, loss 0.0850\n",
      "test epoch: 2401, loss: 0.84\n",
      "train epoch: 2402, loss 0.0850\n",
      "test epoch: 2402, loss: 0.84\n",
      "train epoch: 2403, loss 0.0850\n",
      "test epoch: 2403, loss: 0.84\n",
      "train epoch: 2404, loss 0.0850\n",
      "test epoch: 2404, loss: 0.84\n",
      "train epoch: 2405, loss 0.0850\n",
      "test epoch: 2405, loss: 0.84\n",
      "train epoch: 2406, loss 0.0850\n",
      "test epoch: 2406, loss: 0.84\n",
      "train epoch: 2407, loss 0.0849\n",
      "test epoch: 2407, loss: 0.84\n",
      "train epoch: 2408, loss 0.0850\n",
      "test epoch: 2408, loss: 0.84\n",
      "train epoch: 2409, loss 0.0850\n",
      "test epoch: 2409, loss: 0.84\n",
      "train epoch: 2410, loss 0.0850\n",
      "test epoch: 2410, loss: 0.84\n",
      "train epoch: 2411, loss 0.0849\n",
      "test epoch: 2411, loss: 0.84\n",
      "train epoch: 2412, loss 0.0849\n",
      "test epoch: 2412, loss: 0.84\n",
      "train epoch: 2413, loss 0.0850\n",
      "test epoch: 2413, loss: 0.84\n",
      "train epoch: 2414, loss 0.0850\n",
      "test epoch: 2414, loss: 0.84\n",
      "train epoch: 2415, loss 0.0850\n",
      "test epoch: 2415, loss: 0.84\n",
      "train epoch: 2416, loss 0.0850\n",
      "test epoch: 2416, loss: 0.84\n",
      "train epoch: 2417, loss 0.0850\n",
      "test epoch: 2417, loss: 0.84\n",
      "train epoch: 2418, loss 0.0850\n",
      "test epoch: 2418, loss: 0.84\n",
      "train epoch: 2419, loss 0.0849\n",
      "test epoch: 2419, loss: 0.84\n",
      "train epoch: 2420, loss 0.0849\n",
      "test epoch: 2420, loss: 0.84\n",
      "train epoch: 2421, loss 0.0849\n",
      "test epoch: 2421, loss: 0.84\n",
      "train epoch: 2422, loss 0.0849\n",
      "test epoch: 2422, loss: 0.84\n",
      "train epoch: 2423, loss 0.0849\n",
      "test epoch: 2423, loss: 0.84\n",
      "train epoch: 2424, loss 0.0849\n",
      "test epoch: 2424, loss: 0.84\n",
      "train epoch: 2425, loss 0.0849\n",
      "test epoch: 2425, loss: 0.84\n",
      "train epoch: 2426, loss 0.0849\n",
      "test epoch: 2426, loss: 0.84\n",
      "train epoch: 2427, loss 0.0849\n",
      "test epoch: 2427, loss: 0.84\n",
      "train epoch: 2428, loss 0.0849\n",
      "test epoch: 2428, loss: 0.84\n",
      "train epoch: 2429, loss 0.0849\n",
      "test epoch: 2429, loss: 0.84\n",
      "train epoch: 2430, loss 0.0849\n",
      "test epoch: 2430, loss: 0.84\n",
      "train epoch: 2431, loss 0.0848\n",
      "test epoch: 2431, loss: 0.84\n",
      "train epoch: 2432, loss 0.0849\n",
      "test epoch: 2432, loss: 0.84\n",
      "train epoch: 2433, loss 0.0849\n",
      "test epoch: 2433, loss: 0.84\n",
      "train epoch: 2434, loss 0.0848\n",
      "test epoch: 2434, loss: 0.84\n",
      "train epoch: 2435, loss 0.0849\n",
      "test epoch: 2435, loss: 0.84\n",
      "train epoch: 2436, loss 0.0850\n",
      "test epoch: 2436, loss: 0.84\n",
      "train epoch: 2437, loss 0.0848\n",
      "test epoch: 2437, loss: 0.84\n",
      "train epoch: 2438, loss 0.0850\n",
      "test epoch: 2438, loss: 0.84\n",
      "train epoch: 2439, loss 0.0850\n",
      "test epoch: 2439, loss: 0.84\n",
      "train epoch: 2440, loss 0.0849\n",
      "test epoch: 2440, loss: 0.84\n",
      "train epoch: 2441, loss 0.0849\n",
      "test epoch: 2441, loss: 0.84\n",
      "train epoch: 2442, loss 0.0849\n",
      "test epoch: 2442, loss: 0.84\n",
      "train epoch: 2443, loss 0.0849\n",
      "test epoch: 2443, loss: 0.84\n",
      "train epoch: 2444, loss 0.0849\n",
      "test epoch: 2444, loss: 0.84\n",
      "train epoch: 2445, loss 0.0850\n",
      "test epoch: 2445, loss: 0.84\n",
      "train epoch: 2446, loss 0.0850\n",
      "test epoch: 2446, loss: 0.84\n",
      "train epoch: 2447, loss 0.0850\n",
      "test epoch: 2447, loss: 0.84\n",
      "train epoch: 2448, loss 0.0849\n",
      "test epoch: 2448, loss: 0.84\n",
      "train epoch: 2449, loss 0.0848\n",
      "test epoch: 2449, loss: 0.84\n",
      "train epoch: 2450, loss 0.0849\n",
      "test epoch: 2450, loss: 0.84\n",
      "train epoch: 2451, loss 0.0849\n",
      "test epoch: 2451, loss: 0.84\n",
      "train epoch: 2452, loss 0.0849\n",
      "test epoch: 2452, loss: 0.84\n",
      "train epoch: 2453, loss 0.0847\n",
      "test epoch: 2453, loss: 0.84\n",
      "train epoch: 2454, loss 0.0848\n",
      "test epoch: 2454, loss: 0.84\n",
      "train epoch: 2455, loss 0.0849\n",
      "test epoch: 2455, loss: 0.84\n",
      "train epoch: 2456, loss 0.0849\n",
      "test epoch: 2456, loss: 0.84\n",
      "train epoch: 2457, loss 0.0848\n",
      "test epoch: 2457, loss: 0.84\n",
      "train epoch: 2458, loss 0.0849\n",
      "test epoch: 2458, loss: 0.84\n",
      "train epoch: 2459, loss 0.0848\n",
      "test epoch: 2459, loss: 0.84\n",
      "train epoch: 2460, loss 0.0848\n",
      "test epoch: 2460, loss: 0.84\n",
      "train epoch: 2461, loss 0.0848\n",
      "test epoch: 2461, loss: 0.84\n",
      "train epoch: 2462, loss 0.0849\n",
      "test epoch: 2462, loss: 0.84\n",
      "train epoch: 2463, loss 0.0848\n",
      "test epoch: 2463, loss: 0.85\n",
      "train epoch: 2464, loss 0.0848\n",
      "test epoch: 2464, loss: 0.85\n",
      "train epoch: 2465, loss 0.0848\n",
      "test epoch: 2465, loss: 0.85\n",
      "train epoch: 2466, loss 0.0847\n",
      "test epoch: 2466, loss: 0.85\n",
      "train epoch: 2467, loss 0.0848\n",
      "test epoch: 2467, loss: 0.85\n",
      "train epoch: 2468, loss 0.0847\n",
      "test epoch: 2468, loss: 0.85\n",
      "train epoch: 2469, loss 0.0848\n",
      "test epoch: 2469, loss: 0.85\n",
      "train epoch: 2470, loss 0.0847\n",
      "test epoch: 2470, loss: 0.85\n",
      "train epoch: 2471, loss 0.0847\n",
      "test epoch: 2471, loss: 0.85\n",
      "train epoch: 2472, loss 0.0847\n",
      "test epoch: 2472, loss: 0.85\n",
      "train epoch: 2473, loss 0.0847\n",
      "test epoch: 2473, loss: 0.85\n",
      "train epoch: 2474, loss 0.0847\n",
      "test epoch: 2474, loss: 0.85\n",
      "train epoch: 2475, loss 0.0847\n",
      "test epoch: 2475, loss: 0.85\n",
      "train epoch: 2476, loss 0.0847\n",
      "test epoch: 2476, loss: 0.85\n",
      "train epoch: 2477, loss 0.0847\n",
      "test epoch: 2477, loss: 0.85\n",
      "train epoch: 2478, loss 0.0847\n",
      "test epoch: 2478, loss: 0.85\n",
      "train epoch: 2479, loss 0.0847\n",
      "test epoch: 2479, loss: 0.85\n",
      "train epoch: 2480, loss 0.0847\n",
      "test epoch: 2480, loss: 0.85\n",
      "train epoch: 2481, loss 0.0847\n",
      "test epoch: 2481, loss: 0.85\n",
      "train epoch: 2482, loss 0.0846\n",
      "test epoch: 2482, loss: 0.85\n",
      "train epoch: 2483, loss 0.0846\n",
      "test epoch: 2483, loss: 0.85\n",
      "train epoch: 2484, loss 0.0846\n",
      "test epoch: 2484, loss: 0.85\n",
      "train epoch: 2485, loss 0.0846\n",
      "test epoch: 2485, loss: 0.85\n",
      "train epoch: 2486, loss 0.0846\n",
      "test epoch: 2486, loss: 0.85\n",
      "train epoch: 2487, loss 0.0846\n",
      "test epoch: 2487, loss: 0.85\n",
      "train epoch: 2488, loss 0.0846\n",
      "test epoch: 2488, loss: 0.85\n",
      "train epoch: 2489, loss 0.0847\n",
      "test epoch: 2489, loss: 0.85\n",
      "train epoch: 2490, loss 0.0846\n",
      "test epoch: 2490, loss: 0.85\n",
      "train epoch: 2491, loss 0.0847\n",
      "test epoch: 2491, loss: 0.85\n",
      "train epoch: 2492, loss 0.0848\n",
      "test epoch: 2492, loss: 0.85\n",
      "train epoch: 2493, loss 0.0846\n",
      "test epoch: 2493, loss: 0.85\n",
      "train epoch: 2494, loss 0.0847\n",
      "test epoch: 2494, loss: 0.85\n",
      "train epoch: 2495, loss 0.0847\n",
      "test epoch: 2495, loss: 0.85\n",
      "train epoch: 2496, loss 0.0847\n",
      "test epoch: 2496, loss: 0.85\n",
      "train epoch: 2497, loss 0.0846\n",
      "test epoch: 2497, loss: 0.85\n",
      "train epoch: 2498, loss 0.0845\n",
      "test epoch: 2498, loss: 0.85\n",
      "train epoch: 2499, loss 0.0846\n",
      "test epoch: 2499, loss: 0.85\n",
      "train epoch: 2500, loss 0.0846\n",
      "test epoch: 2500, loss: 0.85\n",
      "train epoch: 2501, loss 0.0846\n",
      "test epoch: 2501, loss: 0.85\n",
      "train epoch: 2502, loss 0.0846\n",
      "test epoch: 2502, loss: 0.85\n",
      "train epoch: 2503, loss 0.0846\n",
      "test epoch: 2503, loss: 0.85\n",
      "train epoch: 2504, loss 0.0846\n",
      "test epoch: 2504, loss: 0.85\n",
      "train epoch: 2505, loss 0.0846\n",
      "test epoch: 2505, loss: 0.85\n",
      "train epoch: 2506, loss 0.0846\n",
      "test epoch: 2506, loss: 0.85\n",
      "train epoch: 2507, loss 0.0845\n",
      "test epoch: 2507, loss: 0.85\n",
      "train epoch: 2508, loss 0.0846\n",
      "test epoch: 2508, loss: 0.85\n",
      "train epoch: 2509, loss 0.0845\n",
      "test epoch: 2509, loss: 0.85\n",
      "train epoch: 2510, loss 0.0846\n",
      "test epoch: 2510, loss: 0.85\n",
      "train epoch: 2511, loss 0.0846\n",
      "test epoch: 2511, loss: 0.85\n",
      "train epoch: 2512, loss 0.0845\n",
      "test epoch: 2512, loss: 0.85\n",
      "train epoch: 2513, loss 0.0845\n",
      "test epoch: 2513, loss: 0.85\n",
      "train epoch: 2514, loss 0.0846\n",
      "test epoch: 2514, loss: 0.85\n",
      "train epoch: 2515, loss 0.0846\n",
      "test epoch: 2515, loss: 0.85\n",
      "train epoch: 2516, loss 0.0846\n",
      "test epoch: 2516, loss: 0.85\n",
      "train epoch: 2517, loss 0.0847\n",
      "test epoch: 2517, loss: 0.85\n",
      "train epoch: 2518, loss 0.0846\n",
      "test epoch: 2518, loss: 0.85\n",
      "train epoch: 2519, loss 0.0847\n",
      "test epoch: 2519, loss: 0.85\n",
      "train epoch: 2520, loss 0.0845\n",
      "test epoch: 2520, loss: 0.85\n",
      "train epoch: 2521, loss 0.0845\n",
      "test epoch: 2521, loss: 0.85\n",
      "train epoch: 2522, loss 0.0846\n",
      "test epoch: 2522, loss: 0.85\n",
      "train epoch: 2523, loss 0.0845\n",
      "test epoch: 2523, loss: 0.85\n",
      "train epoch: 2524, loss 0.0845\n",
      "test epoch: 2524, loss: 0.85\n",
      "train epoch: 2525, loss 0.0848\n",
      "test epoch: 2525, loss: 0.85\n",
      "train epoch: 2526, loss 0.0845\n",
      "test epoch: 2526, loss: 0.85\n",
      "train epoch: 2527, loss 0.0848\n",
      "test epoch: 2527, loss: 0.85\n",
      "train epoch: 2528, loss 0.0845\n",
      "test epoch: 2528, loss: 0.85\n",
      "train epoch: 2529, loss 0.0846\n",
      "test epoch: 2529, loss: 0.85\n",
      "train epoch: 2530, loss 0.0845\n",
      "test epoch: 2530, loss: 0.85\n",
      "train epoch: 2531, loss 0.0845\n",
      "test epoch: 2531, loss: 0.85\n",
      "train epoch: 2532, loss 0.0846\n",
      "test epoch: 2532, loss: 0.85\n",
      "train epoch: 2533, loss 0.0845\n",
      "test epoch: 2533, loss: 0.85\n",
      "train epoch: 2534, loss 0.0847\n",
      "test epoch: 2534, loss: 0.85\n",
      "train epoch: 2535, loss 0.0845\n",
      "test epoch: 2535, loss: 0.85\n",
      "train epoch: 2536, loss 0.0846\n",
      "test epoch: 2536, loss: 0.85\n",
      "train epoch: 2537, loss 0.0845\n",
      "test epoch: 2537, loss: 0.85\n",
      "train epoch: 2538, loss 0.0844\n",
      "test epoch: 2538, loss: 0.85\n",
      "train epoch: 2539, loss 0.0845\n",
      "test epoch: 2539, loss: 0.85\n",
      "train epoch: 2540, loss 0.0844\n",
      "test epoch: 2540, loss: 0.85\n",
      "train epoch: 2541, loss 0.0845\n",
      "test epoch: 2541, loss: 0.85\n",
      "train epoch: 2542, loss 0.0845\n",
      "test epoch: 2542, loss: 0.85\n",
      "train epoch: 2543, loss 0.0845\n",
      "test epoch: 2543, loss: 0.85\n",
      "train epoch: 2544, loss 0.0845\n",
      "test epoch: 2544, loss: 0.85\n",
      "train epoch: 2545, loss 0.0845\n",
      "test epoch: 2545, loss: 0.85\n",
      "train epoch: 2546, loss 0.0845\n",
      "test epoch: 2546, loss: 0.85\n",
      "train epoch: 2547, loss 0.0845\n",
      "test epoch: 2547, loss: 0.85\n",
      "train epoch: 2548, loss 0.0845\n",
      "test epoch: 2548, loss: 0.85\n",
      "train epoch: 2549, loss 0.0844\n",
      "test epoch: 2549, loss: 0.85\n",
      "train epoch: 2550, loss 0.0845\n",
      "test epoch: 2550, loss: 0.85\n",
      "train epoch: 2551, loss 0.0845\n",
      "test epoch: 2551, loss: 0.85\n",
      "train epoch: 2552, loss 0.0844\n",
      "test epoch: 2552, loss: 0.85\n",
      "train epoch: 2553, loss 0.0845\n",
      "test epoch: 2553, loss: 0.85\n",
      "train epoch: 2554, loss 0.0845\n",
      "test epoch: 2554, loss: 0.85\n",
      "train epoch: 2555, loss 0.0844\n",
      "test epoch: 2555, loss: 0.85\n",
      "train epoch: 2556, loss 0.0844\n",
      "test epoch: 2556, loss: 0.85\n",
      "train epoch: 2557, loss 0.0843\n",
      "test epoch: 2557, loss: 0.85\n",
      "train epoch: 2558, loss 0.0844\n",
      "test epoch: 2558, loss: 0.85\n",
      "train epoch: 2559, loss 0.0844\n",
      "test epoch: 2559, loss: 0.85\n",
      "train epoch: 2560, loss 0.0843\n",
      "test epoch: 2560, loss: 0.85\n",
      "train epoch: 2561, loss 0.0843\n",
      "test epoch: 2561, loss: 0.85\n",
      "train epoch: 2562, loss 0.0843\n",
      "test epoch: 2562, loss: 0.85\n",
      "train epoch: 2563, loss 0.0844\n",
      "test epoch: 2563, loss: 0.85\n",
      "train epoch: 2564, loss 0.0844\n",
      "test epoch: 2564, loss: 0.85\n",
      "train epoch: 2565, loss 0.0843\n",
      "test epoch: 2565, loss: 0.85\n",
      "train epoch: 2566, loss 0.0844\n",
      "test epoch: 2566, loss: 0.85\n",
      "train epoch: 2567, loss 0.0845\n",
      "test epoch: 2567, loss: 0.85\n",
      "train epoch: 2568, loss 0.0844\n",
      "test epoch: 2568, loss: 0.85\n",
      "train epoch: 2569, loss 0.0845\n",
      "test epoch: 2569, loss: 0.85\n",
      "train epoch: 2570, loss 0.0844\n",
      "test epoch: 2570, loss: 0.85\n",
      "train epoch: 2571, loss 0.0845\n",
      "test epoch: 2571, loss: 0.85\n",
      "train epoch: 2572, loss 0.0844\n",
      "test epoch: 2572, loss: 0.85\n",
      "train epoch: 2573, loss 0.0844\n",
      "test epoch: 2573, loss: 0.85\n",
      "train epoch: 2574, loss 0.0845\n",
      "test epoch: 2574, loss: 0.85\n",
      "train epoch: 2575, loss 0.0843\n",
      "test epoch: 2575, loss: 0.85\n",
      "train epoch: 2576, loss 0.0845\n",
      "test epoch: 2576, loss: 0.85\n",
      "train epoch: 2577, loss 0.0845\n",
      "test epoch: 2577, loss: 0.85\n",
      "train epoch: 2578, loss 0.0844\n",
      "test epoch: 2578, loss: 0.85\n",
      "train epoch: 2579, loss 0.0844\n",
      "test epoch: 2579, loss: 0.85\n",
      "train epoch: 2580, loss 0.0844\n",
      "test epoch: 2580, loss: 0.85\n",
      "train epoch: 2581, loss 0.0843\n",
      "test epoch: 2581, loss: 0.85\n",
      "train epoch: 2582, loss 0.0843\n",
      "test epoch: 2582, loss: 0.85\n",
      "train epoch: 2583, loss 0.0845\n",
      "test epoch: 2583, loss: 0.85\n",
      "train epoch: 2584, loss 0.0848\n",
      "test epoch: 2584, loss: 0.85\n",
      "train epoch: 2585, loss 0.0844\n",
      "test epoch: 2585, loss: 0.85\n",
      "train epoch: 2586, loss 0.0847\n",
      "test epoch: 2586, loss: 0.85\n",
      "train epoch: 2587, loss 0.0844\n",
      "test epoch: 2587, loss: 0.85\n",
      "train epoch: 2588, loss 0.0842\n",
      "test epoch: 2588, loss: 0.85\n",
      "train epoch: 2589, loss 0.0847\n",
      "test epoch: 2589, loss: 0.85\n",
      "train epoch: 2590, loss 0.0844\n",
      "test epoch: 2590, loss: 0.85\n",
      "train epoch: 2591, loss 0.0843\n",
      "test epoch: 2591, loss: 0.85\n",
      "train epoch: 2592, loss 0.0843\n",
      "test epoch: 2592, loss: 0.85\n",
      "train epoch: 2593, loss 0.0843\n",
      "test epoch: 2593, loss: 0.85\n",
      "train epoch: 2594, loss 0.0842\n",
      "test epoch: 2594, loss: 0.85\n",
      "train epoch: 2595, loss 0.0842\n",
      "test epoch: 2595, loss: 0.85\n",
      "train epoch: 2596, loss 0.0843\n",
      "test epoch: 2596, loss: 0.85\n",
      "train epoch: 2597, loss 0.0843\n",
      "test epoch: 2597, loss: 0.85\n",
      "train epoch: 2598, loss 0.0842\n",
      "test epoch: 2598, loss: 0.85\n",
      "train epoch: 2599, loss 0.0843\n",
      "test epoch: 2599, loss: 0.85\n",
      "train epoch: 2600, loss 0.0843\n",
      "test epoch: 2600, loss: 0.85\n",
      "train epoch: 2601, loss 0.0842\n",
      "test epoch: 2601, loss: 0.85\n",
      "train epoch: 2602, loss 0.0844\n",
      "test epoch: 2602, loss: 0.85\n",
      "train epoch: 2603, loss 0.0843\n",
      "test epoch: 2603, loss: 0.85\n",
      "train epoch: 2604, loss 0.0843\n",
      "test epoch: 2604, loss: 0.85\n",
      "train epoch: 2605, loss 0.0843\n",
      "test epoch: 2605, loss: 0.85\n",
      "train epoch: 2606, loss 0.0842\n",
      "test epoch: 2606, loss: 0.85\n",
      "train epoch: 2607, loss 0.0843\n",
      "test epoch: 2607, loss: 0.85\n",
      "train epoch: 2608, loss 0.0842\n",
      "test epoch: 2608, loss: 0.85\n",
      "train epoch: 2609, loss 0.0842\n",
      "test epoch: 2609, loss: 0.85\n",
      "train epoch: 2610, loss 0.0843\n",
      "test epoch: 2610, loss: 0.85\n",
      "train epoch: 2611, loss 0.0842\n",
      "test epoch: 2611, loss: 0.85\n",
      "train epoch: 2612, loss 0.0844\n",
      "test epoch: 2612, loss: 0.85\n",
      "train epoch: 2613, loss 0.0843\n",
      "test epoch: 2613, loss: 0.85\n",
      "train epoch: 2614, loss 0.0843\n",
      "test epoch: 2614, loss: 0.85\n",
      "train epoch: 2615, loss 0.0843\n",
      "test epoch: 2615, loss: 0.85\n",
      "train epoch: 2616, loss 0.0842\n",
      "test epoch: 2616, loss: 0.85\n",
      "train epoch: 2617, loss 0.0843\n",
      "test epoch: 2617, loss: 0.85\n",
      "train epoch: 2618, loss 0.0842\n",
      "test epoch: 2618, loss: 0.85\n",
      "train epoch: 2619, loss 0.0842\n",
      "test epoch: 2619, loss: 0.85\n",
      "train epoch: 2620, loss 0.0842\n",
      "test epoch: 2620, loss: 0.85\n",
      "train epoch: 2621, loss 0.0842\n",
      "test epoch: 2621, loss: 0.85\n",
      "train epoch: 2622, loss 0.0843\n",
      "test epoch: 2622, loss: 0.85\n",
      "train epoch: 2623, loss 0.0841\n",
      "test epoch: 2623, loss: 0.85\n",
      "train epoch: 2624, loss 0.0844\n",
      "test epoch: 2624, loss: 0.85\n",
      "train epoch: 2625, loss 0.0843\n",
      "test epoch: 2625, loss: 0.85\n",
      "train epoch: 2626, loss 0.0842\n",
      "test epoch: 2626, loss: 0.85\n",
      "train epoch: 2627, loss 0.0842\n",
      "test epoch: 2627, loss: 0.85\n",
      "train epoch: 2628, loss 0.0842\n",
      "test epoch: 2628, loss: 0.85\n",
      "train epoch: 2629, loss 0.0843\n",
      "test epoch: 2629, loss: 0.85\n",
      "train epoch: 2630, loss 0.0843\n",
      "test epoch: 2630, loss: 0.85\n",
      "train epoch: 2631, loss 0.0842\n",
      "test epoch: 2631, loss: 0.85\n",
      "train epoch: 2632, loss 0.0841\n",
      "test epoch: 2632, loss: 0.85\n",
      "train epoch: 2633, loss 0.0841\n",
      "test epoch: 2633, loss: 0.85\n",
      "train epoch: 2634, loss 0.0841\n",
      "test epoch: 2634, loss: 0.85\n",
      "train epoch: 2635, loss 0.0841\n",
      "test epoch: 2635, loss: 0.85\n",
      "train epoch: 2636, loss 0.0841\n",
      "test epoch: 2636, loss: 0.85\n",
      "train epoch: 2637, loss 0.0842\n",
      "test epoch: 2637, loss: 0.85\n",
      "train epoch: 2638, loss 0.0842\n",
      "test epoch: 2638, loss: 0.85\n",
      "train epoch: 2639, loss 0.0842\n",
      "test epoch: 2639, loss: 0.85\n",
      "train epoch: 2640, loss 0.0843\n",
      "test epoch: 2640, loss: 0.85\n",
      "train epoch: 2641, loss 0.0841\n",
      "test epoch: 2641, loss: 0.85\n",
      "train epoch: 2642, loss 0.0844\n",
      "test epoch: 2642, loss: 0.85\n",
      "train epoch: 2643, loss 0.0842\n",
      "test epoch: 2643, loss: 0.85\n",
      "train epoch: 2644, loss 0.0843\n",
      "test epoch: 2644, loss: 0.85\n",
      "train epoch: 2645, loss 0.0849\n",
      "test epoch: 2645, loss: 0.85\n",
      "train epoch: 2646, loss 0.0841\n",
      "test epoch: 2646, loss: 0.85\n",
      "train epoch: 2647, loss 0.0853\n",
      "test epoch: 2647, loss: 0.85\n",
      "train epoch: 2648, loss 0.0846\n",
      "test epoch: 2648, loss: 0.85\n",
      "train epoch: 2649, loss 0.0847\n",
      "test epoch: 2649, loss: 0.85\n",
      "train epoch: 2650, loss 0.0847\n",
      "test epoch: 2650, loss: 0.85\n",
      "train epoch: 2651, loss 0.0841\n",
      "test epoch: 2651, loss: 0.85\n",
      "train epoch: 2652, loss 0.0848\n",
      "test epoch: 2652, loss: 0.85\n",
      "train epoch: 2653, loss 0.0841\n",
      "test epoch: 2653, loss: 0.85\n",
      "train epoch: 2654, loss 0.0845\n",
      "test epoch: 2654, loss: 0.85\n",
      "train epoch: 2655, loss 0.0846\n",
      "test epoch: 2655, loss: 0.85\n",
      "train epoch: 2656, loss 0.0843\n",
      "test epoch: 2656, loss: 0.85\n",
      "train epoch: 2657, loss 0.0849\n",
      "test epoch: 2657, loss: 0.85\n",
      "train epoch: 2658, loss 0.0841\n",
      "test epoch: 2658, loss: 0.85\n",
      "train epoch: 2659, loss 0.0843\n",
      "test epoch: 2659, loss: 0.85\n",
      "train epoch: 2660, loss 0.0848\n",
      "test epoch: 2660, loss: 0.85\n",
      "train epoch: 2661, loss 0.0840\n",
      "test epoch: 2661, loss: 0.85\n",
      "train epoch: 2662, loss 0.0842\n",
      "test epoch: 2662, loss: 0.85\n",
      "train epoch: 2663, loss 0.0843\n",
      "test epoch: 2663, loss: 0.85\n",
      "train epoch: 2664, loss 0.0841\n",
      "test epoch: 2664, loss: 0.85\n",
      "train epoch: 2665, loss 0.0843\n",
      "test epoch: 2665, loss: 0.85\n",
      "train epoch: 2666, loss 0.0840\n",
      "test epoch: 2666, loss: 0.85\n",
      "train epoch: 2667, loss 0.0843\n",
      "test epoch: 2667, loss: 0.86\n",
      "train epoch: 2668, loss 0.0847\n",
      "test epoch: 2668, loss: 0.86\n",
      "train epoch: 2669, loss 0.0840\n",
      "test epoch: 2669, loss: 0.86\n",
      "train epoch: 2670, loss 0.0849\n",
      "test epoch: 2670, loss: 0.86\n",
      "train epoch: 2671, loss 0.0843\n",
      "test epoch: 2671, loss: 0.86\n",
      "train epoch: 2672, loss 0.0843\n",
      "test epoch: 2672, loss: 0.86\n",
      "train epoch: 2673, loss 0.0848\n",
      "test epoch: 2673, loss: 0.86\n",
      "train epoch: 2674, loss 0.0840\n",
      "test epoch: 2674, loss: 0.86\n",
      "train epoch: 2675, loss 0.0844\n",
      "test epoch: 2675, loss: 0.86\n",
      "train epoch: 2676, loss 0.0841\n",
      "test epoch: 2676, loss: 0.86\n",
      "train epoch: 2677, loss 0.0842\n",
      "test epoch: 2677, loss: 0.86\n",
      "train epoch: 2678, loss 0.0843\n",
      "test epoch: 2678, loss: 0.86\n",
      "train epoch: 2679, loss 0.0840\n",
      "test epoch: 2679, loss: 0.86\n",
      "train epoch: 2680, loss 0.0840\n",
      "test epoch: 2680, loss: 0.86\n",
      "train epoch: 2681, loss 0.0840\n",
      "test epoch: 2681, loss: 0.86\n",
      "train epoch: 2682, loss 0.0840\n",
      "test epoch: 2682, loss: 0.86\n",
      "train epoch: 2683, loss 0.0840\n",
      "test epoch: 2683, loss: 0.86\n",
      "train epoch: 2684, loss 0.0840\n",
      "test epoch: 2684, loss: 0.86\n",
      "train epoch: 2685, loss 0.0843\n",
      "test epoch: 2685, loss: 0.86\n",
      "train epoch: 2686, loss 0.0841\n",
      "test epoch: 2686, loss: 0.86\n",
      "train epoch: 2687, loss 0.0841\n",
      "test epoch: 2687, loss: 0.86\n",
      "train epoch: 2688, loss 0.0842\n",
      "test epoch: 2688, loss: 0.86\n",
      "train epoch: 2689, loss 0.0840\n",
      "test epoch: 2689, loss: 0.86\n",
      "train epoch: 2690, loss 0.0841\n",
      "test epoch: 2690, loss: 0.86\n",
      "train epoch: 2691, loss 0.0845\n",
      "test epoch: 2691, loss: 0.86\n",
      "train epoch: 2692, loss 0.0841\n",
      "test epoch: 2692, loss: 0.86\n",
      "train epoch: 2693, loss 0.0845\n",
      "test epoch: 2693, loss: 0.86\n",
      "train epoch: 2694, loss 0.0842\n",
      "test epoch: 2694, loss: 0.86\n",
      "train epoch: 2695, loss 0.0841\n",
      "test epoch: 2695, loss: 0.86\n",
      "train epoch: 2696, loss 0.0848\n",
      "test epoch: 2696, loss: 0.86\n",
      "train epoch: 2697, loss 0.0841\n",
      "test epoch: 2697, loss: 0.86\n",
      "train epoch: 2698, loss 0.0840\n",
      "test epoch: 2698, loss: 0.86\n",
      "train epoch: 2699, loss 0.0846\n",
      "test epoch: 2699, loss: 0.86\n",
      "train epoch: 2700, loss 0.0840\n",
      "test epoch: 2700, loss: 0.86\n",
      "train epoch: 2701, loss 0.0839\n",
      "test epoch: 2701, loss: 0.86\n",
      "train epoch: 2702, loss 0.0846\n",
      "test epoch: 2702, loss: 0.86\n",
      "train epoch: 2703, loss 0.0840\n",
      "test epoch: 2703, loss: 0.86\n",
      "train epoch: 2704, loss 0.0839\n",
      "test epoch: 2704, loss: 0.86\n",
      "train epoch: 2705, loss 0.0841\n",
      "test epoch: 2705, loss: 0.86\n",
      "train epoch: 2706, loss 0.0840\n",
      "test epoch: 2706, loss: 0.86\n",
      "train epoch: 2707, loss 0.0839\n",
      "test epoch: 2707, loss: 0.86\n",
      "train epoch: 2708, loss 0.0842\n",
      "test epoch: 2708, loss: 0.86\n",
      "train epoch: 2709, loss 0.0842\n",
      "test epoch: 2709, loss: 0.86\n",
      "train epoch: 2710, loss 0.0839\n",
      "test epoch: 2710, loss: 0.86\n",
      "train epoch: 2711, loss 0.0840\n",
      "test epoch: 2711, loss: 0.86\n",
      "train epoch: 2712, loss 0.0840\n",
      "test epoch: 2712, loss: 0.86\n",
      "train epoch: 2713, loss 0.0839\n",
      "test epoch: 2713, loss: 0.86\n",
      "train epoch: 2714, loss 0.0840\n",
      "test epoch: 2714, loss: 0.86\n",
      "train epoch: 2715, loss 0.0846\n",
      "test epoch: 2715, loss: 0.86\n",
      "train epoch: 2716, loss 0.0840\n",
      "test epoch: 2716, loss: 0.86\n",
      "train epoch: 2717, loss 0.0844\n",
      "test epoch: 2717, loss: 0.86\n",
      "train epoch: 2718, loss 0.0841\n",
      "test epoch: 2718, loss: 0.86\n",
      "train epoch: 2719, loss 0.0840\n",
      "test epoch: 2719, loss: 0.86\n",
      "train epoch: 2720, loss 0.0847\n",
      "test epoch: 2720, loss: 0.86\n",
      "train epoch: 2721, loss 0.0840\n",
      "test epoch: 2721, loss: 0.86\n",
      "train epoch: 2722, loss 0.0840\n",
      "test epoch: 2722, loss: 0.86\n",
      "train epoch: 2723, loss 0.0844\n",
      "test epoch: 2723, loss: 0.86\n",
      "train epoch: 2724, loss 0.0839\n",
      "test epoch: 2724, loss: 0.86\n",
      "train epoch: 2725, loss 0.0843\n",
      "test epoch: 2725, loss: 0.86\n",
      "train epoch: 2726, loss 0.0844\n",
      "test epoch: 2726, loss: 0.86\n",
      "train epoch: 2727, loss 0.0840\n",
      "test epoch: 2727, loss: 0.86\n",
      "train epoch: 2728, loss 0.0840\n",
      "test epoch: 2728, loss: 0.86\n",
      "train epoch: 2729, loss 0.0842\n",
      "test epoch: 2729, loss: 0.86\n",
      "train epoch: 2730, loss 0.0839\n",
      "test epoch: 2730, loss: 0.86\n",
      "train epoch: 2731, loss 0.0841\n",
      "test epoch: 2731, loss: 0.86\n",
      "train epoch: 2732, loss 0.0843\n",
      "test epoch: 2732, loss: 0.86\n",
      "train epoch: 2733, loss 0.0838\n",
      "test epoch: 2733, loss: 0.86\n",
      "train epoch: 2734, loss 0.0840\n",
      "test epoch: 2734, loss: 0.86\n",
      "train epoch: 2735, loss 0.0843\n",
      "test epoch: 2735, loss: 0.86\n",
      "train epoch: 2736, loss 0.0839\n",
      "test epoch: 2736, loss: 0.86\n",
      "train epoch: 2737, loss 0.0843\n",
      "test epoch: 2737, loss: 0.86\n",
      "train epoch: 2738, loss 0.0842\n",
      "test epoch: 2738, loss: 0.86\n",
      "train epoch: 2739, loss 0.0838\n",
      "test epoch: 2739, loss: 0.86\n",
      "train epoch: 2740, loss 0.0840\n",
      "test epoch: 2740, loss: 0.86\n",
      "train epoch: 2741, loss 0.0841\n",
      "test epoch: 2741, loss: 0.86\n",
      "train epoch: 2742, loss 0.0838\n",
      "test epoch: 2742, loss: 0.86\n",
      "train epoch: 2743, loss 0.0841\n",
      "test epoch: 2743, loss: 0.86\n",
      "train epoch: 2744, loss 0.0840\n",
      "test epoch: 2744, loss: 0.86\n",
      "train epoch: 2745, loss 0.0840\n",
      "test epoch: 2745, loss: 0.86\n",
      "train epoch: 2746, loss 0.0838\n",
      "test epoch: 2746, loss: 0.86\n",
      "train epoch: 2747, loss 0.0838\n",
      "test epoch: 2747, loss: 0.86\n",
      "train epoch: 2748, loss 0.0838\n",
      "test epoch: 2748, loss: 0.86\n",
      "train epoch: 2749, loss 0.0838\n",
      "test epoch: 2749, loss: 0.86\n",
      "train epoch: 2750, loss 0.0838\n",
      "test epoch: 2750, loss: 0.86\n",
      "train epoch: 2751, loss 0.0843\n",
      "test epoch: 2751, loss: 0.86\n",
      "train epoch: 2752, loss 0.0840\n",
      "test epoch: 2752, loss: 0.86\n",
      "train epoch: 2753, loss 0.0841\n",
      "test epoch: 2753, loss: 0.86\n",
      "train epoch: 2754, loss 0.0840\n",
      "test epoch: 2754, loss: 0.86\n",
      "train epoch: 2755, loss 0.0838\n",
      "test epoch: 2755, loss: 0.86\n",
      "train epoch: 2756, loss 0.0844\n",
      "test epoch: 2756, loss: 0.86\n",
      "train epoch: 2757, loss 0.0840\n",
      "test epoch: 2757, loss: 0.86\n",
      "train epoch: 2758, loss 0.0839\n",
      "test epoch: 2758, loss: 0.86\n",
      "train epoch: 2759, loss 0.0844\n",
      "test epoch: 2759, loss: 0.86\n",
      "train epoch: 2760, loss 0.0838\n",
      "test epoch: 2760, loss: 0.86\n",
      "train epoch: 2761, loss 0.0839\n",
      "test epoch: 2761, loss: 0.86\n",
      "train epoch: 2762, loss 0.0843\n",
      "test epoch: 2762, loss: 0.86\n",
      "train epoch: 2763, loss 0.0839\n",
      "test epoch: 2763, loss: 0.86\n",
      "train epoch: 2764, loss 0.0838\n",
      "test epoch: 2764, loss: 0.86\n",
      "train epoch: 2765, loss 0.0840\n",
      "test epoch: 2765, loss: 0.86\n",
      "train epoch: 2766, loss 0.0838\n",
      "test epoch: 2766, loss: 0.86\n",
      "train epoch: 2767, loss 0.0838\n",
      "test epoch: 2767, loss: 0.86\n",
      "train epoch: 2768, loss 0.0838\n",
      "test epoch: 2768, loss: 0.86\n",
      "train epoch: 2769, loss 0.0843\n",
      "test epoch: 2769, loss: 0.86\n",
      "train epoch: 2770, loss 0.0839\n",
      "test epoch: 2770, loss: 0.86\n",
      "train epoch: 2771, loss 0.0840\n",
      "test epoch: 2771, loss: 0.86\n",
      "train epoch: 2772, loss 0.0840\n",
      "test epoch: 2772, loss: 0.86\n",
      "train epoch: 2773, loss 0.0838\n",
      "test epoch: 2773, loss: 0.86\n",
      "train epoch: 2774, loss 0.0843\n",
      "test epoch: 2774, loss: 0.86\n",
      "train epoch: 2775, loss 0.0841\n",
      "test epoch: 2775, loss: 0.86\n",
      "train epoch: 2776, loss 0.0839\n",
      "test epoch: 2776, loss: 0.86\n",
      "train epoch: 2777, loss 0.0844\n",
      "test epoch: 2777, loss: 0.86\n",
      "train epoch: 2778, loss 0.0842\n",
      "test epoch: 2778, loss: 0.86\n",
      "train epoch: 2779, loss 0.0840\n",
      "test epoch: 2779, loss: 0.86\n",
      "train epoch: 2780, loss 0.0848\n",
      "test epoch: 2780, loss: 0.86\n",
      "train epoch: 2781, loss 0.0838\n",
      "test epoch: 2781, loss: 0.86\n",
      "train epoch: 2782, loss 0.0839\n",
      "test epoch: 2782, loss: 0.86\n",
      "train epoch: 2783, loss 0.0843\n",
      "test epoch: 2783, loss: 0.86\n",
      "train epoch: 2784, loss 0.0839\n",
      "test epoch: 2784, loss: 0.86\n",
      "train epoch: 2785, loss 0.0838\n",
      "test epoch: 2785, loss: 0.86\n",
      "train epoch: 2786, loss 0.0844\n",
      "test epoch: 2786, loss: 0.86\n",
      "train epoch: 2787, loss 0.0841\n",
      "test epoch: 2787, loss: 0.86\n",
      "train epoch: 2788, loss 0.0839\n",
      "test epoch: 2788, loss: 0.86\n",
      "train epoch: 2789, loss 0.0845\n",
      "test epoch: 2789, loss: 0.86\n",
      "train epoch: 2790, loss 0.0839\n",
      "test epoch: 2790, loss: 0.86\n",
      "train epoch: 2791, loss 0.0839\n",
      "test epoch: 2791, loss: 0.86\n",
      "train epoch: 2792, loss 0.0845\n",
      "test epoch: 2792, loss: 0.86\n",
      "train epoch: 2793, loss 0.0838\n",
      "test epoch: 2793, loss: 0.86\n",
      "train epoch: 2794, loss 0.0837\n",
      "test epoch: 2794, loss: 0.86\n",
      "train epoch: 2795, loss 0.0841\n",
      "test epoch: 2795, loss: 0.86\n",
      "train epoch: 2796, loss 0.0840\n",
      "test epoch: 2796, loss: 0.86\n",
      "train epoch: 2797, loss 0.0838\n",
      "test epoch: 2797, loss: 0.86\n",
      "train epoch: 2798, loss 0.0839\n",
      "test epoch: 2798, loss: 0.86\n",
      "train epoch: 2799, loss 0.0838\n",
      "test epoch: 2799, loss: 0.86\n",
      "train epoch: 2800, loss 0.0838\n",
      "test epoch: 2800, loss: 0.86\n",
      "train epoch: 2801, loss 0.0838\n",
      "test epoch: 2801, loss: 0.86\n",
      "train epoch: 2802, loss 0.0841\n",
      "test epoch: 2802, loss: 0.86\n",
      "train epoch: 2803, loss 0.0839\n",
      "test epoch: 2803, loss: 0.86\n",
      "train epoch: 2804, loss 0.0841\n",
      "test epoch: 2804, loss: 0.86\n",
      "train epoch: 2805, loss 0.0839\n",
      "test epoch: 2805, loss: 0.86\n",
      "train epoch: 2806, loss 0.0839\n",
      "test epoch: 2806, loss: 0.86\n",
      "train epoch: 2807, loss 0.0838\n",
      "test epoch: 2807, loss: 0.86\n",
      "train epoch: 2808, loss 0.0839\n",
      "test epoch: 2808, loss: 0.86\n",
      "train epoch: 2809, loss 0.0838\n",
      "test epoch: 2809, loss: 0.86\n",
      "train epoch: 2810, loss 0.0837\n",
      "test epoch: 2810, loss: 0.86\n",
      "train epoch: 2811, loss 0.0841\n",
      "test epoch: 2811, loss: 0.86\n",
      "train epoch: 2812, loss 0.0842\n",
      "test epoch: 2812, loss: 0.86\n",
      "train epoch: 2813, loss 0.0838\n",
      "test epoch: 2813, loss: 0.86\n",
      "train epoch: 2814, loss 0.0850\n",
      "test epoch: 2814, loss: 0.86\n",
      "train epoch: 2815, loss 0.0840\n",
      "test epoch: 2815, loss: 0.86\n",
      "train epoch: 2816, loss 0.0841\n",
      "test epoch: 2816, loss: 0.86\n",
      "train epoch: 2817, loss 0.0845\n",
      "test epoch: 2817, loss: 0.86\n",
      "train epoch: 2818, loss 0.0838\n",
      "test epoch: 2818, loss: 0.86\n",
      "train epoch: 2819, loss 0.0843\n",
      "test epoch: 2819, loss: 0.86\n",
      "train epoch: 2820, loss 0.0839\n",
      "test epoch: 2820, loss: 0.86\n",
      "train epoch: 2821, loss 0.0841\n",
      "test epoch: 2821, loss: 0.86\n",
      "train epoch: 2822, loss 0.0845\n",
      "test epoch: 2822, loss: 0.86\n",
      "train epoch: 2823, loss 0.0838\n",
      "test epoch: 2823, loss: 0.86\n",
      "train epoch: 2824, loss 0.0853\n",
      "test epoch: 2824, loss: 0.86\n",
      "train epoch: 2825, loss 0.0838\n",
      "test epoch: 2825, loss: 0.86\n",
      "train epoch: 2826, loss 0.0842\n",
      "test epoch: 2826, loss: 0.86\n",
      "train epoch: 2827, loss 0.0843\n",
      "test epoch: 2827, loss: 0.86\n",
      "train epoch: 2828, loss 0.0836\n",
      "test epoch: 2828, loss: 0.86\n",
      "train epoch: 2829, loss 0.0841\n",
      "test epoch: 2829, loss: 0.86\n",
      "train epoch: 2830, loss 0.0837\n",
      "test epoch: 2830, loss: 0.86\n",
      "train epoch: 2831, loss 0.0838\n",
      "test epoch: 2831, loss: 0.86\n",
      "train epoch: 2832, loss 0.0845\n",
      "test epoch: 2832, loss: 0.86\n",
      "train epoch: 2833, loss 0.0836\n",
      "test epoch: 2833, loss: 0.86\n",
      "train epoch: 2834, loss 0.0848\n",
      "test epoch: 2834, loss: 0.86\n",
      "train epoch: 2835, loss 0.0839\n",
      "test epoch: 2835, loss: 0.86\n",
      "train epoch: 2836, loss 0.0840\n",
      "test epoch: 2836, loss: 0.86\n",
      "train epoch: 2837, loss 0.0843\n",
      "test epoch: 2837, loss: 0.86\n",
      "train epoch: 2838, loss 0.0838\n",
      "test epoch: 2838, loss: 0.86\n",
      "train epoch: 2839, loss 0.0838\n",
      "test epoch: 2839, loss: 0.86\n",
      "train epoch: 2840, loss 0.0840\n",
      "test epoch: 2840, loss: 0.86\n",
      "train epoch: 2841, loss 0.0839\n",
      "test epoch: 2841, loss: 0.86\n",
      "train epoch: 2842, loss 0.0836\n",
      "test epoch: 2842, loss: 0.86\n",
      "train epoch: 2843, loss 0.0840\n",
      "test epoch: 2843, loss: 0.86\n",
      "train epoch: 2844, loss 0.0840\n",
      "test epoch: 2844, loss: 0.86\n",
      "train epoch: 2845, loss 0.0838\n",
      "test epoch: 2845, loss: 0.86\n",
      "train epoch: 2846, loss 0.0845\n",
      "test epoch: 2846, loss: 0.86\n",
      "train epoch: 2847, loss 0.0838\n",
      "test epoch: 2847, loss: 0.86\n",
      "train epoch: 2848, loss 0.0838\n",
      "test epoch: 2848, loss: 0.86\n",
      "train epoch: 2849, loss 0.0844\n",
      "test epoch: 2849, loss: 0.86\n",
      "train epoch: 2850, loss 0.0836\n",
      "test epoch: 2850, loss: 0.86\n",
      "train epoch: 2851, loss 0.0838\n",
      "test epoch: 2851, loss: 0.86\n",
      "train epoch: 2852, loss 0.0840\n",
      "test epoch: 2852, loss: 0.86\n",
      "train epoch: 2853, loss 0.0838\n",
      "test epoch: 2853, loss: 0.86\n",
      "train epoch: 2854, loss 0.0837\n",
      "test epoch: 2854, loss: 0.86\n",
      "train epoch: 2855, loss 0.0838\n",
      "test epoch: 2855, loss: 0.86\n",
      "train epoch: 2856, loss 0.0840\n",
      "test epoch: 2856, loss: 0.86\n",
      "train epoch: 2857, loss 0.0836\n",
      "test epoch: 2857, loss: 0.86\n",
      "train epoch: 2858, loss 0.0839\n",
      "test epoch: 2858, loss: 0.86\n",
      "train epoch: 2859, loss 0.0841\n",
      "test epoch: 2859, loss: 0.86\n",
      "train epoch: 2860, loss 0.0837\n",
      "test epoch: 2860, loss: 0.86\n",
      "train epoch: 2861, loss 0.0844\n",
      "test epoch: 2861, loss: 0.86\n",
      "train epoch: 2862, loss 0.0838\n",
      "test epoch: 2862, loss: 0.86\n",
      "train epoch: 2863, loss 0.0837\n",
      "test epoch: 2863, loss: 0.86\n",
      "train epoch: 2864, loss 0.0844\n",
      "test epoch: 2864, loss: 0.86\n",
      "train epoch: 2865, loss 0.0837\n",
      "test epoch: 2865, loss: 0.86\n",
      "train epoch: 2866, loss 0.0838\n",
      "test epoch: 2866, loss: 0.86\n",
      "train epoch: 2867, loss 0.0841\n",
      "test epoch: 2867, loss: 0.86\n",
      "train epoch: 2868, loss 0.0838\n",
      "test epoch: 2868, loss: 0.86\n",
      "train epoch: 2869, loss 0.0837\n",
      "test epoch: 2869, loss: 0.86\n",
      "train epoch: 2870, loss 0.0841\n",
      "test epoch: 2870, loss: 0.86\n",
      "train epoch: 2871, loss 0.0838\n",
      "test epoch: 2871, loss: 0.86\n",
      "train epoch: 2872, loss 0.0836\n",
      "test epoch: 2872, loss: 0.86\n",
      "train epoch: 2873, loss 0.0842\n",
      "test epoch: 2873, loss: 0.86\n",
      "train epoch: 2874, loss 0.0839\n",
      "test epoch: 2874, loss: 0.86\n",
      "train epoch: 2875, loss 0.0837\n",
      "test epoch: 2875, loss: 0.86\n",
      "train epoch: 2876, loss 0.0844\n",
      "test epoch: 2876, loss: 0.86\n",
      "train epoch: 2877, loss 0.0836\n",
      "test epoch: 2877, loss: 0.86\n",
      "train epoch: 2878, loss 0.0839\n",
      "test epoch: 2878, loss: 0.86\n",
      "train epoch: 2879, loss 0.0842\n",
      "test epoch: 2879, loss: 0.86\n",
      "train epoch: 2880, loss 0.0836\n",
      "test epoch: 2880, loss: 0.86\n",
      "train epoch: 2881, loss 0.0840\n",
      "test epoch: 2881, loss: 0.86\n",
      "train epoch: 2882, loss 0.0838\n",
      "test epoch: 2882, loss: 0.86\n",
      "train epoch: 2883, loss 0.0837\n",
      "test epoch: 2883, loss: 0.86\n",
      "train epoch: 2884, loss 0.0845\n",
      "test epoch: 2884, loss: 0.86\n",
      "train epoch: 2885, loss 0.0835\n",
      "test epoch: 2885, loss: 0.86\n",
      "train epoch: 2886, loss 0.0844\n",
      "test epoch: 2886, loss: 0.86\n",
      "train epoch: 2887, loss 0.0839\n",
      "test epoch: 2887, loss: 0.86\n",
      "train epoch: 2888, loss 0.0838\n",
      "test epoch: 2888, loss: 0.86\n",
      "train epoch: 2889, loss 0.0843\n",
      "test epoch: 2889, loss: 0.86\n",
      "train epoch: 2890, loss 0.0836\n",
      "test epoch: 2890, loss: 0.86\n",
      "train epoch: 2891, loss 0.0837\n",
      "test epoch: 2891, loss: 0.86\n",
      "train epoch: 2892, loss 0.0839\n",
      "test epoch: 2892, loss: 0.86\n",
      "train epoch: 2893, loss 0.0837\n",
      "test epoch: 2893, loss: 0.86\n",
      "train epoch: 2894, loss 0.0835\n",
      "test epoch: 2894, loss: 0.86\n",
      "train epoch: 2895, loss 0.0842\n",
      "test epoch: 2895, loss: 0.86\n",
      "train epoch: 2896, loss 0.0838\n",
      "test epoch: 2896, loss: 0.86\n",
      "train epoch: 2897, loss 0.0837\n",
      "test epoch: 2897, loss: 0.86\n",
      "train epoch: 2898, loss 0.0842\n",
      "test epoch: 2898, loss: 0.86\n",
      "train epoch: 2899, loss 0.0837\n",
      "test epoch: 2899, loss: 0.86\n",
      "train epoch: 2900, loss 0.0837\n",
      "test epoch: 2900, loss: 0.86\n",
      "train epoch: 2901, loss 0.0842\n",
      "test epoch: 2901, loss: 0.86\n",
      "train epoch: 2902, loss 0.0837\n",
      "test epoch: 2902, loss: 0.86\n",
      "train epoch: 2903, loss 0.0837\n",
      "test epoch: 2903, loss: 0.86\n",
      "train epoch: 2904, loss 0.0838\n",
      "test epoch: 2904, loss: 0.86\n",
      "train epoch: 2905, loss 0.0838\n",
      "test epoch: 2905, loss: 0.86\n",
      "train epoch: 2906, loss 0.0835\n",
      "test epoch: 2906, loss: 0.86\n",
      "train epoch: 2907, loss 0.0837\n",
      "test epoch: 2907, loss: 0.86\n",
      "train epoch: 2908, loss 0.0835\n",
      "test epoch: 2908, loss: 0.86\n",
      "train epoch: 2909, loss 0.0836\n",
      "test epoch: 2909, loss: 0.86\n",
      "train epoch: 2910, loss 0.0839\n",
      "test epoch: 2910, loss: 0.86\n",
      "train epoch: 2911, loss 0.0836\n",
      "test epoch: 2911, loss: 0.86\n",
      "train epoch: 2912, loss 0.0837\n",
      "test epoch: 2912, loss: 0.86\n",
      "train epoch: 2913, loss 0.0837\n",
      "test epoch: 2913, loss: 0.86\n",
      "train epoch: 2914, loss 0.0835\n",
      "test epoch: 2914, loss: 0.86\n",
      "train epoch: 2915, loss 0.0837\n",
      "test epoch: 2915, loss: 0.86\n",
      "train epoch: 2916, loss 0.0838\n",
      "test epoch: 2916, loss: 0.86\n",
      "train epoch: 2917, loss 0.0836\n",
      "test epoch: 2917, loss: 0.86\n",
      "train epoch: 2918, loss 0.0835\n",
      "test epoch: 2918, loss: 0.86\n",
      "train epoch: 2919, loss 0.0838\n",
      "test epoch: 2919, loss: 0.86\n",
      "train epoch: 2920, loss 0.0835\n",
      "test epoch: 2920, loss: 0.86\n",
      "train epoch: 2921, loss 0.0837\n",
      "test epoch: 2921, loss: 0.86\n",
      "train epoch: 2922, loss 0.0842\n",
      "test epoch: 2922, loss: 0.86\n",
      "train epoch: 2923, loss 0.0835\n",
      "test epoch: 2923, loss: 0.86\n",
      "train epoch: 2924, loss 0.0844\n",
      "test epoch: 2924, loss: 0.86\n",
      "train epoch: 2925, loss 0.0839\n",
      "test epoch: 2925, loss: 0.86\n",
      "train epoch: 2926, loss 0.0838\n",
      "test epoch: 2926, loss: 0.86\n",
      "train epoch: 2927, loss 0.0844\n",
      "test epoch: 2927, loss: 0.86\n",
      "train epoch: 2928, loss 0.0835\n",
      "test epoch: 2928, loss: 0.86\n",
      "train epoch: 2929, loss 0.0837\n",
      "test epoch: 2929, loss: 0.86\n",
      "train epoch: 2930, loss 0.0836\n",
      "test epoch: 2930, loss: 0.86\n",
      "train epoch: 2931, loss 0.0837\n",
      "test epoch: 2931, loss: 0.86\n",
      "train epoch: 2932, loss 0.0836\n",
      "test epoch: 2932, loss: 0.86\n",
      "train epoch: 2933, loss 0.0839\n",
      "test epoch: 2933, loss: 0.86\n",
      "train epoch: 2934, loss 0.0840\n",
      "test epoch: 2934, loss: 0.86\n",
      "train epoch: 2935, loss 0.0835\n",
      "test epoch: 2935, loss: 0.86\n",
      "train epoch: 2936, loss 0.0848\n",
      "test epoch: 2936, loss: 0.86\n",
      "train epoch: 2937, loss 0.0836\n",
      "test epoch: 2937, loss: 0.86\n",
      "train epoch: 2938, loss 0.0838\n",
      "test epoch: 2938, loss: 0.86\n",
      "train epoch: 2939, loss 0.0838\n",
      "test epoch: 2939, loss: 0.86\n",
      "train epoch: 2940, loss 0.0836\n",
      "test epoch: 2940, loss: 0.86\n",
      "train epoch: 2941, loss 0.0834\n",
      "test epoch: 2941, loss: 0.86\n",
      "train epoch: 2942, loss 0.0836\n",
      "test epoch: 2942, loss: 0.86\n",
      "train epoch: 2943, loss 0.0838\n",
      "test epoch: 2943, loss: 0.86\n",
      "train epoch: 2944, loss 0.0834\n",
      "test epoch: 2944, loss: 0.86\n",
      "train epoch: 2945, loss 0.0838\n",
      "test epoch: 2945, loss: 0.86\n",
      "train epoch: 2946, loss 0.0839\n",
      "test epoch: 2946, loss: 0.86\n",
      "train epoch: 2947, loss 0.0835\n",
      "test epoch: 2947, loss: 0.86\n",
      "train epoch: 2948, loss 0.0845\n",
      "test epoch: 2948, loss: 0.86\n",
      "train epoch: 2949, loss 0.0836\n",
      "test epoch: 2949, loss: 0.86\n",
      "train epoch: 2950, loss 0.0837\n",
      "test epoch: 2950, loss: 0.86\n",
      "train epoch: 2951, loss 0.0841\n",
      "test epoch: 2951, loss: 0.86\n",
      "train epoch: 2952, loss 0.0834\n",
      "test epoch: 2952, loss: 0.86\n",
      "train epoch: 2953, loss 0.0837\n",
      "test epoch: 2953, loss: 0.86\n",
      "train epoch: 2954, loss 0.0836\n",
      "test epoch: 2954, loss: 0.86\n",
      "train epoch: 2955, loss 0.0836\n",
      "test epoch: 2955, loss: 0.86\n",
      "train epoch: 2956, loss 0.0834\n",
      "test epoch: 2956, loss: 0.86\n",
      "train epoch: 2957, loss 0.0835\n",
      "test epoch: 2957, loss: 0.86\n",
      "train epoch: 2958, loss 0.0835\n",
      "test epoch: 2958, loss: 0.86\n",
      "train epoch: 2959, loss 0.0836\n",
      "test epoch: 2959, loss: 0.86\n",
      "train epoch: 2960, loss 0.0837\n",
      "test epoch: 2960, loss: 0.86\n",
      "train epoch: 2961, loss 0.0834\n",
      "test epoch: 2961, loss: 0.86\n",
      "train epoch: 2962, loss 0.0836\n",
      "test epoch: 2962, loss: 0.86\n",
      "train epoch: 2963, loss 0.0835\n",
      "test epoch: 2963, loss: 0.86\n",
      "train epoch: 2964, loss 0.0836\n",
      "test epoch: 2964, loss: 0.86\n",
      "train epoch: 2965, loss 0.0836\n",
      "test epoch: 2965, loss: 0.86\n",
      "train epoch: 2966, loss 0.0836\n",
      "test epoch: 2966, loss: 0.86\n",
      "train epoch: 2967, loss 0.0834\n",
      "test epoch: 2967, loss: 0.86\n",
      "train epoch: 2968, loss 0.0834\n",
      "test epoch: 2968, loss: 0.86\n",
      "train epoch: 2969, loss 0.0838\n",
      "test epoch: 2969, loss: 0.86\n",
      "train epoch: 2970, loss 0.0837\n",
      "test epoch: 2970, loss: 0.86\n",
      "train epoch: 2971, loss 0.0834\n",
      "test epoch: 2971, loss: 0.86\n",
      "train epoch: 2972, loss 0.0840\n",
      "test epoch: 2972, loss: 0.86\n",
      "train epoch: 2973, loss 0.0839\n",
      "test epoch: 2973, loss: 0.86\n",
      "train epoch: 2974, loss 0.0836\n",
      "test epoch: 2974, loss: 0.86\n",
      "train epoch: 2975, loss 0.0845\n",
      "test epoch: 2975, loss: 0.86\n",
      "train epoch: 2976, loss 0.0835\n",
      "test epoch: 2976, loss: 0.86\n",
      "train epoch: 2977, loss 0.0837\n",
      "test epoch: 2977, loss: 0.86\n",
      "train epoch: 2978, loss 0.0839\n",
      "test epoch: 2978, loss: 0.87\n",
      "train epoch: 2979, loss 0.0834\n",
      "test epoch: 2979, loss: 0.87\n",
      "train epoch: 2980, loss 0.0835\n",
      "test epoch: 2980, loss: 0.87\n",
      "train epoch: 2981, loss 0.0837\n",
      "test epoch: 2981, loss: 0.87\n",
      "train epoch: 2982, loss 0.0834\n",
      "test epoch: 2982, loss: 0.87\n",
      "train epoch: 2983, loss 0.0836\n",
      "test epoch: 2983, loss: 0.87\n",
      "train epoch: 2984, loss 0.0839\n",
      "test epoch: 2984, loss: 0.87\n",
      "train epoch: 2985, loss 0.0834\n",
      "test epoch: 2985, loss: 0.87\n",
      "train epoch: 2986, loss 0.0834\n",
      "test epoch: 2986, loss: 0.87\n",
      "train epoch: 2987, loss 0.0835\n",
      "test epoch: 2987, loss: 0.87\n",
      "train epoch: 2988, loss 0.0834\n",
      "test epoch: 2988, loss: 0.87\n",
      "train epoch: 2989, loss 0.0836\n",
      "test epoch: 2989, loss: 0.87\n",
      "train epoch: 2990, loss 0.0836\n",
      "test epoch: 2990, loss: 0.87\n",
      "train epoch: 2991, loss 0.0834\n",
      "test epoch: 2991, loss: 0.87\n",
      "train epoch: 2992, loss 0.0835\n",
      "test epoch: 2992, loss: 0.87\n",
      "train epoch: 2993, loss 0.0835\n",
      "test epoch: 2993, loss: 0.87\n",
      "train epoch: 2994, loss 0.0834\n",
      "test epoch: 2994, loss: 0.87\n",
      "train epoch: 2995, loss 0.0834\n",
      "test epoch: 2995, loss: 0.87\n",
      "train epoch: 2996, loss 0.0835\n",
      "test epoch: 2996, loss: 0.87\n",
      "train epoch: 2997, loss 0.0834\n",
      "test epoch: 2997, loss: 0.87\n",
      "train epoch: 2998, loss 0.0834\n",
      "test epoch: 2998, loss: 0.87\n",
      "train epoch: 2999, loss 0.0835\n",
      "test epoch: 2999, loss: 0.87\n",
      "train epoch: 3000, loss 0.0836\n",
      "test epoch: 3000, loss: 0.87\n",
      "train epoch: 3001, loss 0.0835\n",
      "test epoch: 3001, loss: 0.87\n",
      "train epoch: 3002, loss 0.0835\n",
      "test epoch: 3002, loss: 0.87\n",
      "train epoch: 3003, loss 0.0835\n",
      "test epoch: 3003, loss: 0.87\n",
      "train epoch: 3004, loss 0.0836\n",
      "test epoch: 3004, loss: 0.87\n",
      "train epoch: 3005, loss 0.0834\n",
      "test epoch: 3005, loss: 0.87\n",
      "train epoch: 3006, loss 0.0835\n",
      "test epoch: 3006, loss: 0.87\n",
      "train epoch: 3007, loss 0.0834\n",
      "test epoch: 3007, loss: 0.87\n",
      "train epoch: 3008, loss 0.0834\n",
      "test epoch: 3008, loss: 0.87\n",
      "train epoch: 3009, loss 0.0834\n",
      "test epoch: 3009, loss: 0.87\n",
      "train epoch: 3010, loss 0.0834\n",
      "test epoch: 3010, loss: 0.87\n",
      "train epoch: 3011, loss 0.0837\n",
      "test epoch: 3011, loss: 0.87\n",
      "train epoch: 3012, loss 0.0835\n",
      "test epoch: 3012, loss: 0.87\n",
      "train epoch: 3013, loss 0.0834\n",
      "test epoch: 3013, loss: 0.87\n",
      "train epoch: 3014, loss 0.0837\n",
      "test epoch: 3014, loss: 0.87\n",
      "train epoch: 3015, loss 0.0834\n",
      "test epoch: 3015, loss: 0.87\n",
      "train epoch: 3016, loss 0.0834\n",
      "test epoch: 3016, loss: 0.87\n",
      "train epoch: 3017, loss 0.0834\n",
      "test epoch: 3017, loss: 0.87\n",
      "train epoch: 3018, loss 0.0834\n",
      "test epoch: 3018, loss: 0.87\n",
      "train epoch: 3019, loss 0.0834\n",
      "test epoch: 3019, loss: 0.87\n",
      "train epoch: 3020, loss 0.0834\n",
      "test epoch: 3020, loss: 0.87\n",
      "train epoch: 3021, loss 0.0841\n",
      "test epoch: 3021, loss: 0.87\n",
      "train epoch: 3022, loss 0.0834\n",
      "test epoch: 3022, loss: 0.87\n",
      "train epoch: 3023, loss 0.0840\n",
      "test epoch: 3023, loss: 0.87\n",
      "train epoch: 3024, loss 0.0840\n",
      "test epoch: 3024, loss: 0.87\n",
      "train epoch: 3025, loss 0.0836\n",
      "test epoch: 3025, loss: 0.87\n",
      "train epoch: 3026, loss 0.0846\n",
      "test epoch: 3026, loss: 0.87\n",
      "train epoch: 3027, loss 0.0834\n",
      "test epoch: 3027, loss: 0.87\n",
      "train epoch: 3028, loss 0.0836\n",
      "test epoch: 3028, loss: 0.87\n",
      "train epoch: 3029, loss 0.0838\n",
      "test epoch: 3029, loss: 0.87\n",
      "train epoch: 3030, loss 0.0834\n",
      "test epoch: 3030, loss: 0.87\n",
      "train epoch: 3031, loss 0.0833\n",
      "test epoch: 3031, loss: 0.87\n",
      "train epoch: 3032, loss 0.0836\n",
      "test epoch: 3032, loss: 0.87\n",
      "train epoch: 3033, loss 0.0838\n",
      "test epoch: 3033, loss: 0.87\n",
      "train epoch: 3034, loss 0.0833\n",
      "test epoch: 3034, loss: 0.87\n",
      "train epoch: 3035, loss 0.0841\n",
      "test epoch: 3035, loss: 0.87\n",
      "train epoch: 3036, loss 0.0835\n",
      "test epoch: 3036, loss: 0.87\n",
      "train epoch: 3037, loss 0.0834\n",
      "test epoch: 3037, loss: 0.87\n",
      "train epoch: 3038, loss 0.0842\n",
      "test epoch: 3038, loss: 0.87\n",
      "train epoch: 3039, loss 0.0835\n",
      "test epoch: 3039, loss: 0.87\n",
      "train epoch: 3040, loss 0.0835\n",
      "test epoch: 3040, loss: 0.87\n",
      "train epoch: 3041, loss 0.0839\n",
      "test epoch: 3041, loss: 0.87\n",
      "train epoch: 3042, loss 0.0834\n",
      "test epoch: 3042, loss: 0.87\n",
      "train epoch: 3043, loss 0.0834\n",
      "test epoch: 3043, loss: 0.87\n",
      "train epoch: 3044, loss 0.0837\n",
      "test epoch: 3044, loss: 0.87\n",
      "train epoch: 3045, loss 0.0835\n",
      "test epoch: 3045, loss: 0.87\n",
      "train epoch: 3046, loss 0.0834\n",
      "test epoch: 3046, loss: 0.87\n",
      "train epoch: 3047, loss 0.0836\n",
      "test epoch: 3047, loss: 0.87\n",
      "train epoch: 3048, loss 0.0836\n",
      "test epoch: 3048, loss: 0.87\n",
      "train epoch: 3049, loss 0.0833\n",
      "test epoch: 3049, loss: 0.87\n",
      "train epoch: 3050, loss 0.0838\n",
      "test epoch: 3050, loss: 0.87\n",
      "train epoch: 3051, loss 0.0837\n",
      "test epoch: 3051, loss: 0.87\n",
      "train epoch: 3052, loss 0.0834\n",
      "test epoch: 3052, loss: 0.87\n",
      "train epoch: 3053, loss 0.0843\n",
      "test epoch: 3053, loss: 0.87\n",
      "train epoch: 3054, loss 0.0834\n",
      "test epoch: 3054, loss: 0.87\n",
      "train epoch: 3055, loss 0.0834\n",
      "test epoch: 3055, loss: 0.87\n",
      "train epoch: 3056, loss 0.0840\n",
      "test epoch: 3056, loss: 0.87\n",
      "train epoch: 3057, loss 0.0833\n",
      "test epoch: 3057, loss: 0.87\n",
      "train epoch: 3058, loss 0.0834\n",
      "test epoch: 3058, loss: 0.87\n",
      "train epoch: 3059, loss 0.0838\n",
      "test epoch: 3059, loss: 0.87\n",
      "train epoch: 3060, loss 0.0836\n",
      "test epoch: 3060, loss: 0.87\n",
      "train epoch: 3061, loss 0.0834\n",
      "test epoch: 3061, loss: 0.87\n",
      "train epoch: 3062, loss 0.0840\n",
      "test epoch: 3062, loss: 0.87\n",
      "train epoch: 3063, loss 0.0836\n",
      "test epoch: 3063, loss: 0.87\n",
      "train epoch: 3064, loss 0.0834\n",
      "test epoch: 3064, loss: 0.87\n",
      "train epoch: 3065, loss 0.0842\n",
      "test epoch: 3065, loss: 0.87\n",
      "train epoch: 3066, loss 0.0833\n",
      "test epoch: 3066, loss: 0.87\n",
      "train epoch: 3067, loss 0.0836\n",
      "test epoch: 3067, loss: 0.87\n",
      "train epoch: 3068, loss 0.0838\n",
      "test epoch: 3068, loss: 0.87\n",
      "train epoch: 3069, loss 0.0834\n",
      "test epoch: 3069, loss: 0.87\n",
      "train epoch: 3070, loss 0.0834\n",
      "test epoch: 3070, loss: 0.87\n",
      "train epoch: 3071, loss 0.0838\n",
      "test epoch: 3071, loss: 0.87\n",
      "train epoch: 3072, loss 0.0834\n",
      "test epoch: 3072, loss: 0.87\n",
      "train epoch: 3073, loss 0.0833\n",
      "test epoch: 3073, loss: 0.87\n",
      "train epoch: 3074, loss 0.0837\n",
      "test epoch: 3074, loss: 0.87\n",
      "train epoch: 3075, loss 0.0837\n",
      "test epoch: 3075, loss: 0.87\n",
      "train epoch: 3076, loss 0.0834\n",
      "test epoch: 3076, loss: 0.87\n",
      "train epoch: 3077, loss 0.0842\n",
      "test epoch: 3077, loss: 0.87\n",
      "train epoch: 3078, loss 0.0834\n",
      "test epoch: 3078, loss: 0.87\n",
      "train epoch: 3079, loss 0.0834\n",
      "test epoch: 3079, loss: 0.87\n",
      "train epoch: 3080, loss 0.0841\n",
      "test epoch: 3080, loss: 0.87\n",
      "train epoch: 3081, loss 0.0833\n",
      "test epoch: 3081, loss: 0.87\n",
      "train epoch: 3082, loss 0.0834\n",
      "test epoch: 3082, loss: 0.87\n",
      "train epoch: 3083, loss 0.0836\n",
      "test epoch: 3083, loss: 0.87\n",
      "train epoch: 3084, loss 0.0834\n",
      "test epoch: 3084, loss: 0.87\n",
      "train epoch: 3085, loss 0.0833\n",
      "test epoch: 3085, loss: 0.87\n",
      "train epoch: 3086, loss 0.0835\n",
      "test epoch: 3086, loss: 0.87\n",
      "train epoch: 3087, loss 0.0836\n",
      "test epoch: 3087, loss: 0.87\n",
      "train epoch: 3088, loss 0.0833\n",
      "test epoch: 3088, loss: 0.87\n",
      "train epoch: 3089, loss 0.0836\n",
      "test epoch: 3089, loss: 0.87\n",
      "train epoch: 3090, loss 0.0838\n",
      "test epoch: 3090, loss: 0.87\n",
      "train epoch: 3091, loss 0.0833\n",
      "test epoch: 3091, loss: 0.87\n",
      "train epoch: 3092, loss 0.0843\n",
      "test epoch: 3092, loss: 0.87\n",
      "train epoch: 3093, loss 0.0835\n",
      "test epoch: 3093, loss: 0.87\n",
      "train epoch: 3094, loss 0.0835\n",
      "test epoch: 3094, loss: 0.87\n",
      "train epoch: 3095, loss 0.0840\n",
      "test epoch: 3095, loss: 0.87\n",
      "train epoch: 3096, loss 0.0832\n",
      "test epoch: 3096, loss: 0.87\n",
      "train epoch: 3097, loss 0.0835\n",
      "test epoch: 3097, loss: 0.87\n",
      "train epoch: 3098, loss 0.0834\n",
      "test epoch: 3098, loss: 0.87\n",
      "train epoch: 3099, loss 0.0833\n",
      "test epoch: 3099, loss: 0.87\n",
      "train epoch: 3100, loss 0.0836\n",
      "test epoch: 3100, loss: 0.87\n",
      "train epoch: 3101, loss 0.0835\n",
      "test epoch: 3101, loss: 0.87\n",
      "train epoch: 3102, loss 0.0834\n",
      "test epoch: 3102, loss: 0.87\n",
      "train epoch: 3103, loss 0.0833\n",
      "test epoch: 3103, loss: 0.87\n",
      "train epoch: 3104, loss 0.0832\n",
      "test epoch: 3104, loss: 0.87\n",
      "train epoch: 3105, loss 0.0832\n",
      "test epoch: 3105, loss: 0.87\n",
      "train epoch: 3106, loss 0.0836\n",
      "test epoch: 3106, loss: 0.87\n",
      "train epoch: 3107, loss 0.0836\n",
      "test epoch: 3107, loss: 0.87\n",
      "train epoch: 3108, loss 0.0832\n",
      "test epoch: 3108, loss: 0.87\n",
      "train epoch: 3109, loss 0.0838\n",
      "test epoch: 3109, loss: 0.87\n",
      "train epoch: 3110, loss 0.0839\n",
      "test epoch: 3110, loss: 0.87\n",
      "train epoch: 3111, loss 0.0835\n",
      "test epoch: 3111, loss: 0.87\n",
      "train epoch: 3112, loss 0.0843\n",
      "test epoch: 3112, loss: 0.87\n",
      "train epoch: 3113, loss 0.0833\n",
      "test epoch: 3113, loss: 0.87\n",
      "train epoch: 3114, loss 0.0835\n",
      "test epoch: 3114, loss: 0.87\n",
      "train epoch: 3115, loss 0.0837\n",
      "test epoch: 3115, loss: 0.87\n",
      "train epoch: 3116, loss 0.0833\n",
      "test epoch: 3116, loss: 0.87\n",
      "train epoch: 3117, loss 0.0834\n",
      "test epoch: 3117, loss: 0.87\n",
      "train epoch: 3118, loss 0.0836\n",
      "test epoch: 3118, loss: 0.87\n",
      "train epoch: 3119, loss 0.0835\n",
      "test epoch: 3119, loss: 0.87\n",
      "train epoch: 3120, loss 0.0832\n",
      "test epoch: 3120, loss: 0.87\n",
      "train epoch: 3121, loss 0.0837\n",
      "test epoch: 3121, loss: 0.87\n",
      "train epoch: 3122, loss 0.0835\n",
      "test epoch: 3122, loss: 0.87\n",
      "train epoch: 3123, loss 0.0833\n",
      "test epoch: 3123, loss: 0.87\n",
      "train epoch: 3124, loss 0.0838\n",
      "test epoch: 3124, loss: 0.87\n",
      "train epoch: 3125, loss 0.0837\n",
      "test epoch: 3125, loss: 0.87\n",
      "train epoch: 3126, loss 0.0834\n",
      "test epoch: 3126, loss: 0.87\n",
      "train epoch: 3127, loss 0.0841\n",
      "test epoch: 3127, loss: 0.87\n",
      "train epoch: 3128, loss 0.0832\n",
      "test epoch: 3128, loss: 0.87\n",
      "train epoch: 3129, loss 0.0833\n",
      "test epoch: 3129, loss: 0.87\n",
      "train epoch: 3130, loss 0.0837\n",
      "test epoch: 3130, loss: 0.87\n",
      "train epoch: 3131, loss 0.0833\n",
      "test epoch: 3131, loss: 0.87\n",
      "train epoch: 3132, loss 0.0832\n",
      "test epoch: 3132, loss: 0.87\n",
      "train epoch: 3133, loss 0.0835\n",
      "test epoch: 3133, loss: 0.87\n",
      "train epoch: 3134, loss 0.0835\n",
      "test epoch: 3134, loss: 0.87\n",
      "train epoch: 3135, loss 0.0832\n",
      "test epoch: 3135, loss: 0.87\n",
      "train epoch: 3136, loss 0.0839\n",
      "test epoch: 3136, loss: 0.87\n",
      "train epoch: 3137, loss 0.0834\n",
      "test epoch: 3137, loss: 0.87\n",
      "train epoch: 3138, loss 0.0833\n",
      "test epoch: 3138, loss: 0.87\n",
      "train epoch: 3139, loss 0.0840\n",
      "test epoch: 3139, loss: 0.87\n",
      "train epoch: 3140, loss 0.0834\n",
      "test epoch: 3140, loss: 0.87\n",
      "train epoch: 3141, loss 0.0834\n",
      "test epoch: 3141, loss: 0.87\n",
      "train epoch: 3142, loss 0.0838\n",
      "test epoch: 3142, loss: 0.87\n",
      "train epoch: 3143, loss 0.0832\n",
      "test epoch: 3143, loss: 0.87\n",
      "train epoch: 3144, loss 0.0832\n",
      "test epoch: 3144, loss: 0.87\n",
      "train epoch: 3145, loss 0.0837\n",
      "test epoch: 3145, loss: 0.87\n",
      "train epoch: 3146, loss 0.0833\n",
      "test epoch: 3146, loss: 0.87\n",
      "train epoch: 3147, loss 0.0832\n",
      "test epoch: 3147, loss: 0.87\n",
      "train epoch: 3148, loss 0.0836\n",
      "test epoch: 3148, loss: 0.87\n",
      "train epoch: 3149, loss 0.0835\n",
      "test epoch: 3149, loss: 0.87\n",
      "train epoch: 3150, loss 0.0832\n",
      "test epoch: 3150, loss: 0.87\n",
      "train epoch: 3151, loss 0.0841\n",
      "test epoch: 3151, loss: 0.87\n",
      "train epoch: 3152, loss 0.0833\n",
      "test epoch: 3152, loss: 0.87\n",
      "train epoch: 3153, loss 0.0834\n",
      "test epoch: 3153, loss: 0.87\n",
      "train epoch: 3154, loss 0.0838\n",
      "test epoch: 3154, loss: 0.87\n",
      "train epoch: 3155, loss 0.0833\n",
      "test epoch: 3155, loss: 0.87\n",
      "train epoch: 3156, loss 0.0833\n",
      "test epoch: 3156, loss: 0.87\n",
      "train epoch: 3157, loss 0.0837\n",
      "test epoch: 3157, loss: 0.87\n",
      "train epoch: 3158, loss 0.0834\n",
      "test epoch: 3158, loss: 0.87\n",
      "train epoch: 3159, loss 0.0832\n",
      "test epoch: 3159, loss: 0.87\n",
      "train epoch: 3160, loss 0.0836\n",
      "test epoch: 3160, loss: 0.87\n",
      "train epoch: 3161, loss 0.0834\n",
      "test epoch: 3161, loss: 0.87\n",
      "train epoch: 3162, loss 0.0832\n",
      "test epoch: 3162, loss: 0.87\n",
      "train epoch: 3163, loss 0.0835\n",
      "test epoch: 3163, loss: 0.87\n",
      "train epoch: 3164, loss 0.0835\n",
      "test epoch: 3164, loss: 0.87\n",
      "train epoch: 3165, loss 0.0833\n",
      "test epoch: 3165, loss: 0.87\n",
      "train epoch: 3166, loss 0.0842\n",
      "test epoch: 3166, loss: 0.87\n",
      "train epoch: 3167, loss 0.0833\n",
      "test epoch: 3167, loss: 0.87\n",
      "train epoch: 3168, loss 0.0833\n",
      "test epoch: 3168, loss: 0.87\n",
      "train epoch: 3169, loss 0.0837\n",
      "test epoch: 3169, loss: 0.87\n",
      "train epoch: 3170, loss 0.0833\n",
      "test epoch: 3170, loss: 0.87\n",
      "train epoch: 3171, loss 0.0832\n",
      "test epoch: 3171, loss: 0.87\n",
      "train epoch: 3172, loss 0.0836\n",
      "test epoch: 3172, loss: 0.87\n",
      "train epoch: 3173, loss 0.0834\n",
      "test epoch: 3173, loss: 0.87\n",
      "train epoch: 3174, loss 0.0832\n",
      "test epoch: 3174, loss: 0.87\n",
      "train epoch: 3175, loss 0.0835\n",
      "test epoch: 3175, loss: 0.87\n",
      "train epoch: 3176, loss 0.0834\n",
      "test epoch: 3176, loss: 0.87\n",
      "train epoch: 3177, loss 0.0832\n",
      "test epoch: 3177, loss: 0.87\n",
      "train epoch: 3178, loss 0.0834\n",
      "test epoch: 3178, loss: 0.87\n",
      "train epoch: 3179, loss 0.0831\n",
      "test epoch: 3179, loss: 0.87\n",
      "train epoch: 3180, loss 0.0832\n",
      "test epoch: 3180, loss: 0.87\n",
      "train epoch: 3181, loss 0.0834\n",
      "test epoch: 3181, loss: 0.87\n",
      "train epoch: 3182, loss 0.0834\n",
      "test epoch: 3182, loss: 0.87\n",
      "train epoch: 3183, loss 0.0832\n",
      "test epoch: 3183, loss: 0.87\n",
      "train epoch: 3184, loss 0.0834\n",
      "test epoch: 3184, loss: 0.87\n",
      "train epoch: 3185, loss 0.0834\n",
      "test epoch: 3185, loss: 0.87\n",
      "train epoch: 3186, loss 0.0832\n",
      "test epoch: 3186, loss: 0.87\n",
      "train epoch: 3187, loss 0.0832\n",
      "test epoch: 3187, loss: 0.87\n",
      "train epoch: 3188, loss 0.0832\n",
      "test epoch: 3188, loss: 0.87\n",
      "train epoch: 3189, loss 0.0832\n",
      "test epoch: 3189, loss: 0.87\n",
      "train epoch: 3190, loss 0.0835\n",
      "test epoch: 3190, loss: 0.87\n",
      "train epoch: 3191, loss 0.0834\n",
      "test epoch: 3191, loss: 0.87\n",
      "train epoch: 3192, loss 0.0831\n",
      "test epoch: 3192, loss: 0.87\n",
      "train epoch: 3193, loss 0.0837\n",
      "test epoch: 3193, loss: 0.87\n",
      "train epoch: 3194, loss 0.0836\n",
      "test epoch: 3194, loss: 0.87\n",
      "train epoch: 3195, loss 0.0833\n",
      "test epoch: 3195, loss: 0.87\n",
      "train epoch: 3196, loss 0.0840\n",
      "test epoch: 3196, loss: 0.87\n",
      "train epoch: 3197, loss 0.0832\n",
      "test epoch: 3197, loss: 0.87\n",
      "train epoch: 3198, loss 0.0833\n",
      "test epoch: 3198, loss: 0.87\n",
      "train epoch: 3199, loss 0.0837\n",
      "test epoch: 3199, loss: 0.87\n",
      "train epoch: 3200, loss 0.0832\n",
      "test epoch: 3200, loss: 0.87\n",
      "train epoch: 3201, loss 0.0833\n",
      "test epoch: 3201, loss: 0.87\n",
      "train epoch: 3202, loss 0.0835\n",
      "test epoch: 3202, loss: 0.87\n",
      "train epoch: 3203, loss 0.0834\n",
      "test epoch: 3203, loss: 0.87\n",
      "train epoch: 3204, loss 0.0832\n",
      "test epoch: 3204, loss: 0.87\n",
      "train epoch: 3205, loss 0.0838\n",
      "test epoch: 3205, loss: 0.87\n",
      "train epoch: 3206, loss 0.0833\n",
      "test epoch: 3206, loss: 0.87\n",
      "train epoch: 3207, loss 0.0832\n",
      "test epoch: 3207, loss: 0.87\n",
      "train epoch: 3208, loss 0.0839\n",
      "test epoch: 3208, loss: 0.87\n",
      "train epoch: 3209, loss 0.0832\n",
      "test epoch: 3209, loss: 0.87\n",
      "train epoch: 3210, loss 0.0832\n",
      "test epoch: 3210, loss: 0.87\n",
      "train epoch: 3211, loss 0.0837\n",
      "test epoch: 3211, loss: 0.87\n",
      "train epoch: 3212, loss 0.0833\n",
      "test epoch: 3212, loss: 0.87\n",
      "train epoch: 3213, loss 0.0832\n",
      "test epoch: 3213, loss: 0.87\n",
      "train epoch: 3214, loss 0.0834\n",
      "test epoch: 3214, loss: 0.87\n",
      "train epoch: 3215, loss 0.0834\n",
      "test epoch: 3215, loss: 0.87\n",
      "train epoch: 3216, loss 0.0832\n",
      "test epoch: 3216, loss: 0.87\n",
      "train epoch: 3217, loss 0.0836\n",
      "test epoch: 3217, loss: 0.87\n",
      "train epoch: 3218, loss 0.0834\n",
      "test epoch: 3218, loss: 0.87\n",
      "train epoch: 3219, loss 0.0832\n",
      "test epoch: 3219, loss: 0.87\n",
      "train epoch: 3220, loss 0.0841\n",
      "test epoch: 3220, loss: 0.87\n",
      "train epoch: 3221, loss 0.0832\n",
      "test epoch: 3221, loss: 0.87\n",
      "train epoch: 3222, loss 0.0833\n",
      "test epoch: 3222, loss: 0.87\n",
      "train epoch: 3223, loss 0.0837\n",
      "test epoch: 3223, loss: 0.87\n",
      "train epoch: 3224, loss 0.0832\n",
      "test epoch: 3224, loss: 0.87\n",
      "train epoch: 3225, loss 0.0832\n",
      "test epoch: 3225, loss: 0.87\n",
      "train epoch: 3226, loss 0.0835\n",
      "test epoch: 3226, loss: 0.87\n",
      "train epoch: 3227, loss 0.0833\n",
      "test epoch: 3227, loss: 0.87\n",
      "train epoch: 3228, loss 0.0831\n",
      "test epoch: 3228, loss: 0.87\n",
      "train epoch: 3229, loss 0.0834\n",
      "test epoch: 3229, loss: 0.87\n",
      "train epoch: 3230, loss 0.0834\n",
      "test epoch: 3230, loss: 0.87\n",
      "train epoch: 3231, loss 0.0831\n",
      "test epoch: 3231, loss: 0.87\n",
      "train epoch: 3232, loss 0.0835\n",
      "test epoch: 3232, loss: 0.87\n",
      "train epoch: 3233, loss 0.0835\n",
      "test epoch: 3233, loss: 0.87\n",
      "train epoch: 3234, loss 0.0832\n",
      "test epoch: 3234, loss: 0.87\n",
      "train epoch: 3235, loss 0.0841\n",
      "test epoch: 3235, loss: 0.87\n",
      "train epoch: 3236, loss 0.0832\n",
      "test epoch: 3236, loss: 0.87\n",
      "train epoch: 3237, loss 0.0833\n",
      "test epoch: 3237, loss: 0.87\n",
      "train epoch: 3238, loss 0.0837\n",
      "test epoch: 3238, loss: 0.87\n",
      "train epoch: 3239, loss 0.0831\n",
      "test epoch: 3239, loss: 0.87\n",
      "train epoch: 3240, loss 0.0831\n",
      "test epoch: 3240, loss: 0.87\n",
      "train epoch: 3241, loss 0.0835\n",
      "test epoch: 3241, loss: 0.87\n",
      "train epoch: 3242, loss 0.0834\n",
      "test epoch: 3242, loss: 0.87\n",
      "train epoch: 3243, loss 0.0831\n",
      "test epoch: 3243, loss: 0.87\n",
      "train epoch: 3244, loss 0.0840\n",
      "test epoch: 3244, loss: 0.87\n",
      "train epoch: 3245, loss 0.0833\n",
      "test epoch: 3245, loss: 0.87\n",
      "train epoch: 3246, loss 0.0833\n",
      "test epoch: 3246, loss: 0.87\n",
      "train epoch: 3247, loss 0.0838\n",
      "test epoch: 3247, loss: 0.87\n",
      "train epoch: 3248, loss 0.0832\n",
      "test epoch: 3248, loss: 0.87\n",
      "train epoch: 3249, loss 0.0832\n",
      "test epoch: 3249, loss: 0.87\n",
      "train epoch: 3250, loss 0.0835\n",
      "test epoch: 3250, loss: 0.87\n",
      "train epoch: 3251, loss 0.0833\n",
      "test epoch: 3251, loss: 0.87\n",
      "train epoch: 3252, loss 0.0831\n",
      "test epoch: 3252, loss: 0.87\n",
      "train epoch: 3253, loss 0.0835\n",
      "test epoch: 3253, loss: 0.87\n",
      "train epoch: 3254, loss 0.0833\n",
      "test epoch: 3254, loss: 0.87\n",
      "train epoch: 3255, loss 0.0831\n",
      "test epoch: 3255, loss: 0.87\n",
      "train epoch: 3256, loss 0.0839\n",
      "test epoch: 3256, loss: 0.87\n",
      "train epoch: 3257, loss 0.0833\n",
      "test epoch: 3257, loss: 0.87\n",
      "train epoch: 3258, loss 0.0832\n",
      "test epoch: 3258, loss: 0.87\n",
      "train epoch: 3259, loss 0.0839\n",
      "test epoch: 3259, loss: 0.87\n",
      "train epoch: 3260, loss 0.0831\n",
      "test epoch: 3260, loss: 0.87\n",
      "train epoch: 3261, loss 0.0834\n",
      "test epoch: 3261, loss: 0.87\n",
      "train epoch: 3262, loss 0.0833\n",
      "test epoch: 3262, loss: 0.87\n",
      "train epoch: 3263, loss 0.0832\n",
      "test epoch: 3263, loss: 0.87\n",
      "train epoch: 3264, loss 0.0841\n",
      "test epoch: 3264, loss: 0.87\n",
      "train epoch: 3265, loss 0.0831\n",
      "test epoch: 3265, loss: 0.87\n",
      "train epoch: 3266, loss 0.0849\n",
      "test epoch: 3266, loss: 0.87\n",
      "train epoch: 3267, loss 0.0832\n",
      "test epoch: 3267, loss: 0.87\n",
      "train epoch: 3268, loss 0.0837\n",
      "test epoch: 3268, loss: 0.87\n",
      "train epoch: 3269, loss 0.0837\n",
      "test epoch: 3269, loss: 0.87\n",
      "train epoch: 3270, loss 0.0831\n",
      "test epoch: 3270, loss: 0.87\n",
      "train epoch: 3271, loss 0.0835\n",
      "test epoch: 3271, loss: 0.87\n",
      "train epoch: 3272, loss 0.0831\n",
      "test epoch: 3272, loss: 0.87\n",
      "train epoch: 3273, loss 0.0834\n",
      "test epoch: 3273, loss: 0.87\n",
      "train epoch: 3274, loss 0.0836\n",
      "test epoch: 3274, loss: 0.87\n",
      "train epoch: 3275, loss 0.0832\n",
      "test epoch: 3275, loss: 0.87\n",
      "train epoch: 3276, loss 0.0843\n",
      "test epoch: 3276, loss: 0.87\n",
      "train epoch: 3277, loss 0.0832\n",
      "test epoch: 3277, loss: 0.87\n",
      "train epoch: 3278, loss 0.0834\n",
      "test epoch: 3278, loss: 0.87\n",
      "train epoch: 3279, loss 0.0834\n",
      "test epoch: 3279, loss: 0.87\n",
      "train epoch: 3280, loss 0.0832\n",
      "test epoch: 3280, loss: 0.87\n",
      "train epoch: 3281, loss 0.0831\n",
      "test epoch: 3281, loss: 0.87\n",
      "train epoch: 3282, loss 0.0834\n",
      "test epoch: 3282, loss: 0.87\n",
      "train epoch: 3283, loss 0.0834\n",
      "test epoch: 3283, loss: 0.87\n",
      "train epoch: 3284, loss 0.0831\n",
      "test epoch: 3284, loss: 0.87\n",
      "train epoch: 3285, loss 0.0839\n",
      "test epoch: 3285, loss: 0.87\n",
      "train epoch: 3286, loss 0.0833\n",
      "test epoch: 3286, loss: 0.87\n",
      "train epoch: 3287, loss 0.0833\n",
      "test epoch: 3287, loss: 0.87\n",
      "train epoch: 3288, loss 0.0839\n",
      "test epoch: 3288, loss: 0.87\n",
      "train epoch: 3289, loss 0.0834\n",
      "test epoch: 3289, loss: 0.87\n",
      "train epoch: 3290, loss 0.0831\n",
      "test epoch: 3290, loss: 0.87\n",
      "train epoch: 3291, loss 0.0840\n",
      "test epoch: 3291, loss: 0.87\n",
      "train epoch: 3292, loss 0.0831\n",
      "test epoch: 3292, loss: 0.87\n",
      "train epoch: 3293, loss 0.0832\n",
      "test epoch: 3293, loss: 0.87\n",
      "train epoch: 3294, loss 0.0835\n",
      "test epoch: 3294, loss: 0.87\n",
      "train epoch: 3295, loss 0.0832\n",
      "test epoch: 3295, loss: 0.87\n",
      "train epoch: 3296, loss 0.0831\n",
      "test epoch: 3296, loss: 0.87\n",
      "train epoch: 3297, loss 0.0834\n",
      "test epoch: 3297, loss: 0.87\n",
      "train epoch: 3298, loss 0.0832\n",
      "test epoch: 3298, loss: 0.87\n",
      "train epoch: 3299, loss 0.0830\n",
      "test epoch: 3299, loss: 0.87\n",
      "train epoch: 3300, loss 0.0832\n",
      "test epoch: 3300, loss: 0.87\n",
      "train epoch: 3301, loss 0.0834\n",
      "test epoch: 3301, loss: 0.87\n",
      "train epoch: 3302, loss 0.0831\n",
      "test epoch: 3302, loss: 0.87\n",
      "train epoch: 3303, loss 0.0838\n",
      "test epoch: 3303, loss: 0.87\n",
      "train epoch: 3304, loss 0.0833\n",
      "test epoch: 3304, loss: 0.87\n",
      "train epoch: 3305, loss 0.0833\n",
      "test epoch: 3305, loss: 0.87\n",
      "train epoch: 3306, loss 0.0839\n",
      "test epoch: 3306, loss: 0.87\n",
      "train epoch: 3307, loss 0.0831\n",
      "test epoch: 3307, loss: 0.87\n",
      "train epoch: 3308, loss 0.0832\n",
      "test epoch: 3308, loss: 0.87\n",
      "train epoch: 3309, loss 0.0834\n",
      "test epoch: 3309, loss: 0.87\n",
      "train epoch: 3310, loss 0.0832\n",
      "test epoch: 3310, loss: 0.87\n",
      "train epoch: 3311, loss 0.0831\n",
      "test epoch: 3311, loss: 0.87\n",
      "train epoch: 3312, loss 0.0834\n",
      "test epoch: 3312, loss: 0.87\n",
      "train epoch: 3313, loss 0.0834\n",
      "test epoch: 3313, loss: 0.87\n",
      "train epoch: 3314, loss 0.0830\n",
      "test epoch: 3314, loss: 0.87\n",
      "train epoch: 3315, loss 0.0838\n",
      "test epoch: 3315, loss: 0.87\n",
      "train epoch: 3316, loss 0.0833\n",
      "test epoch: 3316, loss: 0.87\n",
      "train epoch: 3317, loss 0.0832\n",
      "test epoch: 3317, loss: 0.87\n",
      "train epoch: 3318, loss 0.0837\n",
      "test epoch: 3318, loss: 0.87\n",
      "train epoch: 3319, loss 0.0831\n",
      "test epoch: 3319, loss: 0.87\n",
      "train epoch: 3320, loss 0.0831\n",
      "test epoch: 3320, loss: 0.87\n",
      "train epoch: 3321, loss 0.0836\n",
      "test epoch: 3321, loss: 0.87\n",
      "train epoch: 3322, loss 0.0831\n",
      "test epoch: 3322, loss: 0.87\n",
      "train epoch: 3323, loss 0.0831\n",
      "test epoch: 3323, loss: 0.87\n",
      "train epoch: 3324, loss 0.0833\n",
      "test epoch: 3324, loss: 0.87\n",
      "train epoch: 3325, loss 0.0833\n",
      "test epoch: 3325, loss: 0.87\n",
      "train epoch: 3326, loss 0.0831\n",
      "test epoch: 3326, loss: 0.87\n",
      "train epoch: 3327, loss 0.0835\n",
      "test epoch: 3327, loss: 0.87\n",
      "train epoch: 3328, loss 0.0833\n",
      "test epoch: 3328, loss: 0.87\n",
      "train epoch: 3329, loss 0.0831\n",
      "test epoch: 3329, loss: 0.87\n",
      "train epoch: 3330, loss 0.0839\n",
      "test epoch: 3330, loss: 0.87\n",
      "train epoch: 3331, loss 0.0831\n",
      "test epoch: 3331, loss: 0.87\n",
      "train epoch: 3332, loss 0.0832\n",
      "test epoch: 3332, loss: 0.87\n",
      "train epoch: 3333, loss 0.0836\n",
      "test epoch: 3333, loss: 0.87\n",
      "train epoch: 3334, loss 0.0831\n",
      "test epoch: 3334, loss: 0.87\n",
      "train epoch: 3335, loss 0.0831\n",
      "test epoch: 3335, loss: 0.87\n",
      "train epoch: 3336, loss 0.0834\n",
      "test epoch: 3336, loss: 0.87\n",
      "train epoch: 3337, loss 0.0832\n",
      "test epoch: 3337, loss: 0.87\n",
      "train epoch: 3338, loss 0.0831\n",
      "test epoch: 3338, loss: 0.87\n",
      "train epoch: 3339, loss 0.0833\n",
      "test epoch: 3339, loss: 0.87\n",
      "train epoch: 3340, loss 0.0833\n",
      "test epoch: 3340, loss: 0.87\n",
      "train epoch: 3341, loss 0.0830\n",
      "test epoch: 3341, loss: 0.87\n",
      "train epoch: 3342, loss 0.0839\n",
      "test epoch: 3342, loss: 0.87\n",
      "train epoch: 3343, loss 0.0832\n",
      "test epoch: 3343, loss: 0.87\n",
      "train epoch: 3344, loss 0.0832\n",
      "test epoch: 3344, loss: 0.87\n",
      "train epoch: 3345, loss 0.0836\n",
      "test epoch: 3345, loss: 0.87\n",
      "train epoch: 3346, loss 0.0830\n",
      "test epoch: 3346, loss: 0.87\n",
      "train epoch: 3347, loss 0.0830\n",
      "test epoch: 3347, loss: 0.87\n",
      "train epoch: 3348, loss 0.0835\n",
      "test epoch: 3348, loss: 0.87\n",
      "train epoch: 3349, loss 0.0831\n",
      "test epoch: 3349, loss: 0.87\n",
      "train epoch: 3350, loss 0.0830\n",
      "test epoch: 3350, loss: 0.87\n",
      "train epoch: 3351, loss 0.0835\n",
      "test epoch: 3351, loss: 0.87\n",
      "train epoch: 3352, loss 0.0832\n",
      "test epoch: 3352, loss: 0.87\n",
      "train epoch: 3353, loss 0.0831\n",
      "test epoch: 3353, loss: 0.87\n",
      "train epoch: 3354, loss 0.0837\n",
      "test epoch: 3354, loss: 0.87\n",
      "train epoch: 3355, loss 0.0830\n",
      "test epoch: 3355, loss: 0.87\n",
      "train epoch: 3356, loss 0.0830\n",
      "test epoch: 3356, loss: 0.87\n",
      "train epoch: 3357, loss 0.0833\n",
      "test epoch: 3357, loss: 0.87\n",
      "train epoch: 3358, loss 0.0831\n",
      "test epoch: 3358, loss: 0.87\n",
      "train epoch: 3359, loss 0.0830\n",
      "test epoch: 3359, loss: 0.87\n",
      "train epoch: 3360, loss 0.0831\n",
      "test epoch: 3360, loss: 0.87\n",
      "train epoch: 3361, loss 0.0834\n",
      "test epoch: 3361, loss: 0.87\n",
      "train epoch: 3362, loss 0.0830\n",
      "test epoch: 3362, loss: 0.87\n",
      "train epoch: 3363, loss 0.0837\n",
      "test epoch: 3363, loss: 0.87\n",
      "train epoch: 3364, loss 0.0833\n",
      "test epoch: 3364, loss: 0.87\n",
      "train epoch: 3365, loss 0.0831\n",
      "test epoch: 3365, loss: 0.87\n",
      "train epoch: 3366, loss 0.0838\n",
      "test epoch: 3366, loss: 0.87\n",
      "train epoch: 3367, loss 0.0831\n",
      "test epoch: 3367, loss: 0.87\n",
      "train epoch: 3368, loss 0.0832\n",
      "test epoch: 3368, loss: 0.87\n",
      "train epoch: 3369, loss 0.0834\n",
      "test epoch: 3369, loss: 0.87\n",
      "train epoch: 3370, loss 0.0831\n",
      "test epoch: 3370, loss: 0.87\n",
      "train epoch: 3371, loss 0.0830\n",
      "test epoch: 3371, loss: 0.87\n",
      "train epoch: 3372, loss 0.0833\n",
      "test epoch: 3372, loss: 0.87\n",
      "train epoch: 3373, loss 0.0833\n",
      "test epoch: 3373, loss: 0.87\n",
      "train epoch: 3374, loss 0.0830\n",
      "test epoch: 3374, loss: 0.87\n",
      "train epoch: 3375, loss 0.0836\n",
      "test epoch: 3375, loss: 0.87\n",
      "train epoch: 3376, loss 0.0833\n",
      "test epoch: 3376, loss: 0.87\n",
      "train epoch: 3377, loss 0.0832\n",
      "test epoch: 3377, loss: 0.87\n",
      "train epoch: 3378, loss 0.0838\n",
      "test epoch: 3378, loss: 0.87\n",
      "train epoch: 3379, loss 0.0830\n",
      "test epoch: 3379, loss: 0.87\n",
      "train epoch: 3380, loss 0.0834\n",
      "test epoch: 3380, loss: 0.87\n",
      "train epoch: 3381, loss 0.0832\n",
      "test epoch: 3381, loss: 0.87\n",
      "train epoch: 3382, loss 0.0830\n",
      "test epoch: 3382, loss: 0.87\n",
      "train epoch: 3383, loss 0.0831\n",
      "test epoch: 3383, loss: 0.87\n",
      "train epoch: 3384, loss 0.0832\n",
      "test epoch: 3384, loss: 0.87\n",
      "train epoch: 3385, loss 0.0832\n",
      "test epoch: 3385, loss: 0.87\n",
      "train epoch: 3386, loss 0.0830\n",
      "test epoch: 3386, loss: 0.87\n",
      "train epoch: 3387, loss 0.0836\n",
      "test epoch: 3387, loss: 0.87\n",
      "train epoch: 3388, loss 0.0832\n",
      "test epoch: 3388, loss: 0.87\n",
      "train epoch: 3389, loss 0.0831\n",
      "test epoch: 3389, loss: 0.87\n",
      "train epoch: 3390, loss 0.0837\n",
      "test epoch: 3390, loss: 0.87\n",
      "train epoch: 3391, loss 0.0832\n",
      "test epoch: 3391, loss: 0.87\n",
      "train epoch: 3392, loss 0.0831\n",
      "test epoch: 3392, loss: 0.87\n",
      "train epoch: 3393, loss 0.0836\n",
      "test epoch: 3393, loss: 0.87\n",
      "train epoch: 3394, loss 0.0831\n",
      "test epoch: 3394, loss: 0.87\n",
      "train epoch: 3395, loss 0.0830\n",
      "test epoch: 3395, loss: 0.87\n",
      "train epoch: 3396, loss 0.0835\n",
      "test epoch: 3396, loss: 0.87\n",
      "train epoch: 3397, loss 0.0832\n",
      "test epoch: 3397, loss: 0.87\n",
      "train epoch: 3398, loss 0.0831\n",
      "test epoch: 3398, loss: 0.87\n",
      "train epoch: 3399, loss 0.0836\n",
      "test epoch: 3399, loss: 0.87\n",
      "train epoch: 3400, loss 0.0832\n",
      "test epoch: 3400, loss: 0.87\n",
      "train epoch: 3401, loss 0.0831\n",
      "test epoch: 3401, loss: 0.87\n",
      "train epoch: 3402, loss 0.0837\n",
      "test epoch: 3402, loss: 0.87\n",
      "train epoch: 3403, loss 0.0830\n",
      "test epoch: 3403, loss: 0.87\n",
      "train epoch: 3404, loss 0.0832\n",
      "test epoch: 3404, loss: 0.87\n",
      "train epoch: 3405, loss 0.0835\n",
      "test epoch: 3405, loss: 0.87\n",
      "train epoch: 3406, loss 0.0830\n",
      "test epoch: 3406, loss: 0.87\n",
      "train epoch: 3407, loss 0.0832\n",
      "test epoch: 3407, loss: 0.87\n",
      "train epoch: 3408, loss 0.0833\n",
      "test epoch: 3408, loss: 0.87\n",
      "train epoch: 3409, loss 0.0831\n",
      "test epoch: 3409, loss: 0.87\n",
      "train epoch: 3410, loss 0.0830\n",
      "test epoch: 3410, loss: 0.87\n",
      "train epoch: 3411, loss 0.0830\n",
      "test epoch: 3411, loss: 0.87\n",
      "train epoch: 3412, loss 0.0833\n",
      "test epoch: 3412, loss: 0.87\n",
      "train epoch: 3413, loss 0.0830\n",
      "test epoch: 3413, loss: 0.87\n",
      "train epoch: 3414, loss 0.0831\n",
      "test epoch: 3414, loss: 0.87\n",
      "train epoch: 3415, loss 0.0830\n",
      "test epoch: 3415, loss: 0.87\n",
      "train epoch: 3416, loss 0.0830\n",
      "test epoch: 3416, loss: 0.87\n",
      "train epoch: 3417, loss 0.0832\n",
      "test epoch: 3417, loss: 0.87\n",
      "train epoch: 3418, loss 0.0831\n",
      "test epoch: 3418, loss: 0.87\n",
      "train epoch: 3419, loss 0.0830\n",
      "test epoch: 3419, loss: 0.87\n",
      "train epoch: 3420, loss 0.0832\n",
      "test epoch: 3420, loss: 0.87\n",
      "train epoch: 3421, loss 0.0831\n",
      "test epoch: 3421, loss: 0.87\n",
      "train epoch: 3422, loss 0.0831\n",
      "test epoch: 3422, loss: 0.87\n",
      "train epoch: 3423, loss 0.0830\n",
      "test epoch: 3423, loss: 0.87\n",
      "train epoch: 3424, loss 0.0831\n",
      "test epoch: 3424, loss: 0.87\n",
      "train epoch: 3425, loss 0.0832\n",
      "test epoch: 3425, loss: 0.87\n",
      "train epoch: 3426, loss 0.0830\n",
      "test epoch: 3426, loss: 0.87\n",
      "train epoch: 3427, loss 0.0833\n",
      "test epoch: 3427, loss: 0.87\n",
      "train epoch: 3428, loss 0.0835\n",
      "test epoch: 3428, loss: 0.87\n",
      "train epoch: 3429, loss 0.0831\n",
      "test epoch: 3429, loss: 0.87\n",
      "train epoch: 3430, loss 0.0839\n",
      "test epoch: 3430, loss: 0.87\n",
      "train epoch: 3431, loss 0.0832\n",
      "test epoch: 3431, loss: 0.87\n",
      "train epoch: 3432, loss 0.0833\n",
      "test epoch: 3432, loss: 0.87\n",
      "train epoch: 3433, loss 0.0838\n",
      "test epoch: 3433, loss: 0.87\n",
      "train epoch: 3434, loss 0.0830\n",
      "test epoch: 3434, loss: 0.87\n",
      "train epoch: 3435, loss 0.0834\n",
      "test epoch: 3435, loss: 0.87\n",
      "train epoch: 3436, loss 0.0830\n",
      "test epoch: 3436, loss: 0.87\n",
      "train epoch: 3437, loss 0.0831\n",
      "test epoch: 3437, loss: 0.87\n",
      "train epoch: 3438, loss 0.0832\n",
      "test epoch: 3438, loss: 0.87\n",
      "train epoch: 3439, loss 0.0830\n",
      "test epoch: 3439, loss: 0.87\n",
      "train epoch: 3440, loss 0.0831\n",
      "test epoch: 3440, loss: 0.87\n",
      "train epoch: 3441, loss 0.0832\n",
      "test epoch: 3441, loss: 0.87\n",
      "train epoch: 3442, loss 0.0830\n",
      "test epoch: 3442, loss: 0.87\n",
      "train epoch: 3443, loss 0.0831\n",
      "test epoch: 3443, loss: 0.87\n",
      "train epoch: 3444, loss 0.0833\n",
      "test epoch: 3444, loss: 0.87\n",
      "train epoch: 3445, loss 0.0832\n",
      "test epoch: 3445, loss: 0.87\n",
      "train epoch: 3446, loss 0.0830\n",
      "test epoch: 3446, loss: 0.87\n",
      "train epoch: 3447, loss 0.0831\n",
      "test epoch: 3447, loss: 0.87\n",
      "train epoch: 3448, loss 0.0831\n",
      "test epoch: 3448, loss: 0.87\n",
      "train epoch: 3449, loss 0.0831\n",
      "test epoch: 3449, loss: 0.87\n",
      "train epoch: 3450, loss 0.0833\n",
      "test epoch: 3450, loss: 0.87\n",
      "train epoch: 3451, loss 0.0831\n",
      "test epoch: 3451, loss: 0.87\n",
      "train epoch: 3452, loss 0.0831\n",
      "test epoch: 3452, loss: 0.87\n",
      "train epoch: 3453, loss 0.0832\n",
      "test epoch: 3453, loss: 0.87\n",
      "train epoch: 3454, loss 0.0832\n",
      "test epoch: 3454, loss: 0.87\n",
      "train epoch: 3455, loss 0.0831\n",
      "test epoch: 3455, loss: 0.87\n",
      "train epoch: 3456, loss 0.0830\n",
      "test epoch: 3456, loss: 0.87\n",
      "train epoch: 3457, loss 0.0831\n",
      "test epoch: 3457, loss: 0.87\n",
      "train epoch: 3458, loss 0.0831\n",
      "test epoch: 3458, loss: 0.87\n",
      "train epoch: 3459, loss 0.0833\n",
      "test epoch: 3459, loss: 0.87\n",
      "train epoch: 3460, loss 0.0831\n",
      "test epoch: 3460, loss: 0.87\n",
      "train epoch: 3461, loss 0.0831\n",
      "test epoch: 3461, loss: 0.87\n",
      "train epoch: 3462, loss 0.0829\n",
      "test epoch: 3462, loss: 0.87\n",
      "train epoch: 3463, loss 0.0831\n",
      "test epoch: 3463, loss: 0.87\n",
      "train epoch: 3464, loss 0.0832\n",
      "test epoch: 3464, loss: 0.87\n",
      "train epoch: 3465, loss 0.0832\n",
      "test epoch: 3465, loss: 0.87\n",
      "train epoch: 3466, loss 0.0830\n",
      "test epoch: 3466, loss: 0.87\n",
      "train epoch: 3467, loss 0.0832\n",
      "test epoch: 3467, loss: 0.87\n",
      "train epoch: 3468, loss 0.0830\n",
      "test epoch: 3468, loss: 0.87\n",
      "train epoch: 3469, loss 0.0830\n",
      "test epoch: 3469, loss: 0.87\n",
      "train epoch: 3470, loss 0.0829\n",
      "test epoch: 3470, loss: 0.87\n",
      "train epoch: 3471, loss 0.0830\n",
      "test epoch: 3471, loss: 0.87\n",
      "train epoch: 3472, loss 0.0830\n",
      "test epoch: 3472, loss: 0.87\n",
      "train epoch: 3473, loss 0.0830\n",
      "test epoch: 3473, loss: 0.87\n",
      "train epoch: 3474, loss 0.0830\n",
      "test epoch: 3474, loss: 0.87\n",
      "train epoch: 3475, loss 0.0834\n",
      "test epoch: 3475, loss: 0.87\n",
      "train epoch: 3476, loss 0.0830\n",
      "test epoch: 3476, loss: 0.87\n",
      "train epoch: 3477, loss 0.0833\n",
      "test epoch: 3477, loss: 0.87\n",
      "train epoch: 3478, loss 0.0837\n",
      "test epoch: 3478, loss: 0.87\n",
      "train epoch: 3479, loss 0.0832\n",
      "test epoch: 3479, loss: 0.87\n",
      "train epoch: 3480, loss 0.0843\n",
      "test epoch: 3480, loss: 0.87\n",
      "train epoch: 3481, loss 0.0830\n",
      "test epoch: 3481, loss: 0.87\n",
      "train epoch: 3482, loss 0.0836\n",
      "test epoch: 3482, loss: 0.87\n",
      "train epoch: 3483, loss 0.0832\n",
      "test epoch: 3483, loss: 0.87\n",
      "train epoch: 3484, loss 0.0830\n",
      "test epoch: 3484, loss: 0.87\n",
      "train epoch: 3485, loss 0.0830\n",
      "test epoch: 3485, loss: 0.87\n",
      "train epoch: 3486, loss 0.0830\n",
      "test epoch: 3486, loss: 0.87\n",
      "train epoch: 3487, loss 0.0829\n",
      "test epoch: 3487, loss: 0.87\n",
      "train epoch: 3488, loss 0.0836\n",
      "test epoch: 3488, loss: 0.87\n",
      "train epoch: 3489, loss 0.0830\n",
      "test epoch: 3489, loss: 0.87\n",
      "train epoch: 3490, loss 0.0833\n",
      "test epoch: 3490, loss: 0.87\n",
      "train epoch: 3491, loss 0.0836\n",
      "test epoch: 3491, loss: 0.87\n",
      "train epoch: 3492, loss 0.0831\n",
      "test epoch: 3492, loss: 0.87\n",
      "train epoch: 3493, loss 0.0842\n",
      "test epoch: 3493, loss: 0.87\n",
      "train epoch: 3494, loss 0.0830\n",
      "test epoch: 3494, loss: 0.87\n",
      "train epoch: 3495, loss 0.0834\n",
      "test epoch: 3495, loss: 0.87\n",
      "train epoch: 3496, loss 0.0834\n",
      "test epoch: 3496, loss: 0.87\n",
      "train epoch: 3497, loss 0.0829\n",
      "test epoch: 3497, loss: 0.87\n",
      "train epoch: 3498, loss 0.0831\n",
      "test epoch: 3498, loss: 0.87\n",
      "train epoch: 3499, loss 0.0830\n",
      "test epoch: 3499, loss: 0.87\n",
      "train epoch: 3500, loss 0.0829\n",
      "test epoch: 3500, loss: 0.87\n",
      "train epoch: 3501, loss 0.0830\n",
      "test epoch: 3501, loss: 0.87\n",
      "train epoch: 3502, loss 0.0832\n",
      "test epoch: 3502, loss: 0.87\n",
      "train epoch: 3503, loss 0.0831\n",
      "test epoch: 3503, loss: 0.87\n",
      "train epoch: 3504, loss 0.0830\n",
      "test epoch: 3504, loss: 0.87\n",
      "train epoch: 3505, loss 0.0830\n",
      "test epoch: 3505, loss: 0.87\n",
      "train epoch: 3506, loss 0.0830\n",
      "test epoch: 3506, loss: 0.87\n",
      "train epoch: 3507, loss 0.0831\n",
      "test epoch: 3507, loss: 0.87\n",
      "train epoch: 3508, loss 0.0831\n",
      "test epoch: 3508, loss: 0.87\n",
      "train epoch: 3509, loss 0.0830\n",
      "test epoch: 3509, loss: 0.87\n",
      "train epoch: 3510, loss 0.0830\n",
      "test epoch: 3510, loss: 0.87\n",
      "train epoch: 3511, loss 0.0832\n",
      "test epoch: 3511, loss: 0.87\n",
      "train epoch: 3512, loss 0.0829\n",
      "test epoch: 3512, loss: 0.87\n",
      "train epoch: 3513, loss 0.0831\n",
      "test epoch: 3513, loss: 0.87\n",
      "train epoch: 3514, loss 0.0835\n",
      "test epoch: 3514, loss: 0.87\n",
      "train epoch: 3515, loss 0.0831\n",
      "test epoch: 3515, loss: 0.87\n",
      "train epoch: 3516, loss 0.0840\n",
      "test epoch: 3516, loss: 0.87\n",
      "train epoch: 3517, loss 0.0833\n",
      "test epoch: 3517, loss: 0.87\n",
      "train epoch: 3518, loss 0.0833\n",
      "test epoch: 3518, loss: 0.87\n",
      "train epoch: 3519, loss 0.0839\n",
      "test epoch: 3519, loss: 0.87\n",
      "train epoch: 3520, loss 0.0829\n",
      "test epoch: 3520, loss: 0.87\n",
      "train epoch: 3521, loss 0.0836\n",
      "test epoch: 3521, loss: 0.87\n",
      "train epoch: 3522, loss 0.0830\n",
      "test epoch: 3522, loss: 0.87\n",
      "train epoch: 3523, loss 0.0836\n",
      "test epoch: 3523, loss: 0.87\n",
      "train epoch: 3524, loss 0.0834\n",
      "test epoch: 3524, loss: 0.87\n",
      "train epoch: 3525, loss 0.0831\n",
      "test epoch: 3525, loss: 0.87\n",
      "train epoch: 3526, loss 0.0840\n",
      "test epoch: 3526, loss: 0.87\n",
      "train epoch: 3527, loss 0.0829\n",
      "test epoch: 3527, loss: 0.87\n",
      "train epoch: 3528, loss 0.0833\n",
      "test epoch: 3528, loss: 0.87\n",
      "train epoch: 3529, loss 0.0831\n",
      "test epoch: 3529, loss: 0.87\n",
      "train epoch: 3530, loss 0.0829\n",
      "test epoch: 3530, loss: 0.87\n",
      "train epoch: 3531, loss 0.0830\n",
      "test epoch: 3531, loss: 0.87\n",
      "train epoch: 3532, loss 0.0832\n",
      "test epoch: 3532, loss: 0.87\n",
      "train epoch: 3533, loss 0.0831\n",
      "test epoch: 3533, loss: 0.87\n",
      "train epoch: 3534, loss 0.0829\n",
      "test epoch: 3534, loss: 0.87\n",
      "train epoch: 3535, loss 0.0834\n",
      "test epoch: 3535, loss: 0.87\n",
      "train epoch: 3536, loss 0.0833\n",
      "test epoch: 3536, loss: 0.87\n",
      "train epoch: 3537, loss 0.0831\n",
      "test epoch: 3537, loss: 0.87\n",
      "train epoch: 3538, loss 0.0838\n",
      "test epoch: 3538, loss: 0.87\n",
      "train epoch: 3539, loss 0.0831\n",
      "test epoch: 3539, loss: 0.87\n",
      "train epoch: 3540, loss 0.0831\n",
      "test epoch: 3540, loss: 0.87\n",
      "train epoch: 3541, loss 0.0835\n",
      "test epoch: 3541, loss: 0.87\n",
      "train epoch: 3542, loss 0.0830\n",
      "test epoch: 3542, loss: 0.87\n",
      "train epoch: 3543, loss 0.0830\n",
      "test epoch: 3543, loss: 0.87\n",
      "train epoch: 3544, loss 0.0834\n",
      "test epoch: 3544, loss: 0.87\n",
      "train epoch: 3545, loss 0.0832\n",
      "test epoch: 3545, loss: 0.87\n",
      "train epoch: 3546, loss 0.0829\n",
      "test epoch: 3546, loss: 0.87\n",
      "train epoch: 3547, loss 0.0834\n",
      "test epoch: 3547, loss: 0.87\n",
      "train epoch: 3548, loss 0.0832\n",
      "test epoch: 3548, loss: 0.87\n",
      "train epoch: 3549, loss 0.0830\n",
      "test epoch: 3549, loss: 0.87\n",
      "train epoch: 3550, loss 0.0834\n",
      "test epoch: 3550, loss: 0.87\n",
      "train epoch: 3551, loss 0.0833\n",
      "test epoch: 3551, loss: 0.87\n",
      "train epoch: 3552, loss 0.0830\n",
      "test epoch: 3552, loss: 0.87\n",
      "train epoch: 3553, loss 0.0839\n",
      "test epoch: 3553, loss: 0.87\n",
      "train epoch: 3554, loss 0.0830\n",
      "test epoch: 3554, loss: 0.87\n",
      "train epoch: 3555, loss 0.0831\n",
      "test epoch: 3555, loss: 0.87\n",
      "train epoch: 3556, loss 0.0834\n",
      "test epoch: 3556, loss: 0.87\n",
      "train epoch: 3557, loss 0.0830\n",
      "test epoch: 3557, loss: 0.87\n",
      "train epoch: 3558, loss 0.0829\n",
      "test epoch: 3558, loss: 0.87\n",
      "train epoch: 3559, loss 0.0833\n",
      "test epoch: 3559, loss: 0.87\n",
      "train epoch: 3560, loss 0.0832\n",
      "test epoch: 3560, loss: 0.87\n",
      "train epoch: 3561, loss 0.0829\n",
      "test epoch: 3561, loss: 0.87\n",
      "train epoch: 3562, loss 0.0837\n",
      "test epoch: 3562, loss: 0.87\n",
      "train epoch: 3563, loss 0.0831\n",
      "test epoch: 3563, loss: 0.87\n",
      "train epoch: 3564, loss 0.0830\n",
      "test epoch: 3564, loss: 0.87\n",
      "train epoch: 3565, loss 0.0835\n",
      "test epoch: 3565, loss: 0.87\n",
      "train epoch: 3566, loss 0.0830\n",
      "test epoch: 3566, loss: 0.87\n",
      "train epoch: 3567, loss 0.0831\n",
      "test epoch: 3567, loss: 0.87\n",
      "train epoch: 3568, loss 0.0836\n",
      "test epoch: 3568, loss: 0.87\n",
      "train epoch: 3569, loss 0.0833\n",
      "test epoch: 3569, loss: 0.87\n",
      "train epoch: 3570, loss 0.0831\n",
      "test epoch: 3570, loss: 0.87\n",
      "train epoch: 3571, loss 0.0832\n",
      "test epoch: 3571, loss: 0.87\n",
      "train epoch: 3572, loss 0.0833\n",
      "test epoch: 3572, loss: 0.87\n",
      "train epoch: 3573, loss 0.0830\n",
      "test epoch: 3573, loss: 0.87\n",
      "train epoch: 3574, loss 0.0832\n",
      "test epoch: 3574, loss: 0.87\n",
      "train epoch: 3575, loss 0.0833\n",
      "test epoch: 3575, loss: 0.87\n",
      "train epoch: 3576, loss 0.0832\n",
      "test epoch: 3576, loss: 0.87\n",
      "train epoch: 3577, loss 0.0836\n",
      "test epoch: 3577, loss: 0.87\n",
      "train epoch: 3578, loss 0.0835\n",
      "test epoch: 3578, loss: 0.87\n",
      "train epoch: 3579, loss 0.0832\n",
      "test epoch: 3579, loss: 0.87\n",
      "train epoch: 3580, loss 0.0838\n",
      "test epoch: 3580, loss: 0.87\n",
      "train epoch: 3581, loss 0.0832\n",
      "test epoch: 3581, loss: 0.87\n",
      "train epoch: 3582, loss 0.0832\n",
      "test epoch: 3582, loss: 0.87\n",
      "train epoch: 3583, loss 0.0836\n",
      "test epoch: 3583, loss: 0.87\n",
      "train epoch: 3584, loss 0.0830\n",
      "test epoch: 3584, loss: 0.87\n",
      "train epoch: 3585, loss 0.0831\n",
      "test epoch: 3585, loss: 0.87\n",
      "train epoch: 3586, loss 0.0835\n",
      "test epoch: 3586, loss: 0.87\n",
      "train epoch: 3587, loss 0.0832\n",
      "test epoch: 3587, loss: 0.87\n",
      "train epoch: 3588, loss 0.0831\n",
      "test epoch: 3588, loss: 0.87\n",
      "train epoch: 3589, loss 0.0834\n",
      "test epoch: 3589, loss: 0.87\n",
      "train epoch: 3590, loss 0.0834\n",
      "test epoch: 3590, loss: 0.87\n",
      "train epoch: 3591, loss 0.0832\n",
      "test epoch: 3591, loss: 0.87\n",
      "train epoch: 3592, loss 0.0836\n",
      "test epoch: 3592, loss: 0.87\n",
      "train epoch: 3593, loss 0.0832\n",
      "test epoch: 3593, loss: 0.87\n",
      "train epoch: 3594, loss 0.0830\n",
      "test epoch: 3594, loss: 0.87\n",
      "train epoch: 3595, loss 0.0835\n",
      "test epoch: 3595, loss: 0.87\n",
      "train epoch: 3596, loss 0.0832\n",
      "test epoch: 3596, loss: 0.87\n",
      "train epoch: 3597, loss 0.0832\n",
      "test epoch: 3597, loss: 0.87\n",
      "train epoch: 3598, loss 0.0833\n",
      "test epoch: 3598, loss: 0.87\n",
      "train epoch: 3599, loss 0.0830\n",
      "test epoch: 3599, loss: 0.87\n",
      "train epoch: 3600, loss 0.0831\n",
      "test epoch: 3600, loss: 0.87\n",
      "train epoch: 3601, loss 0.0830\n",
      "test epoch: 3601, loss: 0.87\n",
      "train epoch: 3602, loss 0.0833\n",
      "test epoch: 3602, loss: 0.87\n",
      "train epoch: 3603, loss 0.0831\n",
      "test epoch: 3603, loss: 0.87\n",
      "train epoch: 3604, loss 0.0833\n",
      "test epoch: 3604, loss: 0.87\n",
      "train epoch: 3605, loss 0.0830\n",
      "test epoch: 3605, loss: 0.87\n",
      "train epoch: 3606, loss 0.0831\n",
      "test epoch: 3606, loss: 0.87\n",
      "train epoch: 3607, loss 0.0830\n",
      "test epoch: 3607, loss: 0.87\n",
      "train epoch: 3608, loss 0.0830\n",
      "test epoch: 3608, loss: 0.87\n",
      "train epoch: 3609, loss 0.0830\n",
      "test epoch: 3609, loss: 0.87\n",
      "train epoch: 3610, loss 0.0831\n",
      "test epoch: 3610, loss: 0.87\n",
      "train epoch: 3611, loss 0.0833\n",
      "test epoch: 3611, loss: 0.87\n",
      "train epoch: 3612, loss 0.0832\n",
      "test epoch: 3612, loss: 0.87\n",
      "train epoch: 3613, loss 0.0833\n",
      "test epoch: 3613, loss: 0.87\n",
      "train epoch: 3614, loss 0.0834\n",
      "test epoch: 3614, loss: 0.87\n",
      "train epoch: 3615, loss 0.0832\n",
      "test epoch: 3615, loss: 0.87\n",
      "train epoch: 3616, loss 0.0833\n",
      "test epoch: 3616, loss: 0.87\n",
      "train epoch: 3617, loss 0.0832\n",
      "test epoch: 3617, loss: 0.87\n",
      "train epoch: 3618, loss 0.0831\n",
      "test epoch: 3618, loss: 0.87\n",
      "train epoch: 3619, loss 0.0829\n",
      "test epoch: 3619, loss: 0.87\n",
      "train epoch: 3620, loss 0.0832\n",
      "test epoch: 3620, loss: 0.87\n",
      "train epoch: 3621, loss 0.0833\n",
      "test epoch: 3621, loss: 0.87\n",
      "train epoch: 3622, loss 0.0832\n",
      "test epoch: 3622, loss: 0.87\n",
      "train epoch: 3623, loss 0.0835\n",
      "test epoch: 3623, loss: 0.87\n",
      "train epoch: 3624, loss 0.0836\n",
      "test epoch: 3624, loss: 0.87\n",
      "train epoch: 3625, loss 0.0831\n",
      "test epoch: 3625, loss: 0.87\n",
      "train epoch: 3626, loss 0.0842\n",
      "test epoch: 3626, loss: 0.87\n",
      "train epoch: 3627, loss 0.0830\n",
      "test epoch: 3627, loss: 0.87\n",
      "train epoch: 3628, loss 0.0834\n",
      "test epoch: 3628, loss: 0.87\n",
      "train epoch: 3629, loss 0.0834\n",
      "test epoch: 3629, loss: 0.87\n",
      "train epoch: 3630, loss 0.0830\n",
      "test epoch: 3630, loss: 0.87\n",
      "train epoch: 3631, loss 0.0832\n",
      "test epoch: 3631, loss: 0.87\n",
      "train epoch: 3632, loss 0.0830\n",
      "test epoch: 3632, loss: 0.87\n",
      "train epoch: 3633, loss 0.0830\n",
      "test epoch: 3633, loss: 0.87\n",
      "train epoch: 3634, loss 0.0838\n",
      "test epoch: 3634, loss: 0.87\n",
      "train epoch: 3635, loss 0.0830\n",
      "test epoch: 3635, loss: 0.87\n",
      "train epoch: 3636, loss 0.0839\n",
      "test epoch: 3636, loss: 0.87\n",
      "train epoch: 3637, loss 0.0832\n",
      "test epoch: 3637, loss: 0.87\n",
      "train epoch: 3638, loss 0.0832\n",
      "test epoch: 3638, loss: 0.87\n",
      "train epoch: 3639, loss 0.0836\n",
      "test epoch: 3639, loss: 0.87\n",
      "train epoch: 3640, loss 0.0829\n",
      "test epoch: 3640, loss: 0.87\n",
      "train epoch: 3641, loss 0.0833\n",
      "test epoch: 3641, loss: 0.87\n",
      "train epoch: 3642, loss 0.0830\n",
      "test epoch: 3642, loss: 0.87\n",
      "train epoch: 3643, loss 0.0830\n",
      "test epoch: 3643, loss: 0.87\n",
      "train epoch: 3644, loss 0.0829\n",
      "test epoch: 3644, loss: 0.87\n",
      "train epoch: 3645, loss 0.0832\n",
      "test epoch: 3645, loss: 0.87\n",
      "train epoch: 3646, loss 0.0834\n",
      "test epoch: 3646, loss: 0.87\n",
      "train epoch: 3647, loss 0.0829\n",
      "test epoch: 3647, loss: 0.87\n",
      "train epoch: 3648, loss 0.0838\n",
      "test epoch: 3648, loss: 0.87\n",
      "train epoch: 3649, loss 0.0830\n",
      "test epoch: 3649, loss: 0.87\n",
      "train epoch: 3650, loss 0.0831\n",
      "test epoch: 3650, loss: 0.87\n",
      "train epoch: 3651, loss 0.0835\n",
      "test epoch: 3651, loss: 0.87\n",
      "train epoch: 3652, loss 0.0829\n",
      "test epoch: 3652, loss: 0.87\n",
      "train epoch: 3653, loss 0.0829\n",
      "test epoch: 3653, loss: 0.87\n",
      "train epoch: 3654, loss 0.0833\n",
      "test epoch: 3654, loss: 0.87\n",
      "train epoch: 3655, loss 0.0831\n",
      "test epoch: 3655, loss: 0.87\n",
      "train epoch: 3656, loss 0.0829\n",
      "test epoch: 3656, loss: 0.87\n",
      "train epoch: 3657, loss 0.0834\n",
      "test epoch: 3657, loss: 0.87\n",
      "train epoch: 3658, loss 0.0831\n",
      "test epoch: 3658, loss: 0.87\n",
      "train epoch: 3659, loss 0.0829\n",
      "test epoch: 3659, loss: 0.87\n",
      "train epoch: 3660, loss 0.0836\n",
      "test epoch: 3660, loss: 0.87\n",
      "train epoch: 3661, loss 0.0829\n",
      "test epoch: 3661, loss: 0.87\n",
      "train epoch: 3662, loss 0.0831\n",
      "test epoch: 3662, loss: 0.87\n",
      "train epoch: 3663, loss 0.0834\n",
      "test epoch: 3663, loss: 0.87\n",
      "train epoch: 3664, loss 0.0829\n",
      "test epoch: 3664, loss: 0.87\n",
      "train epoch: 3665, loss 0.0832\n",
      "test epoch: 3665, loss: 0.87\n",
      "train epoch: 3666, loss 0.0830\n",
      "test epoch: 3666, loss: 0.87\n",
      "train epoch: 3667, loss 0.0831\n",
      "test epoch: 3667, loss: 0.87\n",
      "train epoch: 3668, loss 0.0836\n",
      "test epoch: 3668, loss: 0.87\n",
      "train epoch: 3669, loss 0.0829\n",
      "test epoch: 3669, loss: 0.87\n",
      "train epoch: 3670, loss 0.0839\n",
      "test epoch: 3670, loss: 0.87\n",
      "train epoch: 3671, loss 0.0830\n",
      "test epoch: 3671, loss: 0.87\n",
      "train epoch: 3672, loss 0.0832\n",
      "test epoch: 3672, loss: 0.87\n",
      "train epoch: 3673, loss 0.0834\n",
      "test epoch: 3673, loss: 0.87\n",
      "train epoch: 3674, loss 0.0828\n",
      "test epoch: 3674, loss: 0.87\n",
      "train epoch: 3675, loss 0.0832\n",
      "test epoch: 3675, loss: 0.87\n",
      "train epoch: 3676, loss 0.0829\n",
      "test epoch: 3676, loss: 0.87\n",
      "train epoch: 3677, loss 0.0830\n",
      "test epoch: 3677, loss: 0.87\n",
      "train epoch: 3678, loss 0.0836\n",
      "test epoch: 3678, loss: 0.87\n",
      "train epoch: 3679, loss 0.0830\n",
      "test epoch: 3679, loss: 0.87\n",
      "train epoch: 3680, loss 0.0841\n",
      "test epoch: 3680, loss: 0.87\n",
      "train epoch: 3681, loss 0.0830\n",
      "test epoch: 3681, loss: 0.87\n",
      "train epoch: 3682, loss 0.0833\n",
      "test epoch: 3682, loss: 0.87\n",
      "train epoch: 3683, loss 0.0834\n",
      "test epoch: 3683, loss: 0.87\n",
      "train epoch: 3684, loss 0.0828\n",
      "test epoch: 3684, loss: 0.87\n",
      "train epoch: 3685, loss 0.0832\n",
      "test epoch: 3685, loss: 0.87\n",
      "train epoch: 3686, loss 0.0829\n",
      "test epoch: 3686, loss: 0.87\n",
      "train epoch: 3687, loss 0.0829\n",
      "test epoch: 3687, loss: 0.87\n",
      "train epoch: 3688, loss 0.0836\n",
      "test epoch: 3688, loss: 0.87\n",
      "train epoch: 3689, loss 0.0829\n",
      "test epoch: 3689, loss: 0.87\n",
      "train epoch: 3690, loss 0.0841\n",
      "test epoch: 3690, loss: 0.87\n",
      "train epoch: 3691, loss 0.0831\n",
      "test epoch: 3691, loss: 0.87\n",
      "train epoch: 3692, loss 0.0832\n",
      "test epoch: 3692, loss: 0.87\n",
      "train epoch: 3693, loss 0.0834\n",
      "test epoch: 3693, loss: 0.87\n",
      "train epoch: 3694, loss 0.0828\n",
      "test epoch: 3694, loss: 0.87\n",
      "train epoch: 3695, loss 0.0831\n",
      "test epoch: 3695, loss: 0.87\n",
      "train epoch: 3696, loss 0.0829\n",
      "test epoch: 3696, loss: 0.87\n",
      "train epoch: 3697, loss 0.0829\n",
      "test epoch: 3697, loss: 0.87\n",
      "train epoch: 3698, loss 0.0836\n",
      "test epoch: 3698, loss: 0.87\n",
      "train epoch: 3699, loss 0.0828\n",
      "test epoch: 3699, loss: 0.87\n",
      "train epoch: 3700, loss 0.0839\n",
      "test epoch: 3700, loss: 0.87\n",
      "train epoch: 3701, loss 0.0831\n",
      "test epoch: 3701, loss: 0.87\n",
      "train epoch: 3702, loss 0.0832\n",
      "test epoch: 3702, loss: 0.87\n",
      "train epoch: 3703, loss 0.0835\n",
      "test epoch: 3703, loss: 0.87\n",
      "train epoch: 3704, loss 0.0829\n",
      "test epoch: 3704, loss: 0.87\n",
      "train epoch: 3705, loss 0.0832\n",
      "test epoch: 3705, loss: 0.87\n",
      "train epoch: 3706, loss 0.0830\n",
      "test epoch: 3706, loss: 0.87\n",
      "train epoch: 3707, loss 0.0830\n",
      "test epoch: 3707, loss: 0.87\n",
      "train epoch: 3708, loss 0.0836\n",
      "test epoch: 3708, loss: 0.87\n",
      "train epoch: 3709, loss 0.0829\n",
      "test epoch: 3709, loss: 0.87\n",
      "train epoch: 3710, loss 0.0843\n",
      "test epoch: 3710, loss: 0.87\n",
      "train epoch: 3711, loss 0.0830\n",
      "test epoch: 3711, loss: 0.87\n",
      "train epoch: 3712, loss 0.0834\n",
      "test epoch: 3712, loss: 0.87\n",
      "train epoch: 3713, loss 0.0834\n",
      "test epoch: 3713, loss: 0.87\n",
      "train epoch: 3714, loss 0.0829\n",
      "test epoch: 3714, loss: 0.87\n",
      "train epoch: 3715, loss 0.0833\n",
      "test epoch: 3715, loss: 0.87\n",
      "train epoch: 3716, loss 0.0829\n",
      "test epoch: 3716, loss: 0.87\n",
      "train epoch: 3717, loss 0.0833\n",
      "test epoch: 3717, loss: 0.87\n",
      "train epoch: 3718, loss 0.0832\n",
      "test epoch: 3718, loss: 0.87\n",
      "train epoch: 3719, loss 0.0829\n",
      "test epoch: 3719, loss: 0.87\n",
      "train epoch: 3720, loss 0.0838\n",
      "test epoch: 3720, loss: 0.87\n",
      "train epoch: 3721, loss 0.0829\n",
      "test epoch: 3721, loss: 0.87\n",
      "train epoch: 3722, loss 0.0831\n",
      "test epoch: 3722, loss: 0.87\n",
      "train epoch: 3723, loss 0.0831\n",
      "test epoch: 3723, loss: 0.87\n",
      "train epoch: 3724, loss 0.0829\n",
      "test epoch: 3724, loss: 0.87\n",
      "train epoch: 3725, loss 0.0828\n",
      "test epoch: 3725, loss: 0.87\n",
      "train epoch: 3726, loss 0.0829\n",
      "test epoch: 3726, loss: 0.87\n",
      "train epoch: 3727, loss 0.0832\n",
      "test epoch: 3727, loss: 0.87\n",
      "train epoch: 3728, loss 0.0828\n",
      "test epoch: 3728, loss: 0.87\n",
      "train epoch: 3729, loss 0.0830\n",
      "test epoch: 3729, loss: 0.87\n",
      "train epoch: 3730, loss 0.0833\n",
      "test epoch: 3730, loss: 0.87\n",
      "train epoch: 3731, loss 0.0829\n",
      "test epoch: 3731, loss: 0.87\n",
      "train epoch: 3732, loss 0.0837\n",
      "test epoch: 3732, loss: 0.87\n",
      "train epoch: 3733, loss 0.0830\n",
      "test epoch: 3733, loss: 0.87\n",
      "train epoch: 3734, loss 0.0830\n",
      "test epoch: 3734, loss: 0.87\n",
      "train epoch: 3735, loss 0.0834\n",
      "test epoch: 3735, loss: 0.87\n",
      "train epoch: 3736, loss 0.0828\n",
      "test epoch: 3736, loss: 0.87\n",
      "train epoch: 3737, loss 0.0829\n",
      "test epoch: 3737, loss: 0.87\n",
      "train epoch: 3738, loss 0.0830\n",
      "test epoch: 3738, loss: 0.87\n",
      "train epoch: 3739, loss 0.0830\n",
      "test epoch: 3739, loss: 0.87\n",
      "train epoch: 3740, loss 0.0828\n",
      "test epoch: 3740, loss: 0.87\n",
      "train epoch: 3741, loss 0.0830\n",
      "test epoch: 3741, loss: 0.87\n",
      "train epoch: 3742, loss 0.0832\n",
      "test epoch: 3742, loss: 0.87\n",
      "train epoch: 3743, loss 0.0828\n",
      "test epoch: 3743, loss: 0.87\n",
      "train epoch: 3744, loss 0.0834\n",
      "test epoch: 3744, loss: 0.87\n",
      "train epoch: 3745, loss 0.0830\n",
      "test epoch: 3745, loss: 0.87\n",
      "train epoch: 3746, loss 0.0829\n",
      "test epoch: 3746, loss: 0.87\n",
      "train epoch: 3747, loss 0.0835\n",
      "test epoch: 3747, loss: 0.87\n",
      "train epoch: 3748, loss 0.0828\n",
      "test epoch: 3748, loss: 0.87\n",
      "train epoch: 3749, loss 0.0830\n",
      "test epoch: 3749, loss: 0.87\n",
      "train epoch: 3750, loss 0.0832\n",
      "test epoch: 3750, loss: 0.87\n",
      "train epoch: 3751, loss 0.0829\n",
      "test epoch: 3751, loss: 0.87\n",
      "train epoch: 3752, loss 0.0828\n",
      "test epoch: 3752, loss: 0.87\n",
      "train epoch: 3753, loss 0.0832\n",
      "test epoch: 3753, loss: 0.87\n",
      "train epoch: 3754, loss 0.0830\n",
      "test epoch: 3754, loss: 0.87\n",
      "train epoch: 3755, loss 0.0828\n",
      "test epoch: 3755, loss: 0.87\n",
      "train epoch: 3756, loss 0.0835\n",
      "test epoch: 3756, loss: 0.87\n",
      "train epoch: 3757, loss 0.0830\n",
      "test epoch: 3757, loss: 0.87\n",
      "train epoch: 3758, loss 0.0829\n",
      "test epoch: 3758, loss: 0.87\n",
      "train epoch: 3759, loss 0.0835\n",
      "test epoch: 3759, loss: 0.87\n",
      "train epoch: 3760, loss 0.0829\n",
      "test epoch: 3760, loss: 0.87\n",
      "train epoch: 3761, loss 0.0830\n",
      "test epoch: 3761, loss: 0.87\n",
      "train epoch: 3762, loss 0.0832\n",
      "test epoch: 3762, loss: 0.87\n",
      "train epoch: 3763, loss 0.0828\n",
      "test epoch: 3763, loss: 0.87\n",
      "train epoch: 3764, loss 0.0830\n",
      "test epoch: 3764, loss: 0.87\n",
      "train epoch: 3765, loss 0.0831\n",
      "test epoch: 3765, loss: 0.87\n",
      "train epoch: 3766, loss 0.0828\n",
      "test epoch: 3766, loss: 0.87\n",
      "train epoch: 3767, loss 0.0828\n",
      "test epoch: 3767, loss: 0.87\n",
      "train epoch: 3768, loss 0.0829\n",
      "test epoch: 3768, loss: 0.87\n",
      "train epoch: 3769, loss 0.0833\n",
      "test epoch: 3769, loss: 0.87\n",
      "train epoch: 3770, loss 0.0829\n",
      "test epoch: 3770, loss: 0.87\n",
      "train epoch: 3771, loss 0.0831\n",
      "test epoch: 3771, loss: 0.87\n",
      "train epoch: 3772, loss 0.0829\n",
      "test epoch: 3772, loss: 0.87\n",
      "train epoch: 3773, loss 0.0829\n",
      "test epoch: 3773, loss: 0.87\n",
      "train epoch: 3774, loss 0.0828\n",
      "test epoch: 3774, loss: 0.87\n",
      "train epoch: 3775, loss 0.0829\n",
      "test epoch: 3775, loss: 0.87\n",
      "train epoch: 3776, loss 0.0829\n",
      "test epoch: 3776, loss: 0.87\n",
      "train epoch: 3777, loss 0.0828\n",
      "test epoch: 3777, loss: 0.87\n",
      "train epoch: 3778, loss 0.0829\n",
      "test epoch: 3778, loss: 0.87\n",
      "train epoch: 3779, loss 0.0833\n",
      "test epoch: 3779, loss: 0.87\n",
      "train epoch: 3780, loss 0.0828\n",
      "test epoch: 3780, loss: 0.87\n",
      "train epoch: 3781, loss 0.0831\n",
      "test epoch: 3781, loss: 0.87\n",
      "train epoch: 3782, loss 0.0834\n",
      "test epoch: 3782, loss: 0.87\n",
      "train epoch: 3783, loss 0.0830\n",
      "test epoch: 3783, loss: 0.87\n",
      "train epoch: 3784, loss 0.0838\n",
      "test epoch: 3784, loss: 0.87\n",
      "train epoch: 3785, loss 0.0829\n",
      "test epoch: 3785, loss: 0.87\n",
      "train epoch: 3786, loss 0.0831\n",
      "test epoch: 3786, loss: 0.87\n",
      "train epoch: 3787, loss 0.0833\n",
      "test epoch: 3787, loss: 0.87\n",
      "train epoch: 3788, loss 0.0829\n",
      "test epoch: 3788, loss: 0.87\n",
      "train epoch: 3789, loss 0.0830\n",
      "test epoch: 3789, loss: 0.87\n",
      "train epoch: 3790, loss 0.0830\n",
      "test epoch: 3790, loss: 0.87\n",
      "train epoch: 3791, loss 0.0828\n",
      "test epoch: 3791, loss: 0.87\n",
      "train epoch: 3792, loss 0.0830\n",
      "test epoch: 3792, loss: 0.87\n",
      "train epoch: 3793, loss 0.0831\n",
      "test epoch: 3793, loss: 0.87\n",
      "train epoch: 3794, loss 0.0829\n",
      "test epoch: 3794, loss: 0.87\n",
      "train epoch: 3795, loss 0.0828\n",
      "test epoch: 3795, loss: 0.87\n",
      "train epoch: 3796, loss 0.0829\n",
      "test epoch: 3796, loss: 0.87\n",
      "train epoch: 3797, loss 0.0828\n",
      "test epoch: 3797, loss: 0.87\n",
      "train epoch: 3798, loss 0.0829\n",
      "test epoch: 3798, loss: 0.87\n",
      "train epoch: 3799, loss 0.0831\n",
      "test epoch: 3799, loss: 0.87\n",
      "train epoch: 3800, loss 0.0829\n",
      "test epoch: 3800, loss: 0.87\n",
      "train epoch: 3801, loss 0.0830\n",
      "test epoch: 3801, loss: 0.87\n",
      "train epoch: 3802, loss 0.0830\n",
      "test epoch: 3802, loss: 0.87\n",
      "train epoch: 3803, loss 0.0828\n",
      "test epoch: 3803, loss: 0.87\n",
      "train epoch: 3804, loss 0.0832\n",
      "test epoch: 3804, loss: 0.87\n",
      "train epoch: 3805, loss 0.0831\n",
      "test epoch: 3805, loss: 0.87\n",
      "train epoch: 3806, loss 0.0829\n",
      "test epoch: 3806, loss: 0.87\n",
      "train epoch: 3807, loss 0.0834\n",
      "test epoch: 3807, loss: 0.87\n",
      "train epoch: 3808, loss 0.0832\n",
      "test epoch: 3808, loss: 0.87\n",
      "train epoch: 3809, loss 0.0831\n",
      "test epoch: 3809, loss: 0.87\n",
      "train epoch: 3810, loss 0.0836\n",
      "test epoch: 3810, loss: 0.87\n",
      "train epoch: 3811, loss 0.0828\n",
      "test epoch: 3811, loss: 0.87\n",
      "train epoch: 3812, loss 0.0832\n",
      "test epoch: 3812, loss: 0.87\n",
      "train epoch: 3813, loss 0.0829\n",
      "test epoch: 3813, loss: 0.87\n",
      "train epoch: 3814, loss 0.0830\n",
      "test epoch: 3814, loss: 0.87\n",
      "train epoch: 3815, loss 0.0836\n",
      "test epoch: 3815, loss: 0.87\n",
      "train epoch: 3816, loss 0.0830\n",
      "test epoch: 3816, loss: 0.87\n",
      "train epoch: 3817, loss 0.0840\n",
      "test epoch: 3817, loss: 0.87\n",
      "train epoch: 3818, loss 0.0829\n",
      "test epoch: 3818, loss: 0.87\n",
      "train epoch: 3819, loss 0.0831\n",
      "test epoch: 3819, loss: 0.87\n",
      "train epoch: 3820, loss 0.0834\n",
      "test epoch: 3820, loss: 0.87\n",
      "train epoch: 3821, loss 0.0828\n",
      "test epoch: 3821, loss: 0.87\n",
      "train epoch: 3822, loss 0.0830\n",
      "test epoch: 3822, loss: 0.87\n",
      "train epoch: 3823, loss 0.0829\n",
      "test epoch: 3823, loss: 0.87\n",
      "train epoch: 3824, loss 0.0829\n",
      "test epoch: 3824, loss: 0.87\n",
      "train epoch: 3825, loss 0.0835\n",
      "test epoch: 3825, loss: 0.87\n",
      "train epoch: 3826, loss 0.0828\n",
      "test epoch: 3826, loss: 0.87\n",
      "train epoch: 3827, loss 0.0839\n",
      "test epoch: 3827, loss: 0.87\n",
      "train epoch: 3828, loss 0.0829\n",
      "test epoch: 3828, loss: 0.87\n",
      "train epoch: 3829, loss 0.0831\n",
      "test epoch: 3829, loss: 0.87\n",
      "train epoch: 3830, loss 0.0833\n",
      "test epoch: 3830, loss: 0.87\n",
      "train epoch: 3831, loss 0.0828\n",
      "test epoch: 3831, loss: 0.87\n",
      "train epoch: 3832, loss 0.0831\n",
      "test epoch: 3832, loss: 0.87\n",
      "train epoch: 3833, loss 0.0830\n",
      "test epoch: 3833, loss: 0.87\n",
      "train epoch: 3834, loss 0.0829\n",
      "test epoch: 3834, loss: 0.87\n",
      "train epoch: 3835, loss 0.0828\n",
      "test epoch: 3835, loss: 0.87\n",
      "train epoch: 3836, loss 0.0828\n",
      "test epoch: 3836, loss: 0.87\n",
      "train epoch: 3837, loss 0.0828\n",
      "test epoch: 3837, loss: 0.87\n",
      "train epoch: 3838, loss 0.0829\n",
      "test epoch: 3838, loss: 0.87\n",
      "train epoch: 3839, loss 0.0831\n",
      "test epoch: 3839, loss: 0.87\n",
      "train epoch: 3840, loss 0.0829\n",
      "test epoch: 3840, loss: 0.87\n",
      "train epoch: 3841, loss 0.0829\n",
      "test epoch: 3841, loss: 0.87\n",
      "train epoch: 3842, loss 0.0830\n",
      "test epoch: 3842, loss: 0.87\n",
      "train epoch: 3843, loss 0.0830\n",
      "test epoch: 3843, loss: 0.87\n",
      "train epoch: 3844, loss 0.0829\n",
      "test epoch: 3844, loss: 0.87\n",
      "train epoch: 3845, loss 0.0829\n",
      "test epoch: 3845, loss: 0.87\n",
      "train epoch: 3846, loss 0.0828\n",
      "test epoch: 3846, loss: 0.87\n",
      "train epoch: 3847, loss 0.0828\n",
      "test epoch: 3847, loss: 0.87\n",
      "train epoch: 3848, loss 0.0829\n",
      "test epoch: 3848, loss: 0.87\n",
      "train epoch: 3849, loss 0.0831\n",
      "test epoch: 3849, loss: 0.87\n",
      "train epoch: 3850, loss 0.0828\n",
      "test epoch: 3850, loss: 0.87\n",
      "train epoch: 3851, loss 0.0829\n",
      "test epoch: 3851, loss: 0.87\n",
      "train epoch: 3852, loss 0.0829\n",
      "test epoch: 3852, loss: 0.87\n",
      "train epoch: 3853, loss 0.0828\n",
      "test epoch: 3853, loss: 0.87\n",
      "train epoch: 3854, loss 0.0828\n",
      "test epoch: 3854, loss: 0.87\n",
      "train epoch: 3855, loss 0.0829\n",
      "test epoch: 3855, loss: 0.87\n",
      "train epoch: 3856, loss 0.0828\n",
      "test epoch: 3856, loss: 0.87\n",
      "train epoch: 3857, loss 0.0828\n",
      "test epoch: 3857, loss: 0.87\n",
      "train epoch: 3858, loss 0.0828\n",
      "test epoch: 3858, loss: 0.87\n",
      "train epoch: 3859, loss 0.0832\n",
      "test epoch: 3859, loss: 0.87\n",
      "train epoch: 3860, loss 0.0829\n",
      "test epoch: 3860, loss: 0.87\n",
      "train epoch: 3861, loss 0.0831\n",
      "test epoch: 3861, loss: 0.87\n",
      "train epoch: 3862, loss 0.0830\n",
      "test epoch: 3862, loss: 0.87\n",
      "train epoch: 3863, loss 0.0829\n",
      "test epoch: 3863, loss: 0.87\n",
      "train epoch: 3864, loss 0.0828\n",
      "test epoch: 3864, loss: 0.87\n",
      "train epoch: 3865, loss 0.0831\n",
      "test epoch: 3865, loss: 0.87\n",
      "train epoch: 3866, loss 0.0831\n",
      "test epoch: 3866, loss: 0.87\n",
      "train epoch: 3867, loss 0.0830\n",
      "test epoch: 3867, loss: 0.87\n",
      "train epoch: 3868, loss 0.0832\n",
      "test epoch: 3868, loss: 0.87\n",
      "train epoch: 3869, loss 0.0829\n",
      "test epoch: 3869, loss: 0.87\n",
      "train epoch: 3870, loss 0.0828\n",
      "test epoch: 3870, loss: 0.87\n",
      "train epoch: 3871, loss 0.0835\n",
      "test epoch: 3871, loss: 0.87\n",
      "train epoch: 3872, loss 0.0829\n",
      "test epoch: 3872, loss: 0.87\n",
      "train epoch: 3873, loss 0.0829\n",
      "test epoch: 3873, loss: 0.87\n",
      "train epoch: 3874, loss 0.0831\n",
      "test epoch: 3874, loss: 0.87\n",
      "train epoch: 3875, loss 0.0828\n",
      "test epoch: 3875, loss: 0.87\n",
      "train epoch: 3876, loss 0.0830\n",
      "test epoch: 3876, loss: 0.87\n",
      "train epoch: 3877, loss 0.0833\n",
      "test epoch: 3877, loss: 0.87\n",
      "train epoch: 3878, loss 0.0828\n",
      "test epoch: 3878, loss: 0.87\n",
      "train epoch: 3879, loss 0.0830\n",
      "test epoch: 3879, loss: 0.87\n",
      "train epoch: 3880, loss 0.0830\n",
      "test epoch: 3880, loss: 0.87\n",
      "train epoch: 3881, loss 0.0828\n",
      "test epoch: 3881, loss: 0.87\n",
      "train epoch: 3882, loss 0.0830\n",
      "test epoch: 3882, loss: 0.87\n",
      "train epoch: 3883, loss 0.0830\n",
      "test epoch: 3883, loss: 0.87\n",
      "train epoch: 3884, loss 0.0828\n",
      "test epoch: 3884, loss: 0.87\n",
      "train epoch: 3885, loss 0.0828\n",
      "test epoch: 3885, loss: 0.87\n",
      "train epoch: 3886, loss 0.0829\n",
      "test epoch: 3886, loss: 0.87\n",
      "train epoch: 3887, loss 0.0828\n",
      "test epoch: 3887, loss: 0.87\n",
      "train epoch: 3888, loss 0.0829\n",
      "test epoch: 3888, loss: 0.87\n",
      "train epoch: 3889, loss 0.0831\n",
      "test epoch: 3889, loss: 0.87\n",
      "train epoch: 3890, loss 0.0828\n",
      "test epoch: 3890, loss: 0.87\n",
      "train epoch: 3891, loss 0.0829\n",
      "test epoch: 3891, loss: 0.87\n",
      "train epoch: 3892, loss 0.0829\n",
      "test epoch: 3892, loss: 0.87\n",
      "train epoch: 3893, loss 0.0828\n",
      "test epoch: 3893, loss: 0.87\n",
      "train epoch: 3894, loss 0.0828\n",
      "test epoch: 3894, loss: 0.87\n",
      "train epoch: 3895, loss 0.0829\n",
      "test epoch: 3895, loss: 0.87\n",
      "train epoch: 3896, loss 0.0828\n",
      "test epoch: 3896, loss: 0.87\n",
      "train epoch: 3897, loss 0.0828\n",
      "test epoch: 3897, loss: 0.87\n",
      "train epoch: 3898, loss 0.0831\n",
      "test epoch: 3898, loss: 0.87\n",
      "train epoch: 3899, loss 0.0828\n",
      "test epoch: 3899, loss: 0.87\n",
      "train epoch: 3900, loss 0.0830\n",
      "test epoch: 3900, loss: 0.87\n",
      "train epoch: 3901, loss 0.0833\n",
      "test epoch: 3901, loss: 0.87\n",
      "train epoch: 3902, loss 0.0829\n",
      "test epoch: 3902, loss: 0.87\n",
      "train epoch: 3903, loss 0.0839\n",
      "test epoch: 3903, loss: 0.87\n",
      "train epoch: 3904, loss 0.0829\n",
      "test epoch: 3904, loss: 0.87\n",
      "train epoch: 3905, loss 0.0831\n",
      "test epoch: 3905, loss: 0.87\n",
      "train epoch: 3906, loss 0.0833\n",
      "test epoch: 3906, loss: 0.87\n",
      "train epoch: 3907, loss 0.0828\n",
      "test epoch: 3907, loss: 0.87\n",
      "train epoch: 3908, loss 0.0830\n",
      "test epoch: 3908, loss: 0.87\n",
      "train epoch: 3909, loss 0.0829\n",
      "test epoch: 3909, loss: 0.87\n",
      "train epoch: 3910, loss 0.0829\n",
      "test epoch: 3910, loss: 0.87\n",
      "train epoch: 3911, loss 0.0835\n",
      "test epoch: 3911, loss: 0.87\n",
      "train epoch: 3912, loss 0.0828\n",
      "test epoch: 3912, loss: 0.87\n",
      "train epoch: 3913, loss 0.0836\n",
      "test epoch: 3913, loss: 0.87\n",
      "train epoch: 3914, loss 0.0831\n",
      "test epoch: 3914, loss: 0.87\n",
      "train epoch: 3915, loss 0.0830\n",
      "test epoch: 3915, loss: 0.87\n",
      "train epoch: 3916, loss 0.0836\n",
      "test epoch: 3916, loss: 0.87\n",
      "train epoch: 3917, loss 0.0828\n",
      "test epoch: 3917, loss: 0.87\n",
      "train epoch: 3918, loss 0.0830\n",
      "test epoch: 3918, loss: 0.87\n",
      "train epoch: 3919, loss 0.0829\n",
      "test epoch: 3919, loss: 0.87\n",
      "train epoch: 3920, loss 0.0829\n",
      "test epoch: 3920, loss: 0.87\n",
      "train epoch: 3921, loss 0.0829\n",
      "test epoch: 3921, loss: 0.87\n",
      "train epoch: 3922, loss 0.0829\n",
      "test epoch: 3922, loss: 0.87\n",
      "train epoch: 3923, loss 0.0829\n",
      "test epoch: 3923, loss: 0.87\n",
      "train epoch: 3924, loss 0.0829\n",
      "test epoch: 3924, loss: 0.87\n",
      "train epoch: 3925, loss 0.0828\n",
      "test epoch: 3925, loss: 0.87\n",
      "train epoch: 3926, loss 0.0828\n",
      "test epoch: 3926, loss: 0.87\n",
      "train epoch: 3927, loss 0.0832\n",
      "test epoch: 3927, loss: 0.87\n",
      "train epoch: 3928, loss 0.0829\n",
      "test epoch: 3928, loss: 0.87\n",
      "train epoch: 3929, loss 0.0828\n",
      "test epoch: 3929, loss: 0.87\n",
      "train epoch: 3930, loss 0.0828\n",
      "test epoch: 3930, loss: 0.87\n",
      "train epoch: 3931, loss 0.0829\n",
      "test epoch: 3931, loss: 0.87\n",
      "train epoch: 3932, loss 0.0828\n",
      "test epoch: 3932, loss: 0.87\n",
      "train epoch: 3933, loss 0.0830\n",
      "test epoch: 3933, loss: 0.87\n",
      "train epoch: 3934, loss 0.0833\n",
      "test epoch: 3934, loss: 0.87\n",
      "train epoch: 3935, loss 0.0828\n",
      "test epoch: 3935, loss: 0.87\n",
      "train epoch: 3936, loss 0.0836\n",
      "test epoch: 3936, loss: 0.87\n",
      "train epoch: 3937, loss 0.0833\n",
      "test epoch: 3937, loss: 0.87\n",
      "train epoch: 3938, loss 0.0831\n",
      "test epoch: 3938, loss: 0.87\n",
      "train epoch: 3939, loss 0.0837\n",
      "test epoch: 3939, loss: 0.87\n",
      "train epoch: 3940, loss 0.0828\n",
      "test epoch: 3940, loss: 0.87\n",
      "train epoch: 3941, loss 0.0834\n",
      "test epoch: 3941, loss: 0.87\n",
      "train epoch: 3942, loss 0.0828\n",
      "test epoch: 3942, loss: 0.87\n",
      "train epoch: 3943, loss 0.0832\n",
      "test epoch: 3943, loss: 0.87\n",
      "train epoch: 3944, loss 0.0834\n",
      "test epoch: 3944, loss: 0.87\n",
      "train epoch: 3945, loss 0.0831\n",
      "test epoch: 3945, loss: 0.87\n",
      "train epoch: 3946, loss 0.0838\n",
      "test epoch: 3946, loss: 0.87\n",
      "train epoch: 3947, loss 0.0830\n",
      "test epoch: 3947, loss: 0.87\n",
      "train epoch: 3948, loss 0.0831\n",
      "test epoch: 3948, loss: 0.87\n",
      "train epoch: 3949, loss 0.0833\n",
      "test epoch: 3949, loss: 0.87\n",
      "train epoch: 3950, loss 0.0829\n",
      "test epoch: 3950, loss: 0.87\n",
      "train epoch: 3951, loss 0.0830\n",
      "test epoch: 3951, loss: 0.87\n",
      "train epoch: 3952, loss 0.0831\n",
      "test epoch: 3952, loss: 0.87\n",
      "train epoch: 3953, loss 0.0830\n",
      "test epoch: 3953, loss: 0.87\n",
      "train epoch: 3954, loss 0.0828\n",
      "test epoch: 3954, loss: 0.87\n",
      "train epoch: 3955, loss 0.0831\n",
      "test epoch: 3955, loss: 0.87\n",
      "train epoch: 3956, loss 0.0831\n",
      "test epoch: 3956, loss: 0.87\n",
      "train epoch: 3957, loss 0.0828\n",
      "test epoch: 3957, loss: 0.87\n",
      "train epoch: 3958, loss 0.0829\n",
      "test epoch: 3958, loss: 0.87\n",
      "train epoch: 3959, loss 0.0832\n",
      "test epoch: 3959, loss: 0.87\n",
      "train epoch: 3960, loss 0.0828\n",
      "test epoch: 3960, loss: 0.87\n",
      "train epoch: 3961, loss 0.0836\n",
      "test epoch: 3961, loss: 0.87\n",
      "train epoch: 3962, loss 0.0830\n",
      "test epoch: 3962, loss: 0.87\n",
      "train epoch: 3963, loss 0.0829\n",
      "test epoch: 3963, loss: 0.87\n",
      "train epoch: 3964, loss 0.0834\n",
      "test epoch: 3964, loss: 0.87\n",
      "train epoch: 3965, loss 0.0828\n",
      "test epoch: 3965, loss: 0.87\n",
      "train epoch: 3966, loss 0.0830\n",
      "test epoch: 3966, loss: 0.87\n",
      "train epoch: 3967, loss 0.0830\n",
      "test epoch: 3967, loss: 0.87\n",
      "train epoch: 3968, loss 0.0828\n",
      "test epoch: 3968, loss: 0.87\n",
      "train epoch: 3969, loss 0.0831\n",
      "test epoch: 3969, loss: 0.87\n",
      "train epoch: 3970, loss 0.0831\n",
      "test epoch: 3970, loss: 0.87\n",
      "train epoch: 3971, loss 0.0828\n",
      "test epoch: 3971, loss: 0.87\n",
      "train epoch: 3972, loss 0.0828\n",
      "test epoch: 3972, loss: 0.87\n",
      "train epoch: 3973, loss 0.0830\n",
      "test epoch: 3973, loss: 0.87\n",
      "train epoch: 3974, loss 0.0827\n",
      "test epoch: 3974, loss: 0.87\n",
      "train epoch: 3975, loss 0.0829\n",
      "test epoch: 3975, loss: 0.87\n",
      "train epoch: 3976, loss 0.0830\n",
      "test epoch: 3976, loss: 0.87\n",
      "train epoch: 3977, loss 0.0829\n",
      "test epoch: 3977, loss: 0.87\n",
      "train epoch: 3978, loss 0.0829\n",
      "test epoch: 3978, loss: 0.87\n",
      "train epoch: 3979, loss 0.0830\n",
      "test epoch: 3979, loss: 0.87\n",
      "train epoch: 3980, loss 0.0828\n",
      "test epoch: 3980, loss: 0.87\n",
      "train epoch: 3981, loss 0.0830\n",
      "test epoch: 3981, loss: 0.87\n",
      "train epoch: 3982, loss 0.0832\n",
      "test epoch: 3982, loss: 0.87\n",
      "train epoch: 3983, loss 0.0828\n",
      "test epoch: 3983, loss: 0.87\n",
      "train epoch: 3984, loss 0.0837\n",
      "test epoch: 3984, loss: 0.87\n",
      "train epoch: 3985, loss 0.0831\n",
      "test epoch: 3985, loss: 0.87\n",
      "train epoch: 3986, loss 0.0831\n",
      "test epoch: 3986, loss: 0.87\n",
      "train epoch: 3987, loss 0.0835\n",
      "test epoch: 3987, loss: 0.87\n",
      "train epoch: 3988, loss 0.0827\n",
      "test epoch: 3988, loss: 0.87\n",
      "train epoch: 3989, loss 0.0833\n",
      "test epoch: 3989, loss: 0.87\n",
      "train epoch: 3990, loss 0.0828\n",
      "test epoch: 3990, loss: 0.87\n",
      "train epoch: 3991, loss 0.0830\n",
      "test epoch: 3991, loss: 0.87\n",
      "train epoch: 3992, loss 0.0834\n",
      "test epoch: 3992, loss: 0.87\n",
      "train epoch: 3993, loss 0.0829\n",
      "test epoch: 3993, loss: 0.87\n",
      "train epoch: 3994, loss 0.0839\n",
      "test epoch: 3994, loss: 0.87\n",
      "train epoch: 3995, loss 0.0830\n",
      "test epoch: 3995, loss: 0.87\n",
      "train epoch: 3996, loss 0.0831\n",
      "test epoch: 3996, loss: 0.87\n",
      "train epoch: 3997, loss 0.0833\n",
      "test epoch: 3997, loss: 0.87\n",
      "train epoch: 3998, loss 0.0829\n",
      "test epoch: 3998, loss: 0.87\n",
      "train epoch: 3999, loss 0.0830\n",
      "test epoch: 3999, loss: 0.87\n",
      "train epoch: 4000, loss 0.0830\n",
      "test epoch: 4000, loss: 0.87\n",
      "train epoch: 4001, loss 0.0829\n",
      "test epoch: 4001, loss: 0.87\n",
      "train epoch: 4002, loss 0.0836\n",
      "test epoch: 4002, loss: 0.87\n",
      "train epoch: 4003, loss 0.0828\n",
      "test epoch: 4003, loss: 0.87\n",
      "train epoch: 4004, loss 0.0833\n",
      "test epoch: 4004, loss: 0.87\n",
      "train epoch: 4005, loss 0.0832\n",
      "test epoch: 4005, loss: 0.87\n",
      "train epoch: 4006, loss 0.0829\n",
      "test epoch: 4006, loss: 0.87\n",
      "train epoch: 4007, loss 0.0836\n",
      "test epoch: 4007, loss: 0.87\n",
      "train epoch: 4008, loss 0.0828\n",
      "test epoch: 4008, loss: 0.87\n",
      "train epoch: 4009, loss 0.0829\n",
      "test epoch: 4009, loss: 0.87\n",
      "train epoch: 4010, loss 0.0831\n",
      "test epoch: 4010, loss: 0.87\n",
      "train epoch: 4011, loss 0.0828\n",
      "test epoch: 4011, loss: 0.87\n",
      "train epoch: 4012, loss 0.0828\n",
      "test epoch: 4012, loss: 0.87\n",
      "train epoch: 4013, loss 0.0829\n",
      "test epoch: 4013, loss: 0.87\n",
      "train epoch: 4014, loss 0.0828\n",
      "test epoch: 4014, loss: 0.87\n",
      "train epoch: 4015, loss 0.0828\n",
      "test epoch: 4015, loss: 0.87\n",
      "train epoch: 4016, loss 0.0829\n",
      "test epoch: 4016, loss: 0.87\n",
      "train epoch: 4017, loss 0.0830\n",
      "test epoch: 4017, loss: 0.87\n",
      "train epoch: 4018, loss 0.0828\n",
      "test epoch: 4018, loss: 0.87\n",
      "train epoch: 4019, loss 0.0829\n",
      "test epoch: 4019, loss: 0.87\n",
      "train epoch: 4020, loss 0.0830\n",
      "test epoch: 4020, loss: 0.87\n",
      "train epoch: 4021, loss 0.0827\n",
      "test epoch: 4021, loss: 0.87\n",
      "train epoch: 4022, loss 0.0827\n",
      "test epoch: 4022, loss: 0.87\n",
      "train epoch: 4023, loss 0.0828\n",
      "test epoch: 4023, loss: 0.87\n",
      "train epoch: 4024, loss 0.0827\n",
      "test epoch: 4024, loss: 0.87\n",
      "train epoch: 4025, loss 0.0828\n",
      "test epoch: 4025, loss: 0.87\n",
      "train epoch: 4026, loss 0.0830\n",
      "test epoch: 4026, loss: 0.87\n",
      "train epoch: 4027, loss 0.0828\n",
      "test epoch: 4027, loss: 0.87\n",
      "train epoch: 4028, loss 0.0829\n",
      "test epoch: 4028, loss: 0.87\n",
      "train epoch: 4029, loss 0.0829\n",
      "test epoch: 4029, loss: 0.87\n",
      "train epoch: 4030, loss 0.0828\n",
      "test epoch: 4030, loss: 0.87\n",
      "train epoch: 4031, loss 0.0829\n",
      "test epoch: 4031, loss: 0.87\n",
      "train epoch: 4032, loss 0.0828\n",
      "test epoch: 4032, loss: 0.87\n",
      "train epoch: 4033, loss 0.0828\n",
      "test epoch: 4033, loss: 0.87\n",
      "train epoch: 4034, loss 0.0828\n",
      "test epoch: 4034, loss: 0.87\n",
      "train epoch: 4035, loss 0.0830\n",
      "test epoch: 4035, loss: 0.87\n",
      "train epoch: 4036, loss 0.0830\n",
      "test epoch: 4036, loss: 0.87\n",
      "train epoch: 4037, loss 0.0827\n",
      "test epoch: 4037, loss: 0.87\n",
      "train epoch: 4038, loss 0.0829\n",
      "test epoch: 4038, loss: 0.87\n",
      "train epoch: 4039, loss 0.0828\n",
      "test epoch: 4039, loss: 0.87\n",
      "train epoch: 4040, loss 0.0827\n",
      "test epoch: 4040, loss: 0.87\n",
      "train epoch: 4041, loss 0.0829\n",
      "test epoch: 4041, loss: 0.87\n",
      "train epoch: 4042, loss 0.0829\n",
      "test epoch: 4042, loss: 0.87\n",
      "train epoch: 4043, loss 0.0827\n",
      "test epoch: 4043, loss: 0.87\n",
      "train epoch: 4044, loss 0.0830\n",
      "test epoch: 4044, loss: 0.87\n",
      "train epoch: 4045, loss 0.0830\n",
      "test epoch: 4045, loss: 0.87\n",
      "train epoch: 4046, loss 0.0828\n",
      "test epoch: 4046, loss: 0.87\n",
      "train epoch: 4047, loss 0.0828\n",
      "test epoch: 4047, loss: 0.87\n",
      "train epoch: 4048, loss 0.0828\n",
      "test epoch: 4048, loss: 0.87\n",
      "train epoch: 4049, loss 0.0828\n",
      "test epoch: 4049, loss: 0.87\n",
      "train epoch: 4050, loss 0.0828\n",
      "test epoch: 4050, loss: 0.87\n",
      "train epoch: 4051, loss 0.0831\n",
      "test epoch: 4051, loss: 0.87\n",
      "train epoch: 4052, loss 0.0828\n",
      "test epoch: 4052, loss: 0.87\n",
      "train epoch: 4053, loss 0.0829\n",
      "test epoch: 4053, loss: 0.87\n",
      "train epoch: 4054, loss 0.0829\n",
      "test epoch: 4054, loss: 0.87\n",
      "train epoch: 4055, loss 0.0828\n",
      "test epoch: 4055, loss: 0.87\n",
      "train epoch: 4056, loss 0.0827\n",
      "test epoch: 4056, loss: 0.87\n",
      "train epoch: 4057, loss 0.0829\n",
      "test epoch: 4057, loss: 0.87\n",
      "train epoch: 4058, loss 0.0828\n",
      "test epoch: 4058, loss: 0.87\n",
      "train epoch: 4059, loss 0.0828\n",
      "test epoch: 4059, loss: 0.87\n",
      "train epoch: 4060, loss 0.0829\n",
      "test epoch: 4060, loss: 0.87\n",
      "train epoch: 4061, loss 0.0831\n",
      "test epoch: 4061, loss: 0.87\n",
      "train epoch: 4062, loss 0.0828\n",
      "test epoch: 4062, loss: 0.87\n",
      "train epoch: 4063, loss 0.0831\n",
      "test epoch: 4063, loss: 0.87\n",
      "train epoch: 4064, loss 0.0833\n",
      "test epoch: 4064, loss: 0.87\n",
      "train epoch: 4065, loss 0.0829\n",
      "test epoch: 4065, loss: 0.87\n",
      "train epoch: 4066, loss 0.0838\n",
      "test epoch: 4066, loss: 0.87\n",
      "train epoch: 4067, loss 0.0828\n",
      "test epoch: 4067, loss: 0.87\n",
      "train epoch: 4068, loss 0.0832\n",
      "test epoch: 4068, loss: 0.87\n",
      "train epoch: 4069, loss 0.0831\n",
      "test epoch: 4069, loss: 0.87\n",
      "train epoch: 4070, loss 0.0828\n",
      "test epoch: 4070, loss: 0.87\n",
      "train epoch: 4071, loss 0.0829\n",
      "test epoch: 4071, loss: 0.87\n",
      "train epoch: 4072, loss 0.0828\n",
      "test epoch: 4072, loss: 0.87\n",
      "train epoch: 4073, loss 0.0828\n",
      "test epoch: 4073, loss: 0.87\n",
      "train epoch: 4074, loss 0.0835\n",
      "test epoch: 4074, loss: 0.87\n",
      "train epoch: 4075, loss 0.0828\n",
      "test epoch: 4075, loss: 0.87\n",
      "train epoch: 4076, loss 0.0834\n",
      "test epoch: 4076, loss: 0.87\n",
      "train epoch: 4077, loss 0.0833\n",
      "test epoch: 4077, loss: 0.87\n",
      "train epoch: 4078, loss 0.0831\n",
      "test epoch: 4078, loss: 0.87\n",
      "train epoch: 4079, loss 0.0836\n",
      "test epoch: 4079, loss: 0.87\n",
      "train epoch: 4080, loss 0.0828\n",
      "test epoch: 4080, loss: 0.87\n",
      "train epoch: 4081, loss 0.0830\n",
      "test epoch: 4081, loss: 0.87\n",
      "train epoch: 4082, loss 0.0832\n",
      "test epoch: 4082, loss: 0.87\n",
      "train epoch: 4083, loss 0.0828\n",
      "test epoch: 4083, loss: 0.87\n",
      "train epoch: 4084, loss 0.0828\n",
      "test epoch: 4084, loss: 0.87\n",
      "train epoch: 4085, loss 0.0828\n",
      "test epoch: 4085, loss: 0.87\n",
      "train epoch: 4086, loss 0.0828\n",
      "test epoch: 4086, loss: 0.87\n",
      "train epoch: 4087, loss 0.0830\n",
      "test epoch: 4087, loss: 0.87\n",
      "train epoch: 4088, loss 0.0830\n",
      "test epoch: 4088, loss: 0.87\n",
      "train epoch: 4089, loss 0.0829\n",
      "test epoch: 4089, loss: 0.87\n",
      "train epoch: 4090, loss 0.0828\n",
      "test epoch: 4090, loss: 0.87\n",
      "train epoch: 4091, loss 0.0829\n",
      "test epoch: 4091, loss: 0.87\n",
      "train epoch: 4092, loss 0.0830\n",
      "test epoch: 4092, loss: 0.87\n",
      "train epoch: 4093, loss 0.0829\n",
      "test epoch: 4093, loss: 0.87\n",
      "train epoch: 4094, loss 0.0829\n",
      "test epoch: 4094, loss: 0.87\n",
      "train epoch: 4095, loss 0.0829\n",
      "test epoch: 4095, loss: 0.87\n",
      "train epoch: 4096, loss 0.0834\n",
      "test epoch: 4096, loss: 0.87\n",
      "train epoch: 4097, loss 0.0829\n",
      "test epoch: 4097, loss: 0.87\n",
      "train epoch: 4098, loss 0.0840\n",
      "test epoch: 4098, loss: 0.87\n",
      "train epoch: 4099, loss 0.0829\n",
      "test epoch: 4099, loss: 0.87\n",
      "train epoch: 4100, loss 0.0831\n",
      "test epoch: 4100, loss: 0.87\n",
      "train epoch: 4101, loss 0.0833\n",
      "test epoch: 4101, loss: 0.87\n",
      "train epoch: 4102, loss 0.0828\n",
      "test epoch: 4102, loss: 0.87\n",
      "train epoch: 4103, loss 0.0831\n",
      "test epoch: 4103, loss: 0.87\n",
      "train epoch: 4104, loss 0.0828\n",
      "test epoch: 4104, loss: 0.87\n",
      "train epoch: 4105, loss 0.0829\n",
      "test epoch: 4105, loss: 0.87\n",
      "train epoch: 4106, loss 0.0828\n",
      "test epoch: 4106, loss: 0.87\n",
      "train epoch: 4107, loss 0.0830\n",
      "test epoch: 4107, loss: 0.87\n",
      "train epoch: 4108, loss 0.0829\n",
      "test epoch: 4108, loss: 0.87\n",
      "train epoch: 4109, loss 0.0829\n",
      "test epoch: 4109, loss: 0.87\n",
      "train epoch: 4110, loss 0.0829\n",
      "test epoch: 4110, loss: 0.87\n",
      "train epoch: 4111, loss 0.0829\n",
      "test epoch: 4111, loss: 0.87\n",
      "train epoch: 4112, loss 0.0830\n",
      "test epoch: 4112, loss: 0.87\n",
      "train epoch: 4113, loss 0.0829\n",
      "test epoch: 4113, loss: 0.87\n",
      "train epoch: 4114, loss 0.0828\n",
      "test epoch: 4114, loss: 0.87\n",
      "train epoch: 4115, loss 0.0828\n",
      "test epoch: 4115, loss: 0.87\n",
      "train epoch: 4116, loss 0.0828\n",
      "test epoch: 4116, loss: 0.87\n",
      "train epoch: 4117, loss 0.0829\n",
      "test epoch: 4117, loss: 0.87\n",
      "train epoch: 4118, loss 0.0830\n",
      "test epoch: 4118, loss: 0.87\n",
      "train epoch: 4119, loss 0.0827\n",
      "test epoch: 4119, loss: 0.87\n",
      "train epoch: 4120, loss 0.0831\n",
      "test epoch: 4120, loss: 0.87\n",
      "train epoch: 4121, loss 0.0832\n",
      "test epoch: 4121, loss: 0.87\n",
      "train epoch: 4122, loss 0.0829\n",
      "test epoch: 4122, loss: 0.87\n",
      "train epoch: 4123, loss 0.0838\n",
      "test epoch: 4123, loss: 0.87\n",
      "train epoch: 4124, loss 0.0830\n",
      "test epoch: 4124, loss: 0.87\n",
      "train epoch: 4125, loss 0.0830\n",
      "test epoch: 4125, loss: 0.87\n",
      "train epoch: 4126, loss 0.0833\n",
      "test epoch: 4126, loss: 0.87\n",
      "train epoch: 4127, loss 0.0829\n",
      "test epoch: 4127, loss: 0.87\n",
      "train epoch: 4128, loss 0.0828\n",
      "test epoch: 4128, loss: 0.87\n",
      "train epoch: 4129, loss 0.0830\n",
      "test epoch: 4129, loss: 0.87\n",
      "train epoch: 4130, loss 0.0831\n",
      "test epoch: 4130, loss: 0.87\n",
      "train epoch: 4131, loss 0.0828\n",
      "test epoch: 4131, loss: 0.87\n",
      "train epoch: 4132, loss 0.0835\n",
      "test epoch: 4132, loss: 0.87\n",
      "train epoch: 4133, loss 0.0830\n",
      "test epoch: 4133, loss: 0.87\n",
      "train epoch: 4134, loss 0.0829\n",
      "test epoch: 4134, loss: 0.87\n",
      "train epoch: 4135, loss 0.0834\n",
      "test epoch: 4135, loss: 0.87\n",
      "train epoch: 4136, loss 0.0828\n",
      "test epoch: 4136, loss: 0.87\n",
      "train epoch: 4137, loss 0.0828\n",
      "test epoch: 4137, loss: 0.87\n",
      "train epoch: 4138, loss 0.0831\n",
      "test epoch: 4138, loss: 0.87\n",
      "train epoch: 4139, loss 0.0828\n",
      "test epoch: 4139, loss: 0.87\n",
      "train epoch: 4140, loss 0.0827\n",
      "test epoch: 4140, loss: 0.87\n",
      "train epoch: 4141, loss 0.0830\n",
      "test epoch: 4141, loss: 0.87\n",
      "train epoch: 4142, loss 0.0830\n",
      "test epoch: 4142, loss: 0.87\n",
      "train epoch: 4143, loss 0.0828\n",
      "test epoch: 4143, loss: 0.87\n",
      "train epoch: 4144, loss 0.0831\n",
      "test epoch: 4144, loss: 0.87\n",
      "train epoch: 4145, loss 0.0830\n",
      "test epoch: 4145, loss: 0.87\n",
      "train epoch: 4146, loss 0.0828\n",
      "test epoch: 4146, loss: 0.87\n",
      "train epoch: 4147, loss 0.0834\n",
      "test epoch: 4147, loss: 0.87\n",
      "train epoch: 4148, loss 0.0829\n",
      "test epoch: 4148, loss: 0.87\n",
      "train epoch: 4149, loss 0.0828\n",
      "test epoch: 4149, loss: 0.87\n",
      "train epoch: 4150, loss 0.0833\n",
      "test epoch: 4150, loss: 0.87\n",
      "train epoch: 4151, loss 0.0828\n",
      "test epoch: 4151, loss: 0.87\n",
      "train epoch: 4152, loss 0.0828\n",
      "test epoch: 4152, loss: 0.87\n",
      "train epoch: 4153, loss 0.0833\n",
      "test epoch: 4153, loss: 0.87\n",
      "train epoch: 4154, loss 0.0828\n",
      "test epoch: 4154, loss: 0.87\n",
      "train epoch: 4155, loss 0.0828\n",
      "test epoch: 4155, loss: 0.87\n",
      "train epoch: 4156, loss 0.0835\n",
      "test epoch: 4156, loss: 0.87\n",
      "train epoch: 4157, loss 0.0828\n",
      "test epoch: 4157, loss: 0.87\n",
      "train epoch: 4158, loss 0.0829\n",
      "test epoch: 4158, loss: 0.87\n",
      "train epoch: 4159, loss 0.0833\n",
      "test epoch: 4159, loss: 0.87\n",
      "train epoch: 4160, loss 0.0829\n",
      "test epoch: 4160, loss: 0.87\n",
      "train epoch: 4161, loss 0.0829\n",
      "test epoch: 4161, loss: 0.87\n",
      "train epoch: 4162, loss 0.0830\n",
      "test epoch: 4162, loss: 0.87\n",
      "train epoch: 4163, loss 0.0829\n",
      "test epoch: 4163, loss: 0.87\n",
      "train epoch: 4164, loss 0.0827\n",
      "test epoch: 4164, loss: 0.87\n",
      "train epoch: 4165, loss 0.0829\n",
      "test epoch: 4165, loss: 0.87\n",
      "train epoch: 4166, loss 0.0830\n",
      "test epoch: 4166, loss: 0.87\n",
      "train epoch: 4167, loss 0.0827\n",
      "test epoch: 4167, loss: 0.87\n",
      "train epoch: 4168, loss 0.0830\n",
      "test epoch: 4168, loss: 0.87\n",
      "train epoch: 4169, loss 0.0830\n",
      "test epoch: 4169, loss: 0.87\n",
      "train epoch: 4170, loss 0.0827\n",
      "test epoch: 4170, loss: 0.87\n",
      "train epoch: 4171, loss 0.0836\n",
      "test epoch: 4171, loss: 0.87\n",
      "train epoch: 4172, loss 0.0828\n",
      "test epoch: 4172, loss: 0.87\n",
      "train epoch: 4173, loss 0.0829\n",
      "test epoch: 4173, loss: 0.87\n",
      "train epoch: 4174, loss 0.0832\n",
      "test epoch: 4174, loss: 0.87\n",
      "train epoch: 4175, loss 0.0827\n",
      "test epoch: 4175, loss: 0.87\n",
      "train epoch: 4176, loss 0.0828\n",
      "test epoch: 4176, loss: 0.87\n",
      "train epoch: 4177, loss 0.0830\n",
      "test epoch: 4177, loss: 0.87\n",
      "train epoch: 4178, loss 0.0829\n",
      "test epoch: 4178, loss: 0.87\n",
      "train epoch: 4179, loss 0.0827\n",
      "test epoch: 4179, loss: 0.87\n",
      "train epoch: 4180, loss 0.0832\n",
      "test epoch: 4180, loss: 0.87\n",
      "train epoch: 4181, loss 0.0830\n",
      "test epoch: 4181, loss: 0.87\n",
      "train epoch: 4182, loss 0.0828\n",
      "test epoch: 4182, loss: 0.87\n",
      "train epoch: 4183, loss 0.0833\n",
      "test epoch: 4183, loss: 0.87\n",
      "train epoch: 4184, loss 0.0829\n",
      "test epoch: 4184, loss: 0.87\n",
      "train epoch: 4185, loss 0.0829\n",
      "test epoch: 4185, loss: 0.87\n",
      "train epoch: 4186, loss 0.0833\n",
      "test epoch: 4186, loss: 0.87\n",
      "train epoch: 4187, loss 0.0828\n",
      "test epoch: 4187, loss: 0.87\n",
      "train epoch: 4188, loss 0.0827\n",
      "test epoch: 4188, loss: 0.87\n",
      "train epoch: 4189, loss 0.0829\n",
      "test epoch: 4189, loss: 0.87\n",
      "train epoch: 4190, loss 0.0829\n",
      "test epoch: 4190, loss: 0.87\n",
      "train epoch: 4191, loss 0.0828\n",
      "test epoch: 4191, loss: 0.87\n",
      "train epoch: 4192, loss 0.0828\n",
      "test epoch: 4192, loss: 0.87\n",
      "train epoch: 4193, loss 0.0827\n",
      "test epoch: 4193, loss: 0.87\n",
      "train epoch: 4194, loss 0.0828\n",
      "test epoch: 4194, loss: 0.87\n",
      "train epoch: 4195, loss 0.0828\n",
      "test epoch: 4195, loss: 0.87\n",
      "train epoch: 4196, loss 0.0830\n",
      "test epoch: 4196, loss: 0.87\n",
      "train epoch: 4197, loss 0.0828\n",
      "test epoch: 4197, loss: 0.87\n",
      "train epoch: 4198, loss 0.0831\n",
      "test epoch: 4198, loss: 0.87\n",
      "train epoch: 4199, loss 0.0827\n",
      "test epoch: 4199, loss: 0.87\n",
      "train epoch: 4200, loss 0.0827\n",
      "test epoch: 4200, loss: 0.87\n",
      "train epoch: 4201, loss 0.0827\n",
      "test epoch: 4201, loss: 0.87\n",
      "train epoch: 4202, loss 0.0827\n",
      "test epoch: 4202, loss: 0.87\n",
      "train epoch: 4203, loss 0.0827\n",
      "test epoch: 4203, loss: 0.87\n",
      "train epoch: 4204, loss 0.0827\n",
      "test epoch: 4204, loss: 0.87\n",
      "train epoch: 4205, loss 0.0831\n",
      "test epoch: 4205, loss: 0.87\n",
      "train epoch: 4206, loss 0.0828\n",
      "test epoch: 4206, loss: 0.87\n",
      "train epoch: 4207, loss 0.0831\n",
      "test epoch: 4207, loss: 0.87\n",
      "train epoch: 4208, loss 0.0833\n",
      "test epoch: 4208, loss: 0.87\n",
      "train epoch: 4209, loss 0.0829\n",
      "test epoch: 4209, loss: 0.87\n",
      "train epoch: 4210, loss 0.0838\n",
      "test epoch: 4210, loss: 0.87\n",
      "train epoch: 4211, loss 0.0827\n",
      "test epoch: 4211, loss: 0.87\n",
      "train epoch: 4212, loss 0.0831\n",
      "test epoch: 4212, loss: 0.87\n",
      "train epoch: 4213, loss 0.0829\n",
      "test epoch: 4213, loss: 0.87\n",
      "train epoch: 4214, loss 0.0827\n",
      "test epoch: 4214, loss: 0.87\n",
      "train epoch: 4215, loss 0.0828\n",
      "test epoch: 4215, loss: 0.87\n",
      "train epoch: 4216, loss 0.0828\n",
      "test epoch: 4216, loss: 0.87\n",
      "train epoch: 4217, loss 0.0827\n",
      "test epoch: 4217, loss: 0.87\n",
      "train epoch: 4218, loss 0.0835\n",
      "test epoch: 4218, loss: 0.87\n",
      "train epoch: 4219, loss 0.0827\n",
      "test epoch: 4219, loss: 0.87\n",
      "train epoch: 4220, loss 0.0832\n",
      "test epoch: 4220, loss: 0.87\n",
      "train epoch: 4221, loss 0.0833\n",
      "test epoch: 4221, loss: 0.87\n",
      "train epoch: 4222, loss 0.0829\n",
      "test epoch: 4222, loss: 0.87\n",
      "train epoch: 4223, loss 0.0836\n",
      "test epoch: 4223, loss: 0.87\n",
      "train epoch: 4224, loss 0.0827\n",
      "test epoch: 4224, loss: 0.87\n",
      "train epoch: 4225, loss 0.0830\n",
      "test epoch: 4225, loss: 0.87\n",
      "train epoch: 4226, loss 0.0830\n",
      "test epoch: 4226, loss: 0.87\n",
      "train epoch: 4227, loss 0.0827\n",
      "test epoch: 4227, loss: 0.87\n",
      "train epoch: 4228, loss 0.0827\n",
      "test epoch: 4228, loss: 0.87\n",
      "train epoch: 4229, loss 0.0828\n",
      "test epoch: 4229, loss: 0.87\n",
      "train epoch: 4230, loss 0.0827\n",
      "test epoch: 4230, loss: 0.87\n",
      "train epoch: 4231, loss 0.0828\n",
      "test epoch: 4231, loss: 0.87\n",
      "train epoch: 4232, loss 0.0831\n",
      "test epoch: 4232, loss: 0.87\n",
      "train epoch: 4233, loss 0.0829\n",
      "test epoch: 4233, loss: 0.87\n",
      "train epoch: 4234, loss 0.0829\n",
      "test epoch: 4234, loss: 0.87\n",
      "train epoch: 4235, loss 0.0829\n",
      "test epoch: 4235, loss: 0.87\n",
      "train epoch: 4236, loss 0.0828\n",
      "test epoch: 4236, loss: 0.87\n",
      "train epoch: 4237, loss 0.0827\n",
      "test epoch: 4237, loss: 0.87\n",
      "train epoch: 4238, loss 0.0827\n",
      "test epoch: 4238, loss: 0.87\n",
      "train epoch: 4239, loss 0.0827\n",
      "test epoch: 4239, loss: 0.87\n",
      "train epoch: 4240, loss 0.0827\n",
      "test epoch: 4240, loss: 0.87\n",
      "train epoch: 4241, loss 0.0831\n",
      "test epoch: 4241, loss: 0.87\n",
      "train epoch: 4242, loss 0.0828\n",
      "test epoch: 4242, loss: 0.87\n",
      "train epoch: 4243, loss 0.0832\n",
      "test epoch: 4243, loss: 0.87\n",
      "train epoch: 4244, loss 0.0831\n",
      "test epoch: 4244, loss: 0.87\n",
      "train epoch: 4245, loss 0.0828\n",
      "test epoch: 4245, loss: 0.87\n",
      "train epoch: 4246, loss 0.0835\n",
      "test epoch: 4246, loss: 0.87\n",
      "train epoch: 4247, loss 0.0828\n",
      "test epoch: 4247, loss: 0.87\n",
      "train epoch: 4248, loss 0.0830\n",
      "test epoch: 4248, loss: 0.87\n",
      "train epoch: 4249, loss 0.0830\n",
      "test epoch: 4249, loss: 0.87\n",
      "train epoch: 4250, loss 0.0829\n",
      "test epoch: 4250, loss: 0.87\n",
      "train epoch: 4251, loss 0.0827\n",
      "test epoch: 4251, loss: 0.87\n",
      "train epoch: 4252, loss 0.0828\n",
      "test epoch: 4252, loss: 0.87\n",
      "train epoch: 4253, loss 0.0831\n",
      "test epoch: 4253, loss: 0.87\n",
      "train epoch: 4254, loss 0.0828\n",
      "test epoch: 4254, loss: 0.87\n",
      "train epoch: 4255, loss 0.0830\n",
      "test epoch: 4255, loss: 0.87\n",
      "train epoch: 4256, loss 0.0833\n",
      "test epoch: 4256, loss: 0.87\n",
      "train epoch: 4257, loss 0.0829\n",
      "test epoch: 4257, loss: 0.87\n",
      "train epoch: 4258, loss 0.0839\n",
      "test epoch: 4258, loss: 0.87\n",
      "train epoch: 4259, loss 0.0829\n",
      "test epoch: 4259, loss: 0.87\n",
      "train epoch: 4260, loss 0.0831\n",
      "test epoch: 4260, loss: 0.87\n",
      "train epoch: 4261, loss 0.0834\n",
      "test epoch: 4261, loss: 0.87\n",
      "train epoch: 4262, loss 0.0827\n",
      "test epoch: 4262, loss: 0.87\n",
      "train epoch: 4263, loss 0.0832\n",
      "test epoch: 4263, loss: 0.87\n",
      "train epoch: 4264, loss 0.0828\n",
      "test epoch: 4264, loss: 0.87\n",
      "train epoch: 4265, loss 0.0830\n",
      "test epoch: 4265, loss: 0.87\n",
      "train epoch: 4266, loss 0.0833\n",
      "test epoch: 4266, loss: 0.87\n",
      "train epoch: 4267, loss 0.0828\n",
      "test epoch: 4267, loss: 0.87\n",
      "train epoch: 4268, loss 0.0837\n",
      "test epoch: 4268, loss: 0.87\n",
      "train epoch: 4269, loss 0.0828\n",
      "test epoch: 4269, loss: 0.87\n",
      "train epoch: 4270, loss 0.0829\n",
      "test epoch: 4270, loss: 0.87\n",
      "train epoch: 4271, loss 0.0831\n",
      "test epoch: 4271, loss: 0.87\n",
      "train epoch: 4272, loss 0.0828\n",
      "test epoch: 4272, loss: 0.87\n",
      "train epoch: 4273, loss 0.0827\n",
      "test epoch: 4273, loss: 0.87\n",
      "train epoch: 4274, loss 0.0829\n",
      "test epoch: 4274, loss: 0.87\n",
      "train epoch: 4275, loss 0.0830\n",
      "test epoch: 4275, loss: 0.87\n",
      "train epoch: 4276, loss 0.0827\n",
      "test epoch: 4276, loss: 0.87\n",
      "train epoch: 4277, loss 0.0832\n",
      "test epoch: 4277, loss: 0.87\n",
      "train epoch: 4278, loss 0.0829\n",
      "test epoch: 4278, loss: 0.87\n",
      "train epoch: 4279, loss 0.0828\n",
      "test epoch: 4279, loss: 0.87\n",
      "train epoch: 4280, loss 0.0835\n",
      "test epoch: 4280, loss: 0.87\n",
      "train epoch: 4281, loss 0.0827\n",
      "test epoch: 4281, loss: 0.87\n",
      "train epoch: 4282, loss 0.0829\n",
      "test epoch: 4282, loss: 0.87\n",
      "train epoch: 4283, loss 0.0832\n",
      "test epoch: 4283, loss: 0.87\n",
      "train epoch: 4284, loss 0.0827\n",
      "test epoch: 4284, loss: 0.87\n",
      "train epoch: 4285, loss 0.0828\n",
      "test epoch: 4285, loss: 0.87\n",
      "train epoch: 4286, loss 0.0831\n",
      "test epoch: 4286, loss: 0.87\n",
      "train epoch: 4287, loss 0.0828\n",
      "test epoch: 4287, loss: 0.87\n",
      "train epoch: 4288, loss 0.0827\n",
      "test epoch: 4288, loss: 0.87\n",
      "train epoch: 4289, loss 0.0829\n",
      "test epoch: 4289, loss: 0.87\n",
      "train epoch: 4290, loss 0.0829\n",
      "test epoch: 4290, loss: 0.87\n",
      "train epoch: 4291, loss 0.0827\n",
      "test epoch: 4291, loss: 0.87\n",
      "train epoch: 4292, loss 0.0829\n",
      "test epoch: 4292, loss: 0.87\n",
      "train epoch: 4293, loss 0.0830\n",
      "test epoch: 4293, loss: 0.87\n",
      "train epoch: 4294, loss 0.0827\n",
      "test epoch: 4294, loss: 0.87\n",
      "train epoch: 4295, loss 0.0834\n",
      "test epoch: 4295, loss: 0.87\n",
      "train epoch: 4296, loss 0.0828\n",
      "test epoch: 4296, loss: 0.87\n",
      "train epoch: 4297, loss 0.0828\n",
      "test epoch: 4297, loss: 0.87\n",
      "train epoch: 4298, loss 0.0832\n",
      "test epoch: 4298, loss: 0.87\n",
      "train epoch: 4299, loss 0.0827\n",
      "test epoch: 4299, loss: 0.87\n",
      "train epoch: 4300, loss 0.0828\n",
      "test epoch: 4300, loss: 0.87\n",
      "train epoch: 4301, loss 0.0829\n",
      "test epoch: 4301, loss: 0.87\n",
      "train epoch: 4302, loss 0.0828\n",
      "test epoch: 4302, loss: 0.87\n",
      "train epoch: 4303, loss 0.0827\n",
      "test epoch: 4303, loss: 0.87\n",
      "train epoch: 4304, loss 0.0830\n",
      "test epoch: 4304, loss: 0.87\n",
      "train epoch: 4305, loss 0.0831\n",
      "test epoch: 4305, loss: 0.87\n",
      "train epoch: 4306, loss 0.0828\n",
      "test epoch: 4306, loss: 0.87\n",
      "train epoch: 4307, loss 0.0835\n",
      "test epoch: 4307, loss: 0.87\n",
      "train epoch: 4308, loss 0.0830\n",
      "test epoch: 4308, loss: 0.87\n",
      "train epoch: 4309, loss 0.0829\n",
      "test epoch: 4309, loss: 0.87\n",
      "train epoch: 4310, loss 0.0834\n",
      "test epoch: 4310, loss: 0.87\n",
      "train epoch: 4311, loss 0.0828\n",
      "test epoch: 4311, loss: 0.87\n",
      "train epoch: 4312, loss 0.0829\n",
      "test epoch: 4312, loss: 0.87\n",
      "train epoch: 4313, loss 0.0830\n",
      "test epoch: 4313, loss: 0.87\n",
      "train epoch: 4314, loss 0.0828\n",
      "test epoch: 4314, loss: 0.87\n",
      "train epoch: 4315, loss 0.0827\n",
      "test epoch: 4315, loss: 0.87\n",
      "train epoch: 4316, loss 0.0828\n",
      "test epoch: 4316, loss: 0.87\n",
      "train epoch: 4317, loss 0.0827\n",
      "test epoch: 4317, loss: 0.87\n",
      "train epoch: 4318, loss 0.0828\n",
      "test epoch: 4318, loss: 0.87\n",
      "train epoch: 4319, loss 0.0830\n",
      "test epoch: 4319, loss: 0.87\n",
      "train epoch: 4320, loss 0.0828\n",
      "test epoch: 4320, loss: 0.87\n",
      "train epoch: 4321, loss 0.0829\n",
      "test epoch: 4321, loss: 0.87\n",
      "train epoch: 4322, loss 0.0829\n",
      "test epoch: 4322, loss: 0.87\n",
      "train epoch: 4323, loss 0.0828\n",
      "test epoch: 4323, loss: 0.87\n",
      "train epoch: 4324, loss 0.0829\n",
      "test epoch: 4324, loss: 0.87\n",
      "train epoch: 4325, loss 0.0828\n",
      "test epoch: 4325, loss: 0.87\n",
      "train epoch: 4326, loss 0.0828\n",
      "test epoch: 4326, loss: 0.87\n",
      "train epoch: 4327, loss 0.0828\n",
      "test epoch: 4327, loss: 0.87\n",
      "train epoch: 4328, loss 0.0830\n",
      "test epoch: 4328, loss: 0.87\n",
      "train epoch: 4329, loss 0.0829\n",
      "test epoch: 4329, loss: 0.87\n",
      "train epoch: 4330, loss 0.0827\n",
      "test epoch: 4330, loss: 0.87\n",
      "train epoch: 4331, loss 0.0829\n",
      "test epoch: 4331, loss: 0.87\n",
      "train epoch: 4332, loss 0.0831\n",
      "test epoch: 4332, loss: 0.87\n",
      "train epoch: 4333, loss 0.0828\n",
      "test epoch: 4333, loss: 0.87\n",
      "train epoch: 4334, loss 0.0833\n",
      "test epoch: 4334, loss: 0.87\n",
      "train epoch: 4335, loss 0.0828\n",
      "test epoch: 4335, loss: 0.87\n",
      "train epoch: 4336, loss 0.0828\n",
      "test epoch: 4336, loss: 0.87\n",
      "train epoch: 4337, loss 0.0832\n",
      "test epoch: 4337, loss: 0.87\n",
      "train epoch: 4338, loss 0.0827\n",
      "test epoch: 4338, loss: 0.87\n",
      "train epoch: 4339, loss 0.0828\n",
      "test epoch: 4339, loss: 0.87\n",
      "train epoch: 4340, loss 0.0829\n",
      "test epoch: 4340, loss: 0.87\n",
      "train epoch: 4341, loss 0.0828\n",
      "test epoch: 4341, loss: 0.87\n",
      "train epoch: 4342, loss 0.0827\n",
      "test epoch: 4342, loss: 0.87\n",
      "train epoch: 4343, loss 0.0829\n",
      "test epoch: 4343, loss: 0.87\n",
      "train epoch: 4344, loss 0.0830\n",
      "test epoch: 4344, loss: 0.87\n",
      "train epoch: 4345, loss 0.0827\n",
      "test epoch: 4345, loss: 0.87\n",
      "train epoch: 4346, loss 0.0829\n",
      "test epoch: 4346, loss: 0.87\n",
      "train epoch: 4347, loss 0.0832\n",
      "test epoch: 4347, loss: 0.87\n",
      "train epoch: 4348, loss 0.0827\n",
      "test epoch: 4348, loss: 0.87\n",
      "train epoch: 4349, loss 0.0832\n",
      "test epoch: 4349, loss: 0.87\n",
      "train epoch: 4350, loss 0.0830\n",
      "test epoch: 4350, loss: 0.87\n",
      "train epoch: 4351, loss 0.0827\n",
      "test epoch: 4351, loss: 0.87\n",
      "train epoch: 4352, loss 0.0834\n",
      "test epoch: 4352, loss: 0.87\n",
      "train epoch: 4353, loss 0.0828\n",
      "test epoch: 4353, loss: 0.87\n",
      "train epoch: 4354, loss 0.0828\n",
      "test epoch: 4354, loss: 0.87\n",
      "train epoch: 4355, loss 0.0831\n",
      "test epoch: 4355, loss: 0.87\n",
      "train epoch: 4356, loss 0.0828\n",
      "test epoch: 4356, loss: 0.87\n",
      "train epoch: 4357, loss 0.0827\n",
      "test epoch: 4357, loss: 0.87\n",
      "train epoch: 4358, loss 0.0829\n",
      "test epoch: 4358, loss: 0.87\n",
      "train epoch: 4359, loss 0.0829\n",
      "test epoch: 4359, loss: 0.87\n",
      "train epoch: 4360, loss 0.0827\n",
      "test epoch: 4360, loss: 0.87\n",
      "train epoch: 4361, loss 0.0828\n",
      "test epoch: 4361, loss: 0.87\n",
      "train epoch: 4362, loss 0.0827\n",
      "test epoch: 4362, loss: 0.87\n",
      "train epoch: 4363, loss 0.0827\n",
      "test epoch: 4363, loss: 0.87\n",
      "train epoch: 4364, loss 0.0827\n",
      "test epoch: 4364, loss: 0.87\n",
      "train epoch: 4365, loss 0.0831\n",
      "test epoch: 4365, loss: 0.87\n",
      "train epoch: 4366, loss 0.0828\n",
      "test epoch: 4366, loss: 0.87\n",
      "train epoch: 4367, loss 0.0830\n",
      "test epoch: 4367, loss: 0.87\n",
      "train epoch: 4368, loss 0.0828\n",
      "test epoch: 4368, loss: 0.87\n",
      "train epoch: 4369, loss 0.0828\n",
      "test epoch: 4369, loss: 0.87\n",
      "train epoch: 4370, loss 0.0827\n",
      "test epoch: 4370, loss: 0.87\n",
      "train epoch: 4371, loss 0.0829\n",
      "test epoch: 4371, loss: 0.87\n",
      "train epoch: 4372, loss 0.0830\n",
      "test epoch: 4372, loss: 0.87\n",
      "train epoch: 4373, loss 0.0829\n",
      "test epoch: 4373, loss: 0.87\n",
      "train epoch: 4374, loss 0.0830\n",
      "test epoch: 4374, loss: 0.87\n",
      "train epoch: 4375, loss 0.0829\n",
      "test epoch: 4375, loss: 0.87\n",
      "train epoch: 4376, loss 0.0827\n",
      "test epoch: 4376, loss: 0.87\n",
      "train epoch: 4377, loss 0.0836\n",
      "test epoch: 4377, loss: 0.87\n",
      "train epoch: 4378, loss 0.0827\n",
      "test epoch: 4378, loss: 0.87\n",
      "train epoch: 4379, loss 0.0829\n",
      "test epoch: 4379, loss: 0.87\n",
      "train epoch: 4380, loss 0.0829\n",
      "test epoch: 4380, loss: 0.87\n",
      "train epoch: 4381, loss 0.0827\n",
      "test epoch: 4381, loss: 0.87\n",
      "train epoch: 4382, loss 0.0830\n",
      "test epoch: 4382, loss: 0.87\n",
      "train epoch: 4383, loss 0.0828\n",
      "test epoch: 4383, loss: 0.87\n",
      "train epoch: 4384, loss 0.0826\n",
      "test epoch: 4384, loss: 0.87\n",
      "train epoch: 4385, loss 0.0827\n",
      "test epoch: 4385, loss: 0.87\n",
      "train epoch: 4386, loss 0.0828\n",
      "test epoch: 4386, loss: 0.87\n",
      "train epoch: 4387, loss 0.0826\n",
      "test epoch: 4387, loss: 0.87\n",
      "train epoch: 4388, loss 0.0828\n",
      "test epoch: 4388, loss: 0.87\n",
      "train epoch: 4389, loss 0.0830\n",
      "test epoch: 4389, loss: 0.87\n",
      "train epoch: 4390, loss 0.0827\n",
      "test epoch: 4390, loss: 0.87\n",
      "train epoch: 4391, loss 0.0827\n",
      "test epoch: 4391, loss: 0.87\n",
      "train epoch: 4392, loss 0.0827\n",
      "test epoch: 4392, loss: 0.87\n",
      "train epoch: 4393, loss 0.0828\n",
      "test epoch: 4393, loss: 0.87\n",
      "train epoch: 4394, loss 0.0828\n",
      "test epoch: 4394, loss: 0.87\n",
      "train epoch: 4395, loss 0.0830\n",
      "test epoch: 4395, loss: 0.87\n",
      "train epoch: 4396, loss 0.0830\n",
      "test epoch: 4396, loss: 0.87\n",
      "train epoch: 4397, loss 0.0828\n",
      "test epoch: 4397, loss: 0.87\n",
      "train epoch: 4398, loss 0.0829\n",
      "test epoch: 4398, loss: 0.87\n",
      "train epoch: 4399, loss 0.0828\n",
      "test epoch: 4399, loss: 0.87\n",
      "train epoch: 4400, loss 0.0827\n",
      "test epoch: 4400, loss: 0.87\n",
      "train epoch: 4401, loss 0.0833\n",
      "test epoch: 4401, loss: 0.87\n",
      "train epoch: 4402, loss 0.0828\n",
      "test epoch: 4402, loss: 0.87\n",
      "train epoch: 4403, loss 0.0827\n",
      "test epoch: 4403, loss: 0.87\n",
      "train epoch: 4404, loss 0.0829\n",
      "test epoch: 4404, loss: 0.87\n",
      "train epoch: 4405, loss 0.0827\n",
      "test epoch: 4405, loss: 0.87\n",
      "train epoch: 4406, loss 0.0828\n",
      "test epoch: 4406, loss: 0.87\n",
      "train epoch: 4407, loss 0.0831\n",
      "test epoch: 4407, loss: 0.87\n",
      "train epoch: 4408, loss 0.0827\n",
      "test epoch: 4408, loss: 0.87\n",
      "train epoch: 4409, loss 0.0827\n",
      "test epoch: 4409, loss: 0.87\n",
      "train epoch: 4410, loss 0.0828\n",
      "test epoch: 4410, loss: 0.87\n",
      "train epoch: 4411, loss 0.0827\n",
      "test epoch: 4411, loss: 0.87\n",
      "train epoch: 4412, loss 0.0826\n",
      "test epoch: 4412, loss: 0.87\n",
      "train epoch: 4413, loss 0.0829\n",
      "test epoch: 4413, loss: 0.87\n",
      "train epoch: 4414, loss 0.0830\n",
      "test epoch: 4414, loss: 0.87\n",
      "train epoch: 4415, loss 0.0827\n",
      "test epoch: 4415, loss: 0.87\n",
      "train epoch: 4416, loss 0.0828\n",
      "test epoch: 4416, loss: 0.87\n",
      "train epoch: 4417, loss 0.0828\n",
      "test epoch: 4417, loss: 0.87\n",
      "train epoch: 4418, loss 0.0828\n",
      "test epoch: 4418, loss: 0.87\n",
      "train epoch: 4419, loss 0.0829\n",
      "test epoch: 4419, loss: 0.87\n",
      "train epoch: 4420, loss 0.0828\n",
      "test epoch: 4420, loss: 0.87\n",
      "train epoch: 4421, loss 0.0828\n",
      "test epoch: 4421, loss: 0.87\n",
      "train epoch: 4422, loss 0.0829\n",
      "test epoch: 4422, loss: 0.87\n",
      "train epoch: 4423, loss 0.0827\n",
      "test epoch: 4423, loss: 0.87\n",
      "train epoch: 4424, loss 0.0828\n",
      "test epoch: 4424, loss: 0.87\n",
      "train epoch: 4425, loss 0.0828\n",
      "test epoch: 4425, loss: 0.87\n",
      "train epoch: 4426, loss 0.0827\n",
      "test epoch: 4426, loss: 0.87\n",
      "train epoch: 4427, loss 0.0827\n",
      "test epoch: 4427, loss: 0.87\n",
      "train epoch: 4428, loss 0.0829\n",
      "test epoch: 4428, loss: 0.87\n",
      "train epoch: 4429, loss 0.0829\n",
      "test epoch: 4429, loss: 0.87\n",
      "train epoch: 4430, loss 0.0827\n",
      "test epoch: 4430, loss: 0.87\n",
      "train epoch: 4431, loss 0.0828\n",
      "test epoch: 4431, loss: 0.87\n",
      "train epoch: 4432, loss 0.0828\n",
      "test epoch: 4432, loss: 0.87\n",
      "train epoch: 4433, loss 0.0827\n",
      "test epoch: 4433, loss: 0.87\n",
      "train epoch: 4434, loss 0.0830\n",
      "test epoch: 4434, loss: 0.87\n",
      "train epoch: 4435, loss 0.0829\n",
      "test epoch: 4435, loss: 0.87\n",
      "train epoch: 4436, loss 0.0827\n",
      "test epoch: 4436, loss: 0.87\n",
      "train epoch: 4437, loss 0.0833\n",
      "test epoch: 4437, loss: 0.87\n",
      "train epoch: 4438, loss 0.0831\n",
      "test epoch: 4438, loss: 0.87\n",
      "train epoch: 4439, loss 0.0829\n",
      "test epoch: 4439, loss: 0.87\n",
      "train epoch: 4440, loss 0.0835\n",
      "test epoch: 4440, loss: 0.87\n",
      "train epoch: 4441, loss 0.0826\n",
      "test epoch: 4441, loss: 0.87\n",
      "train epoch: 4442, loss 0.0830\n",
      "test epoch: 4442, loss: 0.87\n",
      "train epoch: 4443, loss 0.0828\n",
      "test epoch: 4443, loss: 0.87\n",
      "train epoch: 4444, loss 0.0827\n",
      "test epoch: 4444, loss: 0.87\n",
      "train epoch: 4445, loss 0.0827\n",
      "test epoch: 4445, loss: 0.87\n",
      "train epoch: 4446, loss 0.0827\n",
      "test epoch: 4446, loss: 0.87\n",
      "train epoch: 4447, loss 0.0826\n",
      "test epoch: 4447, loss: 0.87\n",
      "train epoch: 4448, loss 0.0827\n",
      "test epoch: 4448, loss: 0.87\n",
      "train epoch: 4449, loss 0.0830\n",
      "test epoch: 4449, loss: 0.87\n",
      "train epoch: 4450, loss 0.0827\n",
      "test epoch: 4450, loss: 0.87\n",
      "train epoch: 4451, loss 0.0827\n",
      "test epoch: 4451, loss: 0.87\n",
      "train epoch: 4452, loss 0.0827\n",
      "test epoch: 4452, loss: 0.87\n",
      "train epoch: 4453, loss 0.0827\n",
      "test epoch: 4453, loss: 0.87\n",
      "train epoch: 4454, loss 0.0829\n",
      "test epoch: 4454, loss: 0.87\n",
      "train epoch: 4455, loss 0.0828\n",
      "test epoch: 4455, loss: 0.87\n",
      "train epoch: 4456, loss 0.0826\n",
      "test epoch: 4456, loss: 0.87\n",
      "train epoch: 4457, loss 0.0827\n",
      "test epoch: 4457, loss: 0.87\n",
      "train epoch: 4458, loss 0.0827\n",
      "test epoch: 4458, loss: 0.87\n",
      "train epoch: 4459, loss 0.0826\n",
      "test epoch: 4459, loss: 0.87\n",
      "train epoch: 4460, loss 0.0827\n",
      "test epoch: 4460, loss: 0.87\n",
      "train epoch: 4461, loss 0.0831\n",
      "test epoch: 4461, loss: 0.87\n",
      "train epoch: 4462, loss 0.0827\n",
      "test epoch: 4462, loss: 0.87\n",
      "train epoch: 4463, loss 0.0829\n",
      "test epoch: 4463, loss: 0.87\n",
      "train epoch: 4464, loss 0.0827\n",
      "test epoch: 4464, loss: 0.87\n",
      "train epoch: 4465, loss 0.0826\n",
      "test epoch: 4465, loss: 0.87\n",
      "train epoch: 4466, loss 0.0826\n",
      "test epoch: 4466, loss: 0.87\n",
      "train epoch: 4467, loss 0.0829\n",
      "test epoch: 4467, loss: 0.87\n",
      "train epoch: 4468, loss 0.0830\n",
      "test epoch: 4468, loss: 0.87\n",
      "train epoch: 4469, loss 0.0828\n",
      "test epoch: 4469, loss: 0.87\n",
      "train epoch: 4470, loss 0.0832\n",
      "test epoch: 4470, loss: 0.87\n",
      "train epoch: 4471, loss 0.0829\n",
      "test epoch: 4471, loss: 0.87\n",
      "train epoch: 4472, loss 0.0828\n",
      "test epoch: 4472, loss: 0.87\n",
      "train epoch: 4473, loss 0.0833\n",
      "test epoch: 4473, loss: 0.87\n",
      "train epoch: 4474, loss 0.0827\n",
      "test epoch: 4474, loss: 0.87\n",
      "train epoch: 4475, loss 0.0831\n",
      "test epoch: 4475, loss: 0.87\n",
      "train epoch: 4476, loss 0.0829\n",
      "test epoch: 4476, loss: 0.87\n",
      "train epoch: 4477, loss 0.0832\n",
      "test epoch: 4477, loss: 0.87\n",
      "train epoch: 4478, loss 0.0832\n",
      "test epoch: 4478, loss: 0.87\n",
      "train epoch: 4479, loss 0.0831\n",
      "test epoch: 4479, loss: 0.87\n",
      "train epoch: 4480, loss 0.0836\n",
      "test epoch: 4480, loss: 0.87\n",
      "train epoch: 4481, loss 0.0830\n",
      "test epoch: 4481, loss: 0.87\n",
      "train epoch: 4482, loss 0.0834\n",
      "test epoch: 4482, loss: 0.87\n",
      "train epoch: 4483, loss 0.0834\n",
      "test epoch: 4483, loss: 0.87\n",
      "train epoch: 4484, loss 0.0829\n",
      "test epoch: 4484, loss: 0.87\n",
      "train epoch: 4485, loss 0.0832\n",
      "test epoch: 4485, loss: 0.87\n",
      "train epoch: 4486, loss 0.0833\n",
      "test epoch: 4486, loss: 0.87\n",
      "train epoch: 4487, loss 0.0828\n",
      "test epoch: 4487, loss: 0.87\n",
      "train epoch: 4488, loss 0.0835\n",
      "test epoch: 4488, loss: 0.87\n",
      "train epoch: 4489, loss 0.0829\n",
      "test epoch: 4489, loss: 0.87\n",
      "train epoch: 4490, loss 0.0829\n",
      "test epoch: 4490, loss: 0.87\n",
      "train epoch: 4491, loss 0.0836\n",
      "test epoch: 4491, loss: 0.87\n",
      "train epoch: 4492, loss 0.0827\n",
      "test epoch: 4492, loss: 0.87\n",
      "train epoch: 4493, loss 0.0831\n",
      "test epoch: 4493, loss: 0.87\n",
      "train epoch: 4494, loss 0.0834\n",
      "test epoch: 4494, loss: 0.87\n",
      "train epoch: 4495, loss 0.0828\n",
      "test epoch: 4495, loss: 0.87\n",
      "train epoch: 4496, loss 0.0827\n",
      "test epoch: 4496, loss: 0.87\n",
      "train epoch: 4497, loss 0.0830\n",
      "test epoch: 4497, loss: 0.87\n",
      "train epoch: 4498, loss 0.0828\n",
      "test epoch: 4498, loss: 0.87\n",
      "train epoch: 4499, loss 0.0828\n",
      "test epoch: 4499, loss: 0.87\n",
      "train epoch: 4500, loss 0.0832\n",
      "test epoch: 4500, loss: 0.87\n",
      "train epoch: 4501, loss 0.0829\n",
      "test epoch: 4501, loss: 0.87\n",
      "train epoch: 4502, loss 0.0827\n",
      "test epoch: 4502, loss: 0.87\n",
      "train epoch: 4503, loss 0.0832\n",
      "test epoch: 4503, loss: 0.87\n",
      "train epoch: 4504, loss 0.0828\n",
      "test epoch: 4504, loss: 0.87\n",
      "train epoch: 4505, loss 0.0828\n",
      "test epoch: 4505, loss: 0.87\n",
      "train epoch: 4506, loss 0.0830\n",
      "test epoch: 4506, loss: 0.87\n",
      "train epoch: 4507, loss 0.0829\n",
      "test epoch: 4507, loss: 0.87\n",
      "train epoch: 4508, loss 0.0827\n",
      "test epoch: 4508, loss: 0.87\n",
      "train epoch: 4509, loss 0.0828\n",
      "test epoch: 4509, loss: 0.87\n",
      "train epoch: 4510, loss 0.0831\n",
      "test epoch: 4510, loss: 0.87\n",
      "train epoch: 4511, loss 0.0827\n",
      "test epoch: 4511, loss: 0.87\n",
      "train epoch: 4512, loss 0.0829\n",
      "test epoch: 4512, loss: 0.87\n",
      "train epoch: 4513, loss 0.0832\n",
      "test epoch: 4513, loss: 0.87\n",
      "train epoch: 4514, loss 0.0827\n",
      "test epoch: 4514, loss: 0.87\n",
      "train epoch: 4515, loss 0.0834\n",
      "test epoch: 4515, loss: 0.87\n",
      "train epoch: 4516, loss 0.0828\n",
      "test epoch: 4516, loss: 0.87\n",
      "train epoch: 4517, loss 0.0828\n",
      "test epoch: 4517, loss: 0.87\n",
      "train epoch: 4518, loss 0.0832\n",
      "test epoch: 4518, loss: 0.87\n",
      "train epoch: 4519, loss 0.0827\n",
      "test epoch: 4519, loss: 0.87\n",
      "train epoch: 4520, loss 0.0829\n",
      "test epoch: 4520, loss: 0.87\n",
      "train epoch: 4521, loss 0.0829\n",
      "test epoch: 4521, loss: 0.87\n",
      "train epoch: 4522, loss 0.0827\n",
      "test epoch: 4522, loss: 0.87\n",
      "train epoch: 4523, loss 0.0826\n",
      "test epoch: 4523, loss: 0.87\n",
      "train epoch: 4524, loss 0.0827\n",
      "test epoch: 4524, loss: 0.87\n",
      "train epoch: 4525, loss 0.0826\n",
      "test epoch: 4525, loss: 0.87\n",
      "train epoch: 4526, loss 0.0826\n",
      "test epoch: 4526, loss: 0.87\n",
      "train epoch: 4527, loss 0.0828\n",
      "test epoch: 4527, loss: 0.87\n",
      "train epoch: 4528, loss 0.0829\n",
      "test epoch: 4528, loss: 0.87\n",
      "train epoch: 4529, loss 0.0828\n",
      "test epoch: 4529, loss: 0.87\n",
      "train epoch: 4530, loss 0.0830\n",
      "test epoch: 4530, loss: 0.87\n",
      "train epoch: 4531, loss 0.0827\n",
      "test epoch: 4531, loss: 0.87\n",
      "train epoch: 4532, loss 0.0828\n",
      "test epoch: 4532, loss: 0.87\n",
      "train epoch: 4533, loss 0.0826\n",
      "test epoch: 4533, loss: 0.87\n",
      "train epoch: 4534, loss 0.0826\n",
      "test epoch: 4534, loss: 0.87\n",
      "train epoch: 4535, loss 0.0827\n",
      "test epoch: 4535, loss: 0.87\n",
      "train epoch: 4536, loss 0.0826\n",
      "test epoch: 4536, loss: 0.87\n",
      "train epoch: 4537, loss 0.0826\n",
      "test epoch: 4537, loss: 0.87\n",
      "train epoch: 4538, loss 0.0827\n",
      "test epoch: 4538, loss: 0.87\n",
      "train epoch: 4539, loss 0.0830\n",
      "test epoch: 4539, loss: 0.87\n",
      "train epoch: 4540, loss 0.0827\n",
      "test epoch: 4540, loss: 0.87\n",
      "train epoch: 4541, loss 0.0827\n",
      "test epoch: 4541, loss: 0.87\n",
      "train epoch: 4542, loss 0.0828\n",
      "test epoch: 4542, loss: 0.87\n",
      "train epoch: 4543, loss 0.0827\n",
      "test epoch: 4543, loss: 0.87\n",
      "train epoch: 4544, loss 0.0827\n",
      "test epoch: 4544, loss: 0.87\n",
      "train epoch: 4545, loss 0.0828\n",
      "test epoch: 4545, loss: 0.87\n",
      "train epoch: 4546, loss 0.0827\n",
      "test epoch: 4546, loss: 0.87\n",
      "train epoch: 4547, loss 0.0828\n",
      "test epoch: 4547, loss: 0.87\n",
      "train epoch: 4548, loss 0.0827\n",
      "test epoch: 4548, loss: 0.87\n",
      "train epoch: 4549, loss 0.0827\n",
      "test epoch: 4549, loss: 0.87\n",
      "train epoch: 4550, loss 0.0826\n",
      "test epoch: 4550, loss: 0.87\n",
      "train epoch: 4551, loss 0.0829\n",
      "test epoch: 4551, loss: 0.87\n",
      "train epoch: 4552, loss 0.0828\n",
      "test epoch: 4552, loss: 0.87\n",
      "train epoch: 4553, loss 0.0828\n",
      "test epoch: 4553, loss: 0.87\n",
      "train epoch: 4554, loss 0.0827\n",
      "test epoch: 4554, loss: 0.87\n",
      "train epoch: 4555, loss 0.0828\n",
      "test epoch: 4555, loss: 0.87\n",
      "train epoch: 4556, loss 0.0827\n",
      "test epoch: 4556, loss: 0.87\n",
      "train epoch: 4557, loss 0.0827\n",
      "test epoch: 4557, loss: 0.87\n",
      "train epoch: 4558, loss 0.0826\n",
      "test epoch: 4558, loss: 0.87\n",
      "train epoch: 4559, loss 0.0826\n",
      "test epoch: 4559, loss: 0.87\n",
      "train epoch: 4560, loss 0.0827\n",
      "test epoch: 4560, loss: 0.87\n",
      "train epoch: 4561, loss 0.0827\n",
      "test epoch: 4561, loss: 0.87\n",
      "train epoch: 4562, loss 0.0828\n",
      "test epoch: 4562, loss: 0.87\n",
      "train epoch: 4563, loss 0.0826\n",
      "test epoch: 4563, loss: 0.87\n",
      "train epoch: 4564, loss 0.0829\n",
      "test epoch: 4564, loss: 0.87\n",
      "train epoch: 4565, loss 0.0832\n",
      "test epoch: 4565, loss: 0.87\n",
      "train epoch: 4566, loss 0.0827\n",
      "test epoch: 4566, loss: 0.87\n",
      "train epoch: 4567, loss 0.0839\n",
      "test epoch: 4567, loss: 0.87\n",
      "train epoch: 4568, loss 0.0827\n",
      "test epoch: 4568, loss: 0.87\n",
      "train epoch: 4569, loss 0.0830\n",
      "test epoch: 4569, loss: 0.87\n",
      "train epoch: 4570, loss 0.0831\n",
      "test epoch: 4570, loss: 0.87\n",
      "train epoch: 4571, loss 0.0828\n",
      "test epoch: 4571, loss: 0.87\n",
      "train epoch: 4572, loss 0.0829\n",
      "test epoch: 4572, loss: 0.87\n",
      "train epoch: 4573, loss 0.0828\n",
      "test epoch: 4573, loss: 0.87\n",
      "train epoch: 4574, loss 0.0828\n",
      "test epoch: 4574, loss: 0.87\n",
      "train epoch: 4575, loss 0.0834\n",
      "test epoch: 4575, loss: 0.87\n",
      "train epoch: 4576, loss 0.0826\n",
      "test epoch: 4576, loss: 0.87\n",
      "train epoch: 4577, loss 0.0832\n",
      "test epoch: 4577, loss: 0.87\n",
      "train epoch: 4578, loss 0.0830\n",
      "test epoch: 4578, loss: 0.87\n",
      "train epoch: 4579, loss 0.0829\n",
      "test epoch: 4579, loss: 0.87\n",
      "train epoch: 4580, loss 0.0834\n",
      "test epoch: 4580, loss: 0.87\n",
      "train epoch: 4581, loss 0.0826\n",
      "test epoch: 4581, loss: 0.87\n",
      "train epoch: 4582, loss 0.0829\n",
      "test epoch: 4582, loss: 0.87\n",
      "train epoch: 4583, loss 0.0828\n",
      "test epoch: 4583, loss: 0.87\n",
      "train epoch: 4584, loss 0.0827\n",
      "test epoch: 4584, loss: 0.87\n",
      "train epoch: 4585, loss 0.0827\n",
      "test epoch: 4585, loss: 0.87\n",
      "train epoch: 4586, loss 0.0828\n",
      "test epoch: 4586, loss: 0.87\n",
      "train epoch: 4587, loss 0.0830\n",
      "test epoch: 4587, loss: 0.87\n",
      "train epoch: 4588, loss 0.0829\n",
      "test epoch: 4588, loss: 0.87\n",
      "train epoch: 4589, loss 0.0828\n",
      "test epoch: 4589, loss: 0.87\n",
      "train epoch: 4590, loss 0.0833\n",
      "test epoch: 4590, loss: 0.87\n",
      "train epoch: 4591, loss 0.0831\n",
      "test epoch: 4591, loss: 0.87\n",
      "train epoch: 4592, loss 0.0832\n",
      "test epoch: 4592, loss: 0.87\n",
      "train epoch: 4593, loss 0.0837\n",
      "test epoch: 4593, loss: 0.87\n",
      "train epoch: 4594, loss 0.0829\n",
      "test epoch: 4594, loss: 0.87\n",
      "train epoch: 4595, loss 0.0839\n",
      "test epoch: 4595, loss: 0.87\n",
      "train epoch: 4596, loss 0.0832\n",
      "test epoch: 4596, loss: 0.87\n",
      "train epoch: 4597, loss 0.0831\n",
      "test epoch: 4597, loss: 0.87\n",
      "train epoch: 4598, loss 0.0835\n",
      "test epoch: 4598, loss: 0.87\n",
      "train epoch: 4599, loss 0.0828\n",
      "test epoch: 4599, loss: 0.87\n",
      "train epoch: 4600, loss 0.0830\n",
      "test epoch: 4600, loss: 0.87\n",
      "train epoch: 4601, loss 0.0830\n",
      "test epoch: 4601, loss: 0.87\n",
      "train epoch: 4602, loss 0.0829\n",
      "test epoch: 4602, loss: 0.87\n",
      "train epoch: 4603, loss 0.0829\n",
      "test epoch: 4603, loss: 0.87\n",
      "train epoch: 4604, loss 0.0830\n",
      "test epoch: 4604, loss: 0.87\n",
      "train epoch: 4605, loss 0.0830\n",
      "test epoch: 4605, loss: 0.87\n",
      "train epoch: 4606, loss 0.0827\n",
      "test epoch: 4606, loss: 0.87\n",
      "train epoch: 4607, loss 0.0830\n",
      "test epoch: 4607, loss: 0.87\n",
      "train epoch: 4608, loss 0.0829\n",
      "test epoch: 4608, loss: 0.87\n",
      "train epoch: 4609, loss 0.0827\n",
      "test epoch: 4609, loss: 0.87\n",
      "train epoch: 4610, loss 0.0832\n",
      "test epoch: 4610, loss: 0.87\n",
      "train epoch: 4611, loss 0.0827\n",
      "test epoch: 4611, loss: 0.87\n",
      "train epoch: 4612, loss 0.0827\n",
      "test epoch: 4612, loss: 0.87\n",
      "train epoch: 4613, loss 0.0829\n",
      "test epoch: 4613, loss: 0.87\n",
      "train epoch: 4614, loss 0.0829\n",
      "test epoch: 4614, loss: 0.87\n",
      "train epoch: 4615, loss 0.0827\n",
      "test epoch: 4615, loss: 0.87\n",
      "train epoch: 4616, loss 0.0828\n",
      "test epoch: 4616, loss: 0.87\n",
      "train epoch: 4617, loss 0.0829\n",
      "test epoch: 4617, loss: 0.87\n",
      "train epoch: 4618, loss 0.0829\n",
      "test epoch: 4618, loss: 0.87\n",
      "train epoch: 4619, loss 0.0830\n",
      "test epoch: 4619, loss: 0.87\n",
      "train epoch: 4620, loss 0.0829\n",
      "test epoch: 4620, loss: 0.87\n",
      "train epoch: 4621, loss 0.0829\n",
      "test epoch: 4621, loss: 0.87\n",
      "train epoch: 4622, loss 0.0829\n",
      "test epoch: 4622, loss: 0.87\n",
      "train epoch: 4623, loss 0.0827\n",
      "test epoch: 4623, loss: 0.87\n",
      "train epoch: 4624, loss 0.0828\n",
      "test epoch: 4624, loss: 0.87\n",
      "train epoch: 4625, loss 0.0828\n",
      "test epoch: 4625, loss: 0.87\n",
      "train epoch: 4626, loss 0.0829\n",
      "test epoch: 4626, loss: 0.87\n",
      "train epoch: 4627, loss 0.0828\n",
      "test epoch: 4627, loss: 0.87\n",
      "train epoch: 4628, loss 0.0828\n",
      "test epoch: 4628, loss: 0.87\n",
      "train epoch: 4629, loss 0.0826\n",
      "test epoch: 4629, loss: 0.87\n",
      "train epoch: 4630, loss 0.0833\n",
      "test epoch: 4630, loss: 0.87\n",
      "train epoch: 4631, loss 0.0827\n",
      "test epoch: 4631, loss: 0.87\n",
      "train epoch: 4632, loss 0.0830\n",
      "test epoch: 4632, loss: 0.87\n",
      "train epoch: 4633, loss 0.0834\n",
      "test epoch: 4633, loss: 0.87\n",
      "train epoch: 4634, loss 0.0829\n",
      "test epoch: 4634, loss: 0.87\n",
      "train epoch: 4635, loss 0.0838\n",
      "test epoch: 4635, loss: 0.87\n",
      "train epoch: 4636, loss 0.0827\n",
      "test epoch: 4636, loss: 0.87\n",
      "train epoch: 4637, loss 0.0831\n",
      "test epoch: 4637, loss: 0.87\n",
      "train epoch: 4638, loss 0.0831\n",
      "test epoch: 4638, loss: 0.87\n",
      "train epoch: 4639, loss 0.0827\n",
      "test epoch: 4639, loss: 0.87\n",
      "train epoch: 4640, loss 0.0830\n",
      "test epoch: 4640, loss: 0.87\n",
      "train epoch: 4641, loss 0.0827\n",
      "test epoch: 4641, loss: 0.87\n",
      "train epoch: 4642, loss 0.0827\n",
      "test epoch: 4642, loss: 0.87\n",
      "train epoch: 4643, loss 0.0827\n",
      "test epoch: 4643, loss: 0.87\n",
      "train epoch: 4644, loss 0.0828\n",
      "test epoch: 4644, loss: 0.87\n",
      "train epoch: 4645, loss 0.0828\n",
      "test epoch: 4645, loss: 0.87\n",
      "train epoch: 4646, loss 0.0828\n",
      "test epoch: 4646, loss: 0.87\n",
      "train epoch: 4647, loss 0.0827\n",
      "test epoch: 4647, loss: 0.87\n",
      "train epoch: 4648, loss 0.0829\n",
      "test epoch: 4648, loss: 0.87\n",
      "train epoch: 4649, loss 0.0826\n",
      "test epoch: 4649, loss: 0.87\n",
      "train epoch: 4650, loss 0.0826\n",
      "test epoch: 4650, loss: 0.87\n",
      "train epoch: 4651, loss 0.0826\n",
      "test epoch: 4651, loss: 0.87\n",
      "train epoch: 4652, loss 0.0826\n",
      "test epoch: 4652, loss: 0.87\n",
      "train epoch: 4653, loss 0.0826\n",
      "test epoch: 4653, loss: 0.87\n",
      "train epoch: 4654, loss 0.0830\n",
      "test epoch: 4654, loss: 0.87\n",
      "train epoch: 4655, loss 0.0828\n",
      "test epoch: 4655, loss: 0.87\n",
      "train epoch: 4656, loss 0.0826\n",
      "test epoch: 4656, loss: 0.87\n",
      "train epoch: 4657, loss 0.0831\n",
      "test epoch: 4657, loss: 0.87\n",
      "train epoch: 4658, loss 0.0830\n",
      "test epoch: 4658, loss: 0.87\n",
      "train epoch: 4659, loss 0.0827\n",
      "test epoch: 4659, loss: 0.87\n",
      "train epoch: 4660, loss 0.0835\n",
      "test epoch: 4660, loss: 0.87\n",
      "train epoch: 4661, loss 0.0827\n",
      "test epoch: 4661, loss: 0.87\n",
      "train epoch: 4662, loss 0.0827\n",
      "test epoch: 4662, loss: 0.87\n",
      "train epoch: 4663, loss 0.0831\n",
      "test epoch: 4663, loss: 0.87\n",
      "train epoch: 4664, loss 0.0827\n",
      "test epoch: 4664, loss: 0.87\n",
      "train epoch: 4665, loss 0.0827\n",
      "test epoch: 4665, loss: 0.87\n",
      "train epoch: 4666, loss 0.0829\n",
      "test epoch: 4666, loss: 0.87\n",
      "train epoch: 4667, loss 0.0828\n",
      "test epoch: 4667, loss: 0.87\n",
      "train epoch: 4668, loss 0.0826\n",
      "test epoch: 4668, loss: 0.87\n",
      "train epoch: 4669, loss 0.0828\n",
      "test epoch: 4669, loss: 0.87\n",
      "train epoch: 4670, loss 0.0829\n",
      "test epoch: 4670, loss: 0.87\n",
      "train epoch: 4671, loss 0.0826\n",
      "test epoch: 4671, loss: 0.87\n",
      "train epoch: 4672, loss 0.0828\n",
      "test epoch: 4672, loss: 0.87\n",
      "train epoch: 4673, loss 0.0827\n",
      "test epoch: 4673, loss: 0.87\n",
      "train epoch: 4674, loss 0.0826\n",
      "test epoch: 4674, loss: 0.87\n",
      "train epoch: 4675, loss 0.0828\n",
      "test epoch: 4675, loss: 0.87\n",
      "train epoch: 4676, loss 0.0830\n",
      "test epoch: 4676, loss: 0.87\n",
      "train epoch: 4677, loss 0.0827\n",
      "test epoch: 4677, loss: 0.87\n",
      "train epoch: 4678, loss 0.0829\n",
      "test epoch: 4678, loss: 0.87\n",
      "train epoch: 4679, loss 0.0827\n",
      "test epoch: 4679, loss: 0.87\n",
      "train epoch: 4680, loss 0.0827\n",
      "test epoch: 4680, loss: 0.87\n",
      "train epoch: 4681, loss 0.0826\n",
      "test epoch: 4681, loss: 0.87\n",
      "train epoch: 4682, loss 0.0827\n",
      "test epoch: 4682, loss: 0.87\n",
      "train epoch: 4683, loss 0.0830\n",
      "test epoch: 4683, loss: 0.87\n",
      "train epoch: 4684, loss 0.0828\n",
      "test epoch: 4684, loss: 0.87\n",
      "train epoch: 4685, loss 0.0833\n",
      "test epoch: 4685, loss: 0.87\n",
      "train epoch: 4686, loss 0.0830\n",
      "test epoch: 4686, loss: 0.87\n",
      "train epoch: 4687, loss 0.0827\n",
      "test epoch: 4687, loss: 0.87\n",
      "train epoch: 4688, loss 0.0837\n",
      "test epoch: 4688, loss: 0.87\n",
      "train epoch: 4689, loss 0.0827\n",
      "test epoch: 4689, loss: 0.87\n",
      "train epoch: 4690, loss 0.0830\n",
      "test epoch: 4690, loss: 0.87\n",
      "train epoch: 4691, loss 0.0828\n",
      "test epoch: 4691, loss: 0.87\n",
      "train epoch: 4692, loss 0.0828\n",
      "test epoch: 4692, loss: 0.87\n",
      "train epoch: 4693, loss 0.0828\n",
      "test epoch: 4693, loss: 0.87\n",
      "train epoch: 4694, loss 0.0828\n",
      "test epoch: 4694, loss: 0.87\n",
      "train epoch: 4695, loss 0.0827\n",
      "test epoch: 4695, loss: 0.87\n",
      "train epoch: 4696, loss 0.0827\n",
      "test epoch: 4696, loss: 0.87\n",
      "train epoch: 4697, loss 0.0827\n",
      "test epoch: 4697, loss: 0.87\n",
      "train epoch: 4698, loss 0.0827\n",
      "test epoch: 4698, loss: 0.87\n",
      "train epoch: 4699, loss 0.0828\n",
      "test epoch: 4699, loss: 0.87\n",
      "train epoch: 4700, loss 0.0827\n",
      "test epoch: 4700, loss: 0.87\n",
      "train epoch: 4701, loss 0.0826\n",
      "test epoch: 4701, loss: 0.87\n",
      "train epoch: 4702, loss 0.0827\n",
      "test epoch: 4702, loss: 0.87\n",
      "train epoch: 4703, loss 0.0826\n",
      "test epoch: 4703, loss: 0.87\n",
      "train epoch: 4704, loss 0.0826\n",
      "test epoch: 4704, loss: 0.87\n",
      "train epoch: 4705, loss 0.0826\n",
      "test epoch: 4705, loss: 0.87\n",
      "train epoch: 4706, loss 0.0830\n",
      "test epoch: 4706, loss: 0.87\n",
      "train epoch: 4707, loss 0.0829\n",
      "test epoch: 4707, loss: 0.87\n",
      "train epoch: 4708, loss 0.0828\n",
      "test epoch: 4708, loss: 0.87\n",
      "train epoch: 4709, loss 0.0830\n",
      "test epoch: 4709, loss: 0.87\n",
      "train epoch: 4710, loss 0.0828\n",
      "test epoch: 4710, loss: 0.87\n",
      "train epoch: 4711, loss 0.0826\n",
      "test epoch: 4711, loss: 0.87\n",
      "train epoch: 4712, loss 0.0828\n",
      "test epoch: 4712, loss: 0.87\n",
      "train epoch: 4713, loss 0.0828\n",
      "test epoch: 4713, loss: 0.87\n",
      "train epoch: 4714, loss 0.0826\n",
      "test epoch: 4714, loss: 0.87\n",
      "train epoch: 4715, loss 0.0828\n",
      "test epoch: 4715, loss: 0.87\n",
      "train epoch: 4716, loss 0.0829\n",
      "test epoch: 4716, loss: 0.87\n",
      "train epoch: 4717, loss 0.0827\n",
      "test epoch: 4717, loss: 0.87\n",
      "train epoch: 4718, loss 0.0831\n",
      "test epoch: 4718, loss: 0.87\n",
      "train epoch: 4719, loss 0.0833\n",
      "test epoch: 4719, loss: 0.87\n",
      "train epoch: 4720, loss 0.0829\n",
      "test epoch: 4720, loss: 0.87\n",
      "train epoch: 4721, loss 0.0837\n",
      "test epoch: 4721, loss: 0.87\n",
      "train epoch: 4722, loss 0.0828\n",
      "test epoch: 4722, loss: 0.87\n",
      "train epoch: 4723, loss 0.0830\n",
      "test epoch: 4723, loss: 0.87\n",
      "train epoch: 4724, loss 0.0832\n",
      "test epoch: 4724, loss: 0.87\n",
      "train epoch: 4725, loss 0.0827\n",
      "test epoch: 4725, loss: 0.87\n",
      "train epoch: 4726, loss 0.0830\n",
      "test epoch: 4726, loss: 0.87\n",
      "train epoch: 4727, loss 0.0831\n",
      "test epoch: 4727, loss: 0.87\n",
      "train epoch: 4728, loss 0.0827\n",
      "test epoch: 4728, loss: 0.87\n",
      "train epoch: 4729, loss 0.0837\n",
      "test epoch: 4729, loss: 0.87\n",
      "train epoch: 4730, loss 0.0830\n",
      "test epoch: 4730, loss: 0.87\n",
      "train epoch: 4731, loss 0.0832\n",
      "test epoch: 4731, loss: 0.87\n",
      "train epoch: 4732, loss 0.0836\n",
      "test epoch: 4732, loss: 0.87\n",
      "train epoch: 4733, loss 0.0829\n",
      "test epoch: 4733, loss: 0.87\n",
      "train epoch: 4734, loss 0.0833\n",
      "test epoch: 4734, loss: 0.87\n",
      "train epoch: 4735, loss 0.0834\n",
      "test epoch: 4735, loss: 0.87\n",
      "train epoch: 4736, loss 0.0827\n",
      "test epoch: 4736, loss: 0.87\n",
      "train epoch: 4737, loss 0.0833\n",
      "test epoch: 4737, loss: 0.87\n",
      "train epoch: 4738, loss 0.0832\n",
      "test epoch: 4738, loss: 0.87\n",
      "train epoch: 4739, loss 0.0827\n",
      "test epoch: 4739, loss: 0.87\n",
      "train epoch: 4740, loss 0.0835\n",
      "test epoch: 4740, loss: 0.87\n",
      "train epoch: 4741, loss 0.0829\n",
      "test epoch: 4741, loss: 0.87\n",
      "train epoch: 4742, loss 0.0830\n",
      "test epoch: 4742, loss: 0.87\n",
      "train epoch: 4743, loss 0.0835\n",
      "test epoch: 4743, loss: 0.87\n",
      "train epoch: 4744, loss 0.0828\n",
      "test epoch: 4744, loss: 0.87\n",
      "train epoch: 4745, loss 0.0830\n",
      "test epoch: 4745, loss: 0.87\n",
      "train epoch: 4746, loss 0.0833\n",
      "test epoch: 4746, loss: 0.87\n",
      "train epoch: 4747, loss 0.0827\n",
      "test epoch: 4747, loss: 0.87\n",
      "train epoch: 4748, loss 0.0827\n",
      "test epoch: 4748, loss: 0.87\n",
      "train epoch: 4749, loss 0.0831\n",
      "test epoch: 4749, loss: 0.87\n",
      "train epoch: 4750, loss 0.0827\n",
      "test epoch: 4750, loss: 0.87\n",
      "train epoch: 4751, loss 0.0827\n",
      "test epoch: 4751, loss: 0.87\n",
      "train epoch: 4752, loss 0.0830\n",
      "test epoch: 4752, loss: 0.87\n",
      "train epoch: 4753, loss 0.0826\n",
      "test epoch: 4753, loss: 0.87\n",
      "train epoch: 4754, loss 0.0828\n",
      "test epoch: 4754, loss: 0.87\n",
      "train epoch: 4755, loss 0.0830\n",
      "test epoch: 4755, loss: 0.87\n",
      "train epoch: 4756, loss 0.0827\n",
      "test epoch: 4756, loss: 0.87\n",
      "train epoch: 4757, loss 0.0827\n",
      "test epoch: 4757, loss: 0.87\n",
      "train epoch: 4758, loss 0.0829\n",
      "test epoch: 4758, loss: 0.87\n",
      "train epoch: 4759, loss 0.0828\n",
      "test epoch: 4759, loss: 0.87\n",
      "train epoch: 4760, loss 0.0826\n",
      "test epoch: 4760, loss: 0.87\n",
      "train epoch: 4761, loss 0.0827\n",
      "test epoch: 4761, loss: 0.87\n",
      "train epoch: 4762, loss 0.0826\n",
      "test epoch: 4762, loss: 0.87\n",
      "train epoch: 4763, loss 0.0826\n",
      "test epoch: 4763, loss: 0.87\n",
      "train epoch: 4764, loss 0.0827\n",
      "test epoch: 4764, loss: 0.87\n",
      "train epoch: 4765, loss 0.0830\n",
      "test epoch: 4765, loss: 0.87\n",
      "train epoch: 4766, loss 0.0828\n",
      "test epoch: 4766, loss: 0.87\n",
      "train epoch: 4767, loss 0.0830\n",
      "test epoch: 4767, loss: 0.87\n",
      "train epoch: 4768, loss 0.0827\n",
      "test epoch: 4768, loss: 0.87\n",
      "train epoch: 4769, loss 0.0828\n",
      "test epoch: 4769, loss: 0.87\n",
      "train epoch: 4770, loss 0.0826\n",
      "test epoch: 4770, loss: 0.87\n",
      "train epoch: 4771, loss 0.0827\n",
      "test epoch: 4771, loss: 0.87\n",
      "train epoch: 4772, loss 0.0827\n",
      "test epoch: 4772, loss: 0.87\n",
      "train epoch: 4773, loss 0.0827\n",
      "test epoch: 4773, loss: 0.87\n",
      "train epoch: 4774, loss 0.0826\n",
      "test epoch: 4774, loss: 0.87\n",
      "train epoch: 4775, loss 0.0828\n",
      "test epoch: 4775, loss: 0.87\n",
      "train epoch: 4776, loss 0.0828\n",
      "test epoch: 4776, loss: 0.87\n",
      "train epoch: 4777, loss 0.0827\n",
      "test epoch: 4777, loss: 0.87\n",
      "train epoch: 4778, loss 0.0827\n",
      "test epoch: 4778, loss: 0.87\n",
      "train epoch: 4779, loss 0.0826\n",
      "test epoch: 4779, loss: 0.87\n",
      "train epoch: 4780, loss 0.0826\n",
      "test epoch: 4780, loss: 0.87\n",
      "train epoch: 4781, loss 0.0827\n",
      "test epoch: 4781, loss: 0.87\n",
      "train epoch: 4782, loss 0.0830\n",
      "test epoch: 4782, loss: 0.87\n",
      "train epoch: 4783, loss 0.0828\n",
      "test epoch: 4783, loss: 0.87\n",
      "train epoch: 4784, loss 0.0827\n",
      "test epoch: 4784, loss: 0.87\n",
      "train epoch: 4785, loss 0.0828\n",
      "test epoch: 4785, loss: 0.87\n",
      "train epoch: 4786, loss 0.0826\n",
      "test epoch: 4786, loss: 0.87\n",
      "train epoch: 4787, loss 0.0826\n",
      "test epoch: 4787, loss: 0.87\n",
      "train epoch: 4788, loss 0.0826\n",
      "test epoch: 4788, loss: 0.87\n",
      "train epoch: 4789, loss 0.0826\n",
      "test epoch: 4789, loss: 0.87\n",
      "train epoch: 4790, loss 0.0826\n",
      "test epoch: 4790, loss: 0.87\n",
      "train epoch: 4791, loss 0.0827\n",
      "test epoch: 4791, loss: 0.87\n",
      "train epoch: 4792, loss 0.0828\n",
      "test epoch: 4792, loss: 0.87\n",
      "train epoch: 4793, loss 0.0827\n",
      "test epoch: 4793, loss: 0.87\n",
      "train epoch: 4794, loss 0.0826\n",
      "test epoch: 4794, loss: 0.87\n",
      "train epoch: 4795, loss 0.0828\n",
      "test epoch: 4795, loss: 0.87\n",
      "train epoch: 4796, loss 0.0828\n",
      "test epoch: 4796, loss: 0.87\n",
      "train epoch: 4797, loss 0.0828\n",
      "test epoch: 4797, loss: 0.87\n",
      "train epoch: 4798, loss 0.0828\n",
      "test epoch: 4798, loss: 0.87\n",
      "train epoch: 4799, loss 0.0829\n",
      "test epoch: 4799, loss: 0.87\n",
      "train epoch: 4800, loss 0.0828\n",
      "test epoch: 4800, loss: 0.87\n",
      "train epoch: 4801, loss 0.0827\n",
      "test epoch: 4801, loss: 0.87\n",
      "train epoch: 4802, loss 0.0831\n",
      "test epoch: 4802, loss: 0.87\n",
      "train epoch: 4803, loss 0.0829\n",
      "test epoch: 4803, loss: 0.87\n",
      "train epoch: 4804, loss 0.0827\n",
      "test epoch: 4804, loss: 0.87\n",
      "train epoch: 4805, loss 0.0829\n",
      "test epoch: 4805, loss: 0.87\n",
      "train epoch: 4806, loss 0.0827\n",
      "test epoch: 4806, loss: 0.87\n",
      "train epoch: 4807, loss 0.0826\n",
      "test epoch: 4807, loss: 0.87\n",
      "train epoch: 4808, loss 0.0830\n",
      "test epoch: 4808, loss: 0.87\n",
      "train epoch: 4809, loss 0.0828\n",
      "test epoch: 4809, loss: 0.87\n",
      "train epoch: 4810, loss 0.0827\n",
      "test epoch: 4810, loss: 0.87\n",
      "train epoch: 4811, loss 0.0827\n",
      "test epoch: 4811, loss: 0.87\n",
      "train epoch: 4812, loss 0.0827\n",
      "test epoch: 4812, loss: 0.87\n",
      "train epoch: 4813, loss 0.0827\n",
      "test epoch: 4813, loss: 0.87\n",
      "train epoch: 4814, loss 0.0828\n",
      "test epoch: 4814, loss: 0.87\n",
      "train epoch: 4815, loss 0.0826\n",
      "test epoch: 4815, loss: 0.87\n",
      "train epoch: 4816, loss 0.0826\n",
      "test epoch: 4816, loss: 0.87\n",
      "train epoch: 4817, loss 0.0827\n",
      "test epoch: 4817, loss: 0.87\n",
      "train epoch: 4818, loss 0.0826\n",
      "test epoch: 4818, loss: 0.87\n",
      "train epoch: 4819, loss 0.0829\n",
      "test epoch: 4819, loss: 0.87\n",
      "train epoch: 4820, loss 0.0829\n",
      "test epoch: 4820, loss: 0.87\n",
      "train epoch: 4821, loss 0.0827\n",
      "test epoch: 4821, loss: 0.87\n",
      "train epoch: 4822, loss 0.0832\n",
      "test epoch: 4822, loss: 0.87\n",
      "train epoch: 4823, loss 0.0831\n",
      "test epoch: 4823, loss: 0.87\n",
      "train epoch: 4824, loss 0.0828\n",
      "test epoch: 4824, loss: 0.87\n",
      "train epoch: 4825, loss 0.0835\n",
      "test epoch: 4825, loss: 0.87\n",
      "train epoch: 4826, loss 0.0828\n",
      "test epoch: 4826, loss: 0.87\n",
      "train epoch: 4827, loss 0.0828\n",
      "test epoch: 4827, loss: 0.87\n",
      "train epoch: 4828, loss 0.0830\n",
      "test epoch: 4828, loss: 0.87\n",
      "train epoch: 4829, loss 0.0830\n",
      "test epoch: 4829, loss: 0.87\n",
      "train epoch: 4830, loss 0.0826\n",
      "test epoch: 4830, loss: 0.87\n",
      "train epoch: 4831, loss 0.0830\n",
      "test epoch: 4831, loss: 0.87\n",
      "train epoch: 4832, loss 0.0830\n",
      "test epoch: 4832, loss: 0.87\n",
      "train epoch: 4833, loss 0.0826\n",
      "test epoch: 4833, loss: 0.87\n",
      "train epoch: 4834, loss 0.0831\n",
      "test epoch: 4834, loss: 0.87\n",
      "train epoch: 4835, loss 0.0830\n",
      "test epoch: 4835, loss: 0.87\n",
      "train epoch: 4836, loss 0.0827\n",
      "test epoch: 4836, loss: 0.87\n",
      "train epoch: 4837, loss 0.0834\n",
      "test epoch: 4837, loss: 0.87\n",
      "train epoch: 4838, loss 0.0828\n",
      "test epoch: 4838, loss: 0.87\n",
      "train epoch: 4839, loss 0.0827\n",
      "test epoch: 4839, loss: 0.87\n",
      "train epoch: 4840, loss 0.0834\n",
      "test epoch: 4840, loss: 0.87\n",
      "train epoch: 4841, loss 0.0826\n",
      "test epoch: 4841, loss: 0.87\n",
      "train epoch: 4842, loss 0.0830\n",
      "test epoch: 4842, loss: 0.87\n",
      "train epoch: 4843, loss 0.0827\n",
      "test epoch: 4843, loss: 0.87\n",
      "train epoch: 4844, loss 0.0828\n",
      "test epoch: 4844, loss: 0.87\n",
      "train epoch: 4845, loss 0.0834\n",
      "test epoch: 4845, loss: 0.87\n",
      "train epoch: 4846, loss 0.0827\n",
      "test epoch: 4846, loss: 0.87\n",
      "train epoch: 4847, loss 0.0838\n",
      "test epoch: 4847, loss: 0.87\n",
      "train epoch: 4848, loss 0.0829\n",
      "test epoch: 4848, loss: 0.87\n",
      "train epoch: 4849, loss 0.0830\n",
      "test epoch: 4849, loss: 0.87\n",
      "train epoch: 4850, loss 0.0832\n",
      "test epoch: 4850, loss: 0.87\n",
      "train epoch: 4851, loss 0.0829\n",
      "test epoch: 4851, loss: 0.87\n",
      "train epoch: 4852, loss 0.0829\n",
      "test epoch: 4852, loss: 0.87\n",
      "train epoch: 4853, loss 0.0830\n",
      "test epoch: 4853, loss: 0.87\n",
      "train epoch: 4854, loss 0.0828\n",
      "test epoch: 4854, loss: 0.87\n",
      "train epoch: 4855, loss 0.0834\n",
      "test epoch: 4855, loss: 0.87\n",
      "train epoch: 4856, loss 0.0832\n",
      "test epoch: 4856, loss: 0.87\n",
      "train epoch: 4857, loss 0.0830\n",
      "test epoch: 4857, loss: 0.87\n",
      "train epoch: 4858, loss 0.0836\n",
      "test epoch: 4858, loss: 0.87\n",
      "train epoch: 4859, loss 0.0829\n",
      "test epoch: 4859, loss: 0.87\n",
      "train epoch: 4860, loss 0.0833\n",
      "test epoch: 4860, loss: 0.87\n",
      "train epoch: 4861, loss 0.0835\n",
      "test epoch: 4861, loss: 0.87\n",
      "train epoch: 4862, loss 0.0827\n",
      "test epoch: 4862, loss: 0.87\n",
      "train epoch: 4863, loss 0.0835\n",
      "test epoch: 4863, loss: 0.87\n",
      "train epoch: 4864, loss 0.0833\n",
      "test epoch: 4864, loss: 0.87\n",
      "train epoch: 4865, loss 0.0829\n",
      "test epoch: 4865, loss: 0.87\n",
      "train epoch: 4866, loss 0.0840\n",
      "test epoch: 4866, loss: 0.87\n",
      "train epoch: 4867, loss 0.0828\n",
      "test epoch: 4867, loss: 0.87\n",
      "train epoch: 4868, loss 0.0831\n",
      "test epoch: 4868, loss: 0.87\n",
      "train epoch: 4869, loss 0.0833\n",
      "test epoch: 4869, loss: 0.87\n",
      "train epoch: 4870, loss 0.0827\n",
      "test epoch: 4870, loss: 0.87\n",
      "train epoch: 4871, loss 0.0832\n",
      "test epoch: 4871, loss: 0.87\n",
      "train epoch: 4872, loss 0.0827\n",
      "test epoch: 4872, loss: 0.87\n",
      "train epoch: 4873, loss 0.0830\n",
      "test epoch: 4873, loss: 0.87\n",
      "train epoch: 4874, loss 0.0831\n",
      "test epoch: 4874, loss: 0.87\n",
      "train epoch: 4875, loss 0.0827\n",
      "test epoch: 4875, loss: 0.87\n",
      "train epoch: 4876, loss 0.0835\n",
      "test epoch: 4876, loss: 0.87\n",
      "train epoch: 4877, loss 0.0827\n",
      "test epoch: 4877, loss: 0.87\n",
      "train epoch: 4878, loss 0.0829\n",
      "test epoch: 4878, loss: 0.87\n",
      "train epoch: 4879, loss 0.0830\n",
      "test epoch: 4879, loss: 0.87\n",
      "train epoch: 4880, loss 0.0826\n",
      "test epoch: 4880, loss: 0.87\n",
      "train epoch: 4881, loss 0.0830\n",
      "test epoch: 4881, loss: 0.87\n",
      "train epoch: 4882, loss 0.0829\n",
      "test epoch: 4882, loss: 0.87\n",
      "train epoch: 4883, loss 0.0827\n",
      "test epoch: 4883, loss: 0.87\n",
      "train epoch: 4884, loss 0.0827\n",
      "test epoch: 4884, loss: 0.87\n",
      "train epoch: 4885, loss 0.0827\n",
      "test epoch: 4885, loss: 0.87\n",
      "train epoch: 4886, loss 0.0827\n",
      "test epoch: 4886, loss: 0.87\n",
      "train epoch: 4887, loss 0.0827\n",
      "test epoch: 4887, loss: 0.87\n",
      "train epoch: 4888, loss 0.0829\n",
      "test epoch: 4888, loss: 0.87\n",
      "train epoch: 4889, loss 0.0828\n",
      "test epoch: 4889, loss: 0.87\n",
      "train epoch: 4890, loss 0.0827\n",
      "test epoch: 4890, loss: 0.87\n",
      "train epoch: 4891, loss 0.0827\n",
      "test epoch: 4891, loss: 0.87\n",
      "train epoch: 4892, loss 0.0828\n",
      "test epoch: 4892, loss: 0.87\n",
      "train epoch: 4893, loss 0.0827\n",
      "test epoch: 4893, loss: 0.87\n",
      "train epoch: 4894, loss 0.0826\n",
      "test epoch: 4894, loss: 0.87\n",
      "train epoch: 4895, loss 0.0826\n",
      "test epoch: 4895, loss: 0.87\n",
      "train epoch: 4896, loss 0.0827\n",
      "test epoch: 4896, loss: 0.87\n",
      "train epoch: 4897, loss 0.0826\n",
      "test epoch: 4897, loss: 0.87\n",
      "train epoch: 4898, loss 0.0826\n",
      "test epoch: 4898, loss: 0.87\n",
      "train epoch: 4899, loss 0.0831\n",
      "test epoch: 4899, loss: 0.87\n",
      "train epoch: 4900, loss 0.0827\n",
      "test epoch: 4900, loss: 0.87\n",
      "train epoch: 4901, loss 0.0830\n",
      "test epoch: 4901, loss: 0.87\n",
      "train epoch: 4902, loss 0.0833\n",
      "test epoch: 4902, loss: 0.87\n",
      "train epoch: 4903, loss 0.0828\n",
      "test epoch: 4903, loss: 0.87\n",
      "train epoch: 4904, loss 0.0838\n",
      "test epoch: 4904, loss: 0.87\n",
      "train epoch: 4905, loss 0.0828\n",
      "test epoch: 4905, loss: 0.87\n",
      "train epoch: 4906, loss 0.0831\n",
      "test epoch: 4906, loss: 0.87\n",
      "train epoch: 4907, loss 0.0829\n",
      "test epoch: 4907, loss: 0.87\n",
      "train epoch: 4908, loss 0.0829\n",
      "test epoch: 4908, loss: 0.87\n",
      "train epoch: 4909, loss 0.0826\n",
      "test epoch: 4909, loss: 0.87\n",
      "train epoch: 4910, loss 0.0828\n",
      "test epoch: 4910, loss: 0.87\n",
      "train epoch: 4911, loss 0.0826\n",
      "test epoch: 4911, loss: 0.87\n",
      "train epoch: 4912, loss 0.0828\n",
      "test epoch: 4912, loss: 0.87\n",
      "train epoch: 4913, loss 0.0831\n",
      "test epoch: 4913, loss: 0.87\n",
      "train epoch: 4914, loss 0.0829\n",
      "test epoch: 4914, loss: 0.87\n",
      "train epoch: 4915, loss 0.0829\n",
      "test epoch: 4915, loss: 0.87\n",
      "train epoch: 4916, loss 0.0829\n",
      "test epoch: 4916, loss: 0.87\n",
      "train epoch: 4917, loss 0.0827\n",
      "test epoch: 4917, loss: 0.87\n",
      "train epoch: 4918, loss 0.0828\n",
      "test epoch: 4918, loss: 0.87\n",
      "train epoch: 4919, loss 0.0832\n",
      "test epoch: 4919, loss: 0.87\n",
      "train epoch: 4920, loss 0.0827\n",
      "test epoch: 4920, loss: 0.87\n",
      "train epoch: 4921, loss 0.0828\n",
      "test epoch: 4921, loss: 0.87\n",
      "train epoch: 4922, loss 0.0828\n",
      "test epoch: 4922, loss: 0.87\n",
      "train epoch: 4923, loss 0.0827\n",
      "test epoch: 4923, loss: 0.87\n",
      "train epoch: 4924, loss 0.0832\n",
      "test epoch: 4924, loss: 0.87\n",
      "train epoch: 4925, loss 0.0834\n",
      "test epoch: 4925, loss: 0.87\n",
      "train epoch: 4926, loss 0.0826\n",
      "test epoch: 4926, loss: 0.87\n",
      "train epoch: 4927, loss 0.0828\n",
      "test epoch: 4927, loss: 0.87\n",
      "train epoch: 4928, loss 0.0829\n",
      "test epoch: 4928, loss: 0.87\n",
      "train epoch: 4929, loss 0.0827\n",
      "test epoch: 4929, loss: 0.87\n",
      "train epoch: 4930, loss 0.0829\n",
      "test epoch: 4930, loss: 0.87\n",
      "train epoch: 4931, loss 0.0828\n",
      "test epoch: 4931, loss: 0.87\n",
      "train epoch: 4932, loss 0.0826\n",
      "test epoch: 4932, loss: 0.87\n",
      "train epoch: 4933, loss 0.0830\n",
      "test epoch: 4933, loss: 0.87\n",
      "train epoch: 4934, loss 0.0829\n",
      "test epoch: 4934, loss: 0.87\n",
      "train epoch: 4935, loss 0.0827\n",
      "test epoch: 4935, loss: 0.87\n",
      "train epoch: 4936, loss 0.0831\n",
      "test epoch: 4936, loss: 0.87\n",
      "train epoch: 4937, loss 0.0829\n",
      "test epoch: 4937, loss: 0.87\n",
      "train epoch: 4938, loss 0.0827\n",
      "test epoch: 4938, loss: 0.87\n",
      "train epoch: 4939, loss 0.0832\n",
      "test epoch: 4939, loss: 0.87\n",
      "train epoch: 4940, loss 0.0826\n",
      "test epoch: 4940, loss: 0.87\n",
      "train epoch: 4941, loss 0.0828\n",
      "test epoch: 4941, loss: 0.87\n",
      "train epoch: 4942, loss 0.0829\n",
      "test epoch: 4942, loss: 0.87\n",
      "train epoch: 4943, loss 0.0827\n",
      "test epoch: 4943, loss: 0.87\n",
      "train epoch: 4944, loss 0.0826\n",
      "test epoch: 4944, loss: 0.87\n",
      "train epoch: 4945, loss 0.0827\n",
      "test epoch: 4945, loss: 0.87\n",
      "train epoch: 4946, loss 0.0828\n",
      "test epoch: 4946, loss: 0.87\n",
      "train epoch: 4947, loss 0.0826\n",
      "test epoch: 4947, loss: 0.87\n",
      "train epoch: 4948, loss 0.0828\n",
      "test epoch: 4948, loss: 0.87\n",
      "train epoch: 4949, loss 0.0830\n",
      "test epoch: 4949, loss: 0.87\n",
      "train epoch: 4950, loss 0.0826\n",
      "test epoch: 4950, loss: 0.87\n",
      "train epoch: 4951, loss 0.0831\n",
      "test epoch: 4951, loss: 0.87\n",
      "train epoch: 4952, loss 0.0828\n",
      "test epoch: 4952, loss: 0.87\n",
      "train epoch: 4953, loss 0.0827\n",
      "test epoch: 4953, loss: 0.87\n",
      "train epoch: 4954, loss 0.0833\n",
      "test epoch: 4954, loss: 0.87\n",
      "train epoch: 4955, loss 0.0827\n",
      "test epoch: 4955, loss: 0.87\n",
      "train epoch: 4956, loss 0.0828\n",
      "test epoch: 4956, loss: 0.87\n",
      "train epoch: 4957, loss 0.0828\n",
      "test epoch: 4957, loss: 0.87\n",
      "train epoch: 4958, loss 0.0827\n",
      "test epoch: 4958, loss: 0.87\n",
      "train epoch: 4959, loss 0.0826\n",
      "test epoch: 4959, loss: 0.87\n",
      "train epoch: 4960, loss 0.0827\n",
      "test epoch: 4960, loss: 0.87\n",
      "train epoch: 4961, loss 0.0828\n",
      "test epoch: 4961, loss: 0.87\n",
      "train epoch: 4962, loss 0.0826\n",
      "test epoch: 4962, loss: 0.87\n",
      "train epoch: 4963, loss 0.0828\n",
      "test epoch: 4963, loss: 0.87\n",
      "train epoch: 4964, loss 0.0830\n",
      "test epoch: 4964, loss: 0.87\n",
      "train epoch: 4965, loss 0.0826\n",
      "test epoch: 4965, loss: 0.87\n",
      "train epoch: 4966, loss 0.0832\n",
      "test epoch: 4966, loss: 0.87\n",
      "train epoch: 4967, loss 0.0828\n",
      "test epoch: 4967, loss: 0.87\n",
      "train epoch: 4968, loss 0.0827\n",
      "test epoch: 4968, loss: 0.87\n",
      "train epoch: 4969, loss 0.0833\n",
      "test epoch: 4969, loss: 0.87\n",
      "train epoch: 4970, loss 0.0825\n",
      "test epoch: 4970, loss: 0.87\n",
      "train epoch: 4971, loss 0.0829\n",
      "test epoch: 4971, loss: 0.87\n",
      "train epoch: 4972, loss 0.0827\n",
      "test epoch: 4972, loss: 0.87\n",
      "train epoch: 4973, loss 0.0826\n",
      "test epoch: 4973, loss: 0.87\n",
      "train epoch: 4974, loss 0.0827\n",
      "test epoch: 4974, loss: 0.87\n",
      "train epoch: 4975, loss 0.0828\n",
      "test epoch: 4975, loss: 0.87\n",
      "train epoch: 4976, loss 0.0826\n",
      "test epoch: 4976, loss: 0.87\n",
      "train epoch: 4977, loss 0.0825\n",
      "test epoch: 4977, loss: 0.87\n",
      "train epoch: 4978, loss 0.0828\n",
      "test epoch: 4978, loss: 0.87\n",
      "train epoch: 4979, loss 0.0829\n",
      "test epoch: 4979, loss: 0.87\n",
      "train epoch: 4980, loss 0.0826\n",
      "test epoch: 4980, loss: 0.87\n",
      "train epoch: 4981, loss 0.0829\n",
      "test epoch: 4981, loss: 0.87\n",
      "train epoch: 4982, loss 0.0830\n",
      "test epoch: 4982, loss: 0.87\n",
      "train epoch: 4983, loss 0.0826\n",
      "test epoch: 4983, loss: 0.87\n",
      "train epoch: 4984, loss 0.0836\n",
      "test epoch: 4984, loss: 0.87\n",
      "train epoch: 4985, loss 0.0826\n",
      "test epoch: 4985, loss: 0.87\n",
      "train epoch: 4986, loss 0.0827\n",
      "test epoch: 4986, loss: 0.87\n",
      "train epoch: 4987, loss 0.0831\n",
      "test epoch: 4987, loss: 0.87\n",
      "train epoch: 4988, loss 0.0826\n",
      "test epoch: 4988, loss: 0.87\n",
      "train epoch: 4989, loss 0.0827\n",
      "test epoch: 4989, loss: 0.87\n",
      "train epoch: 4990, loss 0.0828\n",
      "test epoch: 4990, loss: 0.87\n",
      "train epoch: 4991, loss 0.0827\n",
      "test epoch: 4991, loss: 0.87\n",
      "train epoch: 4992, loss 0.0825\n",
      "test epoch: 4992, loss: 0.87\n",
      "train epoch: 4993, loss 0.0826\n",
      "test epoch: 4993, loss: 0.87\n",
      "train epoch: 4994, loss 0.0826\n",
      "test epoch: 4994, loss: 0.87\n",
      "train epoch: 4995, loss 0.0827\n",
      "test epoch: 4995, loss: 0.87\n",
      "train epoch: 4996, loss 0.0827\n",
      "test epoch: 4996, loss: 0.87\n",
      "train epoch: 4997, loss 0.0826\n",
      "test epoch: 4997, loss: 0.87\n",
      "train epoch: 4998, loss 0.0826\n",
      "test epoch: 4998, loss: 0.87\n",
      "train epoch: 4999, loss 0.0826\n",
      "test epoch: 4999, loss: 0.87\n",
      "train epoch: 5000, loss 0.0827\n",
      "test epoch: 5000, loss: 0.87\n"
     ]
    }
   ],
   "source": [
    "test_loss_diverse, final_state_diverse = train_network_diverse()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot now the output of the neural network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x29e127940>]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit_outputs = evaluate_model_outputs(state = final_state_diverse, input = jnp.expand_dims(x,axis=1))\n",
    "plt.figure()\n",
    "plt.scatter(x_training,y_training,c='red')\n",
    "plt.scatter(x_test,y_test,c='blue')\n",
    "plt.plot(x, logit_outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
